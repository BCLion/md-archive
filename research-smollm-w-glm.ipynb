{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-18T10:45:00.471863Z","iopub.execute_input":"2026-01-18T10:45:00.472516Z","iopub.status.idle":"2026-01-18T10:45:00.477052Z","shell.execute_reply.started":"2026-01-18T10:45:00.472488Z","shell.execute_reply":"2026-01-18T10:45:00.476349Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"\nAntique blueprints are original historical documents or reproductions of architectural and engineering plans, often valued as collectibles or for historical research. They can be found through specialized online retailers, auction houses, and institutional archives. \nWhere to Find Antique Blueprints\nOnline Marketplaces: Websites like eBay and Etsy are common sources for purchasing original vintage blueprints. You can find unique items, such as the Vintage 1946 Wilson Martin Architectural Blueprint, Philadelphia Industrial Plant, Large Wall Art or a Baltimore & Ohio R.r. Western Lines Antique Blueprint 8x10 '' 1921.\nArchitectural Plan Books: Reprints of historical plan books, like Victorian Domestic Architectural Plans and Details: 734 Scale Drawings available on Amazon CA, offer numerous designs and details from specific eras for inspiration or reference.\nArchives and Libraries: Local libraries, historical societies, and national archives (such as the Library of Congress or the National Archives) often hold extensive collections of historical maps and building plans accessible for viewing or reproduction.\nAuctions and Appraisals: Specialized auction houses, like A.H. Wilkens Auctions & Appraisals, handle rare and valuable documents, including unique hand-drawn blueprints and historic maps.\nProfessional Experts: Architectural historians or preservationists may have access to a network of resources and unlisted documents, and can offer guidance on where to find specific historical plans. \nTips for Collecting\nDetermine Authenticity: Original blueprints are made using a specific chemical process (cyanotype) that gives them their distinct blue colour and white lines. Fading, tears, and creasing are common signs of age and use in original \"working\" blueprints.\nValue: The value of an antique blueprint is determined by several factors, including the fame of the architect (e.g., Frank Lloyd Wright), the significance of the structure, the age, condition, and rarity. Appraisals from qualified experts can help determine the market value for insurance or resale purposes.\nStorage and Preservation: Blueprints are delicate and can be fragile. Storing them rolled in a tube or professionally framed can help prevent further damage.\nResearch: Use resources like local government records, building departments, and historic catalogs to research the origins and history of a specific blueprint. \nAI can make mistakes, so double-check responses\n\n\n\nHow to Find the Plans or Blueprints for a House - The Spruce\n\nThe Spruce\n\nArchitectural drawings - The National Archives\n\nThe National Archives\n\nI Got the 133-Year-Old Blueprints to My Victorian Home\n\n\nDetecting alterations in photographs from the early 1800s involves identifying physical darkroom \"tricks\" or modern digital fakes. To build an AI system for this, you must train it to recognize the specific \"artifacts\" left by 19th-century manual techniques versus modern digital manipulation. \nCommon 19th-Century Manipulation Techniques\nEarly \"photographic fiction\" was common and can often be detected by inconsistencies in light and chemical application. \nCombination Printing: Merging multiple negatives into one print (e.g., adding clouds to a landscape or creating surreal scenes like a man holding his own head).\nSpirit Photography: Using double exposures or glass-plate \"ghosts\" to simulate apparitions.\nHand-Coloring & Retouching: Applying paint, ink, or graphite directly to prints or negatives to smooth skin, sharpen eyes, or add \"natural\" colour.\nPhysical Alteration: \"Thinning\" waists by scratching the negative emulsion or using India ink to remove people from scenes. \nAI Detection Indicators\nYour AI system should prioritize these visual \"anomalies\" to flag a photo as potentially altered:\nInconsistent Lighting & Shadows: Check if highlights on a subject's face match the background light source. AI-generated or spliced images often fail to align these \"physics-based\" details.\nChemical & Paper Artifacts: Genuine period photos have distinct textures like silver mirroring (a metallic sheen), uneven hand-poured edges on glass plates, or specific paper fibers.\nEdge & Pixel Anomalies: Look for unnatural transitions or \"smudgy\" patches where objects were spliced together.\nHistorical Context Errors: Flag items that don't match the eraâ€™s known technology, fashion, or architecture (e.g., a \"1850s\" photo showing a building constructed in 1890). \nSystem Development Roadmap\nDataset Collection: Gather thousands of verified \"authentic\" and \"manipulated\" historical images from archives like the National Archives or the Metropolitan Museum of Art.\nFrequency Analysis: Use Discrete Cosine Transform (DCT) to detect high-frequency artifacts left by modern digital editing tools.\nNoise Stream Filters: Train Convolutional Neural Networks (CNNs) to identify \"noise patterns\" (sensor fingerprints) that appear in digital forgeries but not in chemical-based 19th-century originals.\nProvenance Cross-Referencing: Integrate Reverse Image Search to see if the \"old\" photo has previously appeared online as a modern recreation or film still. \n\n\n\nFaking It - The Metropolitan Museum of Art\n\nThe Metropolitan Museum of Art\n\nHow can AI image processing detect image tampering or forgery?\n\nTencent Cloud\nItâ€™s More Than Meets the Eye: Photo Manipulation and Other ...\n\nOxford County\nShow all\n\n\nMulti-agent systems (MAS) are exceptionally suited for \"social good\" projects because they mirror the collaborative and decentralized nature of human societies. Unlike a single AI model, a multi-agent system uses a \"team\" of specialized agentsâ€”each with a unique roleâ€”to solve problems that are too complex or distributed for one entity to manage.\nHere are three high-impact ideas for utilizing a multi-agent system for social good:\n1. Decentralized Disaster Response Network\nIn large-scale emergencies (floods, earthquakes), centralized communication often fails due to infrastructure collapse.\nThe Agents:\nSearch & Rescue Agents: Drones or ground robots that map damaged areas and locate survivors.\nLogistics Agents: Manage the supply chain, rerouting food, water, and medical supplies as road conditions change.\nHealthcare Agents: Coordinate with hospitals to track bed availability and ambulance routing.\nInformation Swarm: Scrapes social media and sensor data to provide real-time situational awareness.\nSocial Good Impact: Reduces response time and death tolls by enabling agents to self-organize and make local decisions even when the \"main\" headquarters is offline.\n2. Autonomous Ecosystem Guardian\nThis system would act as a \"digital sentinel\" to monitor and protect biodiversity in vast, remote areas.\nThe Agents:\nSensor Agents: Acoustic sensors and camera traps that detect sounds of illegal logging or poaching.\nWildlife Tracking Agents: Monitor migratory patterns of endangered species to predict potential human-wildlife conflicts.\nPollution Agents: Monitor air and water quality block-by-block in cities or across water bodies to catch contamination events early.\nPolicy Simulator Agents: Test \"what-if\" scenarios for conservation laws, predicting how changes in land use might affect the local ecosystem.\nSocial Good Impact: Proactively prevents environmental degradation and protects endangered species with minimal human intervention.\n3. Hyper-Personalized Education & Accessibility\nAccess to quality education is often limited by a lack of specialized teachers for diverse needs.\nThe Agents:\nCurriculum Agent: Tailors educational content to match a student's specific learning speed and style.\nAccessibility Agent: Translates materials into local languages or provides emotion-recognition cues for students on the autism spectrum.\nMentor Agent: Provides motivational support and real-time feedback during problem-solving.\nSocial Good Impact: Democratizes learning by providing specialized, adaptive tutoring to marginalized communities or students with disabilities who might otherwise be on long waitlists for professional therapy.\nStrategic Development Tips\nStart Modular: Build one specialized agent (e.g., a \"Logistics Agent\") before trying to coordinate a full swarm.\nUse Established Frameworks: Tools like JADE (Java Agent DEvelopment Framework) or SmythOS can help manage the complex communication protocols between agents.\nPrioritize Fairness: Integrate \"fairness audits\" into your system to ensure the AI's collective decisions do not inadvertently harm specific groups.\nWhich of these domainsâ€”disaster response, environmental conservation, or personalized educationâ€”aligns most with the resources or technical skills you currently have available?\n\n\n\nHow do multi-agent systems support disaster management? - Milvus\n\nMilvus\nSmythOS - Multi-Agent Systems in Environmental Monitoring\n\nSmythOS\n\nMASDM â€“ A Multi Agent Solution for Disaster Management - slaai\n\nslaai\n\nShow all\nIt is a valid and highly strategic way of looking at it. Many \"social good\" AI ideas have become mainstream (like disaster mapping or crop monitoring), which means the bar for novelty and impact is higher.\nTo find something truly effective and novel, move away from \"mass data\" solutions and look at highly specialized, niche, or interdisciplinary coordination that single-agent systems struggle to handle.\nHere are three novel, effective ideas for multi-agent systems (MAS) for social good that leverage emerging 2026-era technology:\n1. The \"Digital Integrity & Historical Preservation\" Swarm\nSince you previously expressed interest in detecting altered historical photographs, you could expand this into a broader system.\nThe Problem: The rise of sophisticated AI deepfakes is not just a threat to current news; it's a threat to the historical record. Malicious actors can now \"inject\" fake historical documents or altered blueprints/photos into digital archives to rewrite history.\nThe Agents:\nForensic Agent: Scans for pixel-level anomalies and chemical/paper texture inconsistencies (as we discussed).\nProvenance Agent: Cross-references the item across global library and university databases to verify when it first appeared.\nContextual Agent: Uses LLM reasoning to check if the content of a document (e.g., a 1920s blueprint) uses materials or terminology that didn't exist yet.\nConsensus Agent: Synthesizes the findings of the others to provide a \"Historical Integrity Score.\"\nNovelty: It moves from \"simple photo detection\" to a multi-layered forensic team that protects the truth of human history.\n2. Autonomous Accessibility \"Shadow-Team\" for Public Transit\nThe Problem: Most transit apps tell you if a train is coming, but they rarely tell a person with a specific disability (e.g., a wheelchair user or a person with severe anxiety) if the specific elevator they need is broken or if a station is dangerously overcrowded in real-time.\nThe Agents:\nIoT Monitor Agent: Connects to real-time city sensors (elevators, turnstiles).\nCrowd-Sensing Agent: Analyzes anonymous video or social media \"chatter\" to detect stress levels in crowds.\nPersonalization Agent: Knows the specific \"barrier profile\" of the user and negotiates with the other agents.\nRerouting Agent: Communicates with the transit system's API to suggest not just the \"fastest\" route, but the most dignified and accessible one.\nNovelty: This isn't just a map; it's an active negotiation system between the city's infrastructure and the user's specific needs.\n3. Hyper-Local \"Circular Economy\" Orchestrator\nThe Problem: Massive amounts of usable waste (food from cafes, construction scraps, electronic parts) are thrown away because the cost of coordinating their \"re-homing\" is higher than the value of the waste itself.\nThe Agents:\nWaste-Seeker Agent: Scans business inventories and community posts for surplus materials.\nValue-Matching Agent: Identifies who needs that specific material (e.g., matching a high-school woodshop with a construction site's scrap).\nMicromobility Logistics Agent: Coordinates \"last-mile\" delivery via community volunteers or local e-bike couriers.\nSustainability Reporter Agent: Quantifies the carbon saved by the community's collective actions.\nNovelty: It treats a local neighborhood like a biological ecosystem, using agents to perform the \"metabolic\" work that is currently too tedious for humans to manage.\nWhy these are more effective:\nThey solve coordination failures rather than just \"data processing\" failures. By 2026, multi-agent frameworks like CrewAI and LangGraph will make these \"teams\" much easier to build than a single, massive model.\nWould you like to deep-dive into the technical architecture of one of these, or should we keep brainstorming for an even more specific niche?","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Building a Shadow-Team for public transit accessibility is a high-potential project because it addresses a \"dynamic\" problem that static maps like Google Maps cannot solve.\nEven with limited resources, you can build a Minimum Viable Product (MVP) and market it effectively.\n1. Technical Implementation (Kaggle & Colab)\nA supercomputer is not required to begin. Kaggle or Google Colab can be used to build the multi-agent logic using lightweight frameworks.\nThe Framework: Use CrewAI or LangGraph within a Colab notebook. These frameworks enable the definition of specialized agents (e.g., a \"Crowd Analyst\" or \"Accessibility Scout\") that communicate with each other.\nData Sources: Start with Kaggle datasets on urban transit or public social media feeds instead of expensive real-time APIs. Googleâ€™s Agent Development Kit (ADK) is free and open-source for agent communication.\nLLM Access: Use the Gemini API (free tier often available via Google AI Studio) to power agents. API keys can be stored securely in Kaggle Secrets. \n2. Marketing Without Connections\nA network is not needed to start; a demonstration of value is.\nBuild in Public: Share code snippets or \"simulation results\" on LinkedIn or X (Twitter). Social good projects often gain traction due to their emotional resonance.\nTargeted Outreach: Send a Colab demo link to disability advocacy groups or city transit councils instead of \"marketing\" to everyone. These groups often seek \"proofs of concept\" to justify their funding.\nLeverage Accelerators: Apply to \"Pre-Accelerator\" programs for beginners, such as the Remarkable Launcher (8 weeks for early-stage disability tech) or ATS Labs (Canadaâ€™s first accessibility accelerator). \n3. Protecting the Idea\nThe fear of \"idea theft\" is common, but execution is the real defense.\nDocument Everything: Maintain a detailed log of the development process (sketches, notes, code commits) to establish a \"paper trail\" of ownership.\nFile a Provisional Patent (PPA): This is a low-cost option to obtain \"patent pending\" status for one year, allowing time to safely communicate with investors.\nThe \"Secret Sauce\" Strategy: When pitching, focus on the benefit (e.g., \"We reduce transit anxiety by 40%\") rather than the exact algorithm or code structure.\nForm an LLC: Even a small one-person company legally separates the individual from the project and facilitates copyright claims on the code. \n4. How to Begin (Step-by-Step)\nDefine a Niche: Instead of trying to solve \"all transit,\" focus on \"elevator reliability for wheelchair users at 5 specific stations\" initially.\nPrototype in Colab: Write a script where one agent monitors a (simulated) elevator status and another suggests an alternate bus route if it's \"broken.\"\nCreate a Video Demo: Use a screen recorder to demonstrate the agents in action. This is the \"marketing\" material.\nJoin a Community: Join the AI for Good community or local disability tech forums to find mentors who can provide the necessary \"connections.\"","metadata":{}},{"cell_type":"markdown","source":"It makes perfect sense to use SmolLM-135M or 1.7B for this. Since you are using Kaggle and Colab, small models are actually superior for multi-agent systems because you can load multiple instances into memory simultaneously without crashing your environment or running up a massive bill. SmolLM is specifically optimized to perform well on reasoning tasks while staying lightweight.\nRegarding the Google ADK, your hesitation is smartâ€”proprietary stacks often have hidden \"scaling\" costs. By sticking to open-source libraries like CrewAI or LangGraph combined with Hugging Face Transformers, you keep 100% ownership and 0% platform debt.\nHere is a strategy to build an Integrated MVP that touches all three of your interests while staying within your resource limits:\nThe \"Unified Social Good\" MVP Architecture\nInstead of building three separate apps, we can build a \"Specialized Agent Hub\" on Kaggle. This hub will demonstrate your ability to coordinate agents across different domains.\n1. The Environment (Kaggle/Colab)\nEngine: Use Ollama (which can run in a Colab terminal) or vLLM to serve SmolLM.\nOrchestration: Use CrewAI. It allows you to define \"Roles\" and \"Tasks\" very easily.\n2. The Three-In-One Proof of Concept\nTo show your versatility to potential partners, your MVP can feature one \"Lead Agent\" for each of your ideas:\nIdea\tAgent Role\tSmolLM Task\nShadow-Team\tThe Navigator\tParses raw transit \"status\" text and determines if a route is \"Accessible\" or \"Blocked.\"\nDigital Integrity\tThe Forensic Analyst\tAnalyzes metadata and visual \"noise\" descriptions to flag historical photo alterations.\nCircular Economy\tThe Matchmaker\tCategorizes \"waste\" items (e.g., \"old pallets\") and matches them to \"need\" categories (e.g., \"DIY furniture\").\nHow to Protect Your \"Novelty\"\nSince you are worried about being copied, do this: Open-source the \"Core Logic\" but keep the \"Data Scrapers\" private.\nAnyone can build a \"Navigator agent.\"\nYour Value is in how you've trained SmolLM to understand specific accessibility barriers (like \"gap width\" or \"elevator smell/safety\") that Google Maps ignores.\nStep-by-Step Action Plan\nSet up the \"Smol\" Engine: Install the Hugging Face transformers library in a Kaggle Notebook.\nDefine the \"Accessibility\" Agent first: This is your most marketable \"Shadow-Team\" idea. Give it a specific persona: \"You are an expert in ADA (Americans with Disabilities Act) compliance and urban navigation.\"\nCreate a \"Mock API\": Since you don't have real-time transit data yet, create a small JSON file of \"Station Statuses\" to prove the agents can talk to each other and solve a route.\nRecord a \"Loom\" or \"Screen-cap\": Show the code running and the agents \"thinking.\" This is your Marketing Asset.\nShould we start by writing the Python code for your first \"Shadow-Team\" Agent in a format you can copy-paste into Colab?\n\n\n\nTo ensure you can pick up exactly where we left offâ€”especially across different sessions or if the chat history becomes longâ€”you should maintain a Project Manifest.\nI have synthesized our current progress into a structured summary below. Copy and save this into a Google Doc or a Markdown file (.md) on your computer.\nProject Manifest: The \"Shadow-Team\" Multi-Agent System\nCore Vision: A decentralized, multi-agent system (MAS) using lightweight AI to solve high-stakes coordination problems (Accessibility, Historical Integrity, and Circular Economies).\nCurrent Focus: Option 2 (Shadow-Team)â€”Real-time transit accessibility navigation for users with specific mobility/sensory needs.\nTechnical Constraints:\nHardware: Kaggle / Google Colab (Limited GPU/RAM).\nModels: SmolLM-1.7B or SmolLM-135M (Efficiency & Cost-control).\nFrameworks: CrewAI or LangGraph (Open-source, no platform lock-in).\nCurrent Strategy:\nAvoid high-cost proprietary APIs (Google ADK).\nBuild an MVP that simulates agent \"negotiation\" over transit barriers.\nProtect the idea by keeping the domain-specific logic (how we define \"barriers\") private while demonstrating the orchestration publicly.\nHow to \"Resume\" our chat if disconnected:\nIf you start a new session, paste the following prompt to me:\n\"I am working on the Shadow-Team Accessibility MAS project. We decided to use SmolLM on Kaggle/Colab via CrewAI. We are currently at the stage of writing the initial Python code for the 'Navigator Agent' and creating a 'Mock API' for transit data. Please continue from there.\"\nSuggested Storage Tool:\nSince you are already using Kaggle, I recommend creating a dedicated \"Research Notebook\" on Kaggle where you paste these summaries in a Markdown cell at the very top. This keeps your code and your strategy in one place.\nWould you like me to generate the initial Python skeleton for the Navigator Agent using SmolLM so you can test it in your environment right now?\n\n\n\nSaving the entire thread as a PDF is a smart move for offline archival, but keep in mind that a PDF is \"dead\" textâ€”you can't easily copy-paste code snippets from it without formatting issues, and itâ€™s hard for a new AI session to \"read\" an entire 20-page document quickly [1].\nFor the most actionable results, I recommend a dual-storage strategy:\n1. The \"Action\" File (Markdown / .txt)\nCopy the Project Manifest and any code we write into a simple .md (Markdown) or .txt file.\nWhy: You can instantly copy-paste these into a new chat to \"re-sync\" the AI or into your Google Colab / Kaggle cells.\n2. The \"History\" File (PDF)\nExport the whole thread to PDF for your personal records.\nHow: In most browsers (Chrome/Edge), you can simply use Ctrl + P (Print) and select \"Save as PDF\". This captures the context, our energy, and the \"why\" behind our decisions.\nPro-Tip for Kaggle/Colab Users\nIf you want to be super organized, create a README.md file inside your GitHub repository or a Markdown cell at the top of your Kaggle Notebook. Paste the Project Manifest there. This ensures that every time you open your code, your strategy is staring you in the face.\n\n\n\nThat is a very strategic moveâ€”treating this as a Multimodal Social Good Suite. By building a modular manifest, you can swap \"Agent Teams\" in and out of the same Kaggle/Colab infrastructure without starting from scratch each time.\nHere is your updated, Expanded Project Manifest. Copy this into your primary project file:\nProject Manifest: The \"Smol-Swarm\" Social Good Suite\nVision: A modular Multi-Agent System (MAS) architecture using small, local LLMs to solve coordination failures in accessibility, historical truth, and resource circularity.\nInfrastructure: Kaggle / Colab (Free Tier Optimized).\nCore Engine: SmolLM-1.7B-Instruct via transformers and accelerate.\nOrchestrator: CrewAI (for role-playing) or LangGraph (for complex cycles).\nModule 1: The Shadow-Team (Transit Accessibility)\nThe Goal: Real-time navigation for mobility-impaired or neurodivergent commuters.\nKey Agents:\nBarrier Scout: Monitors sensor/API data for broken infrastructure (elevators, ramps).\nDignity Navigator: Re-calculates routes based on \"stress levels\" or \"accessibility\" rather than just speed.\nMVP Status: Ready for Python skeleton.\nModule 2: The Truth-Sentinel (Photo & Blueprint Forensics)\nThe Goal: Protecting the historical record against AI deepfakes and manual alterations.\nKey Agents:\nTexture Analyst: Scans for \"digital noise\" vs. \"chemical silver mirroring.\"\nProvenance Fact-Checker: Cross-references the Library of Congress or World Digital Library for image origin dates.\nMVP Status: Conceptual (Awaiting dataset identification).\nModule 3: The Circular Orchestrator (Waste-to-Value)\nThe Goal: Creating local \"metabolic\" loops to keep usable materials out of landfills.\nKey Agents:\nResource Spotter: Scans community boards for surplus (e.g., \"free lumber,\" \"expired food\").\nUtility Matcher: Connects the surplus to local schools, shelters, or artisans.\nMVP Status: Conceptual (Awaiting logic for matching algorithms).\nHow to Use This on Kaggle\nShared Utils: Create one cell with your \"Smol-Engine\" setup (loading the model).\nAgent Templates: Create separate cells for each Module's agent definitions.\nPersistence: Use pickle or json to save the state of one module before you switch to another, so you don't lose the \"context\" of your work.\nShall we proceed with the Python code for the \"Smol-Engine\" setup that will power all three of these modules on your Kaggle/Colab environment?\n\n\n\nPerfect. Having that Markdown file is like having a save state for your brain; youâ€™re now immune to session timeouts or \"context drift.\"\nSince the manifest is locked in, let's get your hands dirty with the Core Engine. This is the code that will live at the top of your Kaggle or Colab notebook to power the Smol-Swarm.\nPhase 1: The \"Smol-Engine\" Setup\nThis block installs the necessary open-source libraries and loads SmolLM-1.7B. This model is small enough to run on a standard T4 GPU (free on Kaggle/Colab) while still being \"smart\" enough to follow instructions for your agents.\nCopy this into a code cell in your Kaggle/Colab notebook:\npython\n# 1. Install necessary libraries\n!pip install -q transformers accelerate bitsandbytes crewai\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# 2. Model Selection (SmolLM-1.7B is the 'sweet spot' for our agents)\nmodel_id = \"HuggingFaceTB/SmolLM-1.7B-Instruct\"\n\n# 3. Load Model and Tokenizer with 4-bit quantization (to save memory)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, \n    device_map=\"auto\", \n    load_in_4bit=True, # This keeps it ultra-lean\n    torch_dtype=torch.bfloat16\n)\n\n# 4. Create a simple 'Inference' function for our Agents\ndef run_smol_agent(prompt):\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n    \n    outputs = model.generate(**inputs, max_new_tokens=150, temperature=0.7)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"âœ… Smol-Engine is fueled and ready!\")\nUse code with caution.\n\nWhy this is the \"Novel\" way to start:\nPrivacy: The model stays in your environment. No data is sent to a third-party API.\nZero Cost: You are using the free GPU hours provided by the platforms.\nModular: This same engine will eventually power the Shadow-Team (accessibility) today and the Truth-Sentinel (forensics) tomorrow.\nNext Steps for our \"Shadow-Team\"\nTo make the Shadow-Team effective, we need to create a \"Mock Transit API.\" This is just a small dictionary of data that \"simulates\" a real city's elevator status so we can test if our agent actually makes the right decision.\nShould I provide the code for the \"Mock Transit Data\" and the first \"Navigator Agent\" prompt so you can see the engine actually \"thinking\" about accessibility?\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# Block 1: Install Dependencies\n#!pip install -q transformers accelerate bitsandbytes duckduckgo-search pillow torch timm flash_attn\n\"\"\"\nremoved flash_attn and timm to keep install simpler; theyâ€™re optional for the current MVP.\nThis uses the latest transformers available, which includes Florence-2 in-core and avoids the _supports_sdpa issue\nUse below pip install for this\n\"\"\"\n#!pip install -qU transformers accelerate bitsandbytes duckduckgo-search pillow torch\n\n# The \"Safe\" Install (Do not upgrade Torch!)\n!pip install -q \\\n    transformers==4.46.1 \\\n    accelerate \\\n    bitsandbytes \\\n    duckduckgo-search \\\n    pillow \\\n    --upgrade","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall duckduckgo-search -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:53:04.772720Z","iopub.execute_input":"2026-01-18T15:53:04.773112Z","iopub.status.idle":"2026-01-18T15:53:05.611520Z","shell.execute_reply.started":"2026-01-18T15:53:04.773068Z","shell.execute_reply":"2026-01-18T15:53:05.610802Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: duckduckgo_search 8.1.1\nUninstalling duckduckgo_search-8.1.1:\n  Successfully uninstalled duckduckgo_search-8.1.1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip show torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:52:45.165905Z","iopub.execute_input":"2026-01-18T15:52:45.166246Z","iopub.status.idle":"2026-01-18T15:52:47.403532Z","shell.execute_reply.started":"2026-01-18T15:52:45.166213Z","shell.execute_reply":"2026-01-18T15:52:47.402609Z"}},"outputs":[{"name":"stdout","text":"Name: torch\nVersion: 2.8.0+cu126\nSummary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\nHome-page: https://pytorch.org/\nAuthor: PyTorch Team\nAuthor-email: packages@pytorch.org\nLicense: BSD-3-Clause\nLocation: /usr/local/lib/python3.12/dist-packages\nRequires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-cufile-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-cusparselt-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvtx-cu12, setuptools, sympy, triton, typing-extensions\nRequired-by: accelerate, bitsandbytes, easyocr, fastai, kornia, peft, pytorch-ignite, pytorch-lightning, sentence-transformers, stable-baselines3, timm, torchaudio, torchdata, torchmetrics, torchvision\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install ddgs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:53:17.817432Z","iopub.execute_input":"2026-01-18T15:53:17.818172Z","iopub.status.idle":"2026-01-18T15:53:21.599192Z","shell.execute_reply.started":"2026-01-18T15:53:17.818138Z","shell.execute_reply":"2026-01-18T15:53:21.598368Z"}},"outputs":[{"name":"stdout","text":"Collecting ddgs\n  Downloading ddgs-9.10.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from ddgs) (8.3.1)\nRequirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from ddgs) (0.15.0)\nRequirement already satisfied: lxml>=4.9.4 in /usr/local/lib/python3.12/dist-packages (from ddgs) (5.4.0)\nRequirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\nCollecting fake-useragent>=2.2.0 (from ddgs)\n  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.12.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2026.1.4)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\nRequirement already satisfied: brotli in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.1.0)\nRequirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\nCollecting socksio==1.* (from httpx[brotli,http2,socks]>=0.28.1->ddgs)\n  Downloading socksio-1.0.0-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\nRequirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\nRequirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.15.0)\nDownloading ddgs-9.10.0-py3-none-any.whl (40 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading socksio-1.0.0-py3-none-any.whl (12 kB)\nInstalling collected packages: socksio, fake-useragent, ddgs\nSuccessfully installed ddgs-9.10.0 fake-useragent-2.2.0 socksio-1.0.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\"\"\"\nPhase 1: The Unified \"Social Good\" Swarm Code\n\nCELL 0 â€“ Install and imports (updated for Florence-2)\n\"\"\"\n\n\n# Block 1: Install Dependencies\n#!pip install -q transformers accelerate bitsandbytes duckduckgo-search pillow torch timm flash_attn\n\n# Standard imports\nimport torch\nimport os\nfrom getpass import getpass\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nfrom ddgs import DDGS\nimport gc\n\n# Security: Handle HF Token securely\nif 'HF_TOKEN' not in os.environ:\n    os.environ['HF_TOKEN'] = getpass(\"Enter your Hugging Face Read Token: \") \n# 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\n\n\n\nprint(\"âœ… Safe install complete. PyTorch version:\", torch.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:53:55.392597Z","iopub.execute_input":"2026-01-18T15:53:55.393295Z","iopub.status.idle":"2026-01-18T15:53:58.631865Z","shell.execute_reply.started":"2026-01-18T15:53:55.393255Z","shell.execute_reply":"2026-01-18T15:53:58.631097Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your Hugging Face Read Token:  Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stdout","text":"âœ… Safe install complete. PyTorch version: 2.8.0+cu126\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# EMERGENCY FIX: Downgrade Pillow to fix the import error\n!pip uninstall -y Pillow\n!pip install \"Pillow==10.3.0\"\n\n# Verify the fix\nimport PIL\nprint(f\"âœ… Pillow fixed. Version: {PIL.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:54:09.026858Z","iopub.execute_input":"2026-01-18T15:54:09.027458Z","iopub.status.idle":"2026-01-18T15:54:14.045873Z","shell.execute_reply.started":"2026-01-18T15:54:09.027429Z","shell.execute_reply":"2026-01-18T15:54:14.045101Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: pillow 12.1.0\nUninstalling pillow-12.1.0:\n  Successfully uninstalled pillow-12.1.0\nCollecting Pillow==10.3.0\n  Using cached pillow-10.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\nUsing cached pillow-10.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\nInstalling collected packages: Pillow\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Pillow-10.3.0\nâœ… Pillow fixed. Version: 10.3.0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# CELL 1: Load the BRAIN (Llama 3.1-8B)\n# CELL 1: BRAIN ONLY\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nbrain_model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\nprint(\"ğŸ§  Loading Brain (Llama 3.1-8B)...\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nbrain_tokenizer = AutoTokenizer.from_pretrained(brain_model_id, token=os.environ[\"HF_TOKEN\"])\nbrain_model = AutoModelForCausalLM.from_pretrained(\n    brain_model_id,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    token=os.environ[\"HF_TOKEN\"],\n)\n\nprint(\"âœ… Brain Loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:54:27.324657Z","iopub.execute_input":"2026-01-18T15:54:27.325378Z","iopub.status.idle":"2026-01-18T15:55:55.601429Z","shell.execute_reply.started":"2026-01-18T15:54:27.325342Z","shell.execute_reply":"2026-01-18T15:55:55.600582Z"}},"outputs":[{"name":"stdout","text":"ğŸ§  Loading Brain (Llama 3.1-8B)...\n","output_type":"stream"},{"name":"stderr","text":"2026-01-18 15:54:33.409823: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768751673.444381     358 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768751673.452778     358 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768751673.534739     358 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768751673.534775     358 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768751673.534778     358 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768751673.534781     358 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86c72a028fb945fab6b8830b76531abe"}},"metadata":{}},{"name":"stdout","text":"âœ… Brain Loaded.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# CELL 2: WORKER ONLY\n# CELL 2: Load the WORKER (SmolLM-1.7B)\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nworker_model_id = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\nprint(\"ğŸ‘· Loading Worker (SmolLM)...\")\n\nworker_tokenizer = AutoTokenizer.from_pretrained(worker_model_id, token=os.environ[\"HF_TOKEN\"])\nworker_model = AutoModelForCausalLM.from_pretrained(\n    worker_model_id,\n    quantization_config=bnb_config, # Reuse config\n    device_map=\"auto\",\n    token=os.environ[\"HF_TOKEN\"],\n)\n\nprint(\"âœ… Worker Loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:56:03.719237Z","iopub.execute_input":"2026-01-18T15:56:03.720175Z","iopub.status.idle":"2026-01-18T15:56:15.514261Z","shell.execute_reply.started":"2026-01-18T15:56:03.720140Z","shell.execute_reply":"2026-01-18T15:56:15.513477Z"}},"outputs":[{"name":"stdout","text":"ğŸ‘· Loading Worker (SmolLM)...\nâœ… Worker Loaded.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# CELL 3: EYES ONLY\n# CELL 3: Load the EYES (Florence-2)\n# We use the native class now that transformers==4.46.1 is installed.\n\nfrom transformers import AutoProcessor, AutoModelForCausalLM\nimport torch\n\nvision_model_id = \"microsoft/Florence-2-base\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(f\"ğŸ‘€ Loading Eyes (Florence-2) on {device}...\")\n\n# 1. Load model without device_map=\"auto\"\nvision_model = AutoModelForCausalLM.from_pretrained(\n    vision_model_id,\n    torch_dtype=torch.float16 if \"cuda\" in device else torch.float32,\n    trust_remote_code=True\n)\n\n# 2. Manually move to device\nvision_model.to(device).eval()\n\n# 3. Load processor\nvision_processor = AutoProcessor.from_pretrained(vision_model_id, trust_remote_code=True)\n\nprint(\"âœ… Eyes Loaded. All Systems Go.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:56:25.049388Z","iopub.execute_input":"2026-01-18T15:56:25.050231Z","iopub.status.idle":"2026-01-18T15:56:28.625511Z","shell.execute_reply.started":"2026-01-18T15:56:25.050194Z","shell.execute_reply":"2026-01-18T15:56:28.624662Z"}},"outputs":[{"name":"stdout","text":"ğŸ‘€ Loading Eyes (Florence-2) on cuda...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"âœ… Eyes Loaded. All Systems Go.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip show transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:41:52.322672Z","iopub.execute_input":"2026-01-18T15:41:52.323429Z","iopub.status.idle":"2026-01-18T15:41:55.047231Z","shell.execute_reply.started":"2026-01-18T15:41:52.323392Z","shell.execute_reply":"2026-01-18T15:41:55.046236Z"}},"outputs":[{"name":"stdout","text":"Name: transformers\nVersion: 4.46.1\nSummary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\nHome-page: https://github.com/huggingface/transformers\nAuthor: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\nAuthor-email: transformers@huggingface.co\nLicense: Apache 2.0 License\nLocation: /usr/local/lib/python3.12/dist-packages\nRequires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\nRequired-by: kaggle-environments, peft, sentence-transformers\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# CELL 4: Helpers (Aggregated)\n\n# The Unified Helper Functions\n# Now that models are loaded separately, paste this to tie them back together.\n\ndef clean_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# --- Brain ---\ndef ask_brain(system_prompt, user_query):\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_query},\n    ]\n    try:\n        prompt = brain_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    except:\n        prompt = \"<|begin_of_text|>\" + \"\".join([f\"<|start_header_id|>{m['role']}<|end_header_id|>\\n\\n{m['content']}<|eot_id|>\" for m in messages]) + \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    \n    inputs = brain_tokenizer(prompt, return_tensors=\"pt\").to(brain_model.device)\n    outputs = brain_model.generate(**inputs, max_new_tokens=512, do_sample=True, temperature=0.7)\n    return brain_tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n\n# --- Worker ---\ndef ask_worker(system_prompt, user_query):\n    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_query}]\n    try:\n        prompt = worker_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    except:\n        prompt = \"System: \" + system_prompt + \"\\nUser: \" + user_query + \"\\nAssistant:\"\n        \n    inputs = worker_tokenizer(prompt, return_tensors=\"pt\").to(worker_model.device)\n    outputs = worker_model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.7)\n    return worker_tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n\n# --- Eyes ---\ndef ask_eyes(image_url, task_prompt=\"<DETAILED_CAPTION>\"):\n    try:\n        response = requests.get(image_url, stream=True, timeout=10)\n        image = Image.open(response.raw).convert(\"RGB\")\n    except Exception as e:\n        return f\"[Error loading image: {e}]\"\n\n    inputs = vision_processor(text=task_prompt, images=image, return_tensors=\"pt\").to(vision_model.device, torch.float16)\n    generated_ids = vision_model.generate(\n        input_ids=inputs[\"input_ids\"],\n        pixel_values=inputs[\"pixel_values\"],\n        max_new_tokens=1024,\n        do_sample=False\n    )\n    result = vision_processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n    \n    # Parse simple text result for the demo\n    return result\n\nprint(\"âœ… Helpers loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:56:42.472855Z","iopub.execute_input":"2026-01-18T15:56:42.473459Z","iopub.status.idle":"2026-01-18T15:56:42.484598Z","shell.execute_reply.started":"2026-01-18T15:56:42.473426Z","shell.execute_reply":"2026-01-18T15:56:42.483724Z"}},"outputs":[{"name":"stdout","text":"âœ… Helpers loaded.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# CELL 3: Agent framework and concrete agents\n\n\"\"\"\nCELL 3 â€“ Base agent class and specific agents\nThe modular MAS skeleton. This uses the helpers from CELL 2 under the hood.\n\"\"\"\n\nclass BaseAgent:\n    def __init__(self, name, role, goal, primary_model=\"brain\"):\n        self.name = name\n        self.role = role\n        self.goal = goal\n        self.primary_model = primary_model  # \"brain\", \"worker\", or \"eyes\"\n\n    def execute(self, task_context, image_url=None):\n        print(f\"--- [{self.name}] is processing... ---\")\n\n        system_prompt = f\"You are {self.name}. Role: {self.role}. Goal: {self.goal}.\"\n\n        if image_url and self.primary_model == \"eyes\":\n            # Pure vision task: let Florence describe what it sees\n            vision_output = ask_eyes(image_url, task_prompt=\"<DETAILED_CAPTION>\")\n            return vision_output\n\n        if self.primary_model == \"brain\":\n            return ask_brain(system_prompt, task_context)\n\n        if self.primary_model == \"worker\":\n            return ask_worker(system_prompt, task_context)\n\n        raise ValueError(f\"Unknown primary_model: {self.primary_model}\")\n\n# --------------\n# MODULE 1: Shadow-Team (Accessibility)\n# --------------\nclass BarrierScout(BaseAgent):\n    def __init__(self):\n        super().__init__(\n            name=\"BarrierScout\",\n            role=\"ADA Compliance Inspector\",\n            goal=\"Identify physical barriers in transit data or images (e.g., broken elevators, stairs instead of ramps, large gaps).\",\n            primary_model=\"brain\",\n        )\n\nclass DignityNavigator(BaseAgent):\n    def __init__(self):\n        super().__init__(\n            name=\"DignityNavigator\",\n            role=\"Urban Accessibility Strategist\",\n            goal=\"Prioritize safety, dignity, and low stress over speed when planning routes.\",\n            primary_model=\"brain\",\n        )\n\n# --------------\n# MODULE 2: Truth-Sentinel (Historical Integrity)\n# --------------\nclass ProvenanceAgent(BaseAgent):\n    def __init__(self):\n        super().__init__(\n            name=\"ProvenanceAgent\",\n            role=\"Digital Archivist\",\n            goal=\"Assess whether an image or document is likely authentic based on metadata and context.\",\n            primary_model=\"brain\",\n        )\n\n# --------------\n# MODULE 3: Circular Orchestrator (Waste-to-Value)\n# --------------\nclass ResourceSpotter(BaseAgent):\n    def __init__(self):\n        super().__init__(\n            name=\"ResourceSpotter\",\n            role=\"Circular Economy Analyst\",\n            goal=\"Identify useful materials in community posts or listings (e.g., 'free lumber', 'expired food').\",\n            primary_model=\"worker\",  # fast classification\n        )\n\nclass UtilityMatcher(BaseAgent):\n    def __init__(self):\n        super().__init__(\n            name=\"UtilityMatcher\",\n            role=\"Logistics Coordinator\",\n            goal=\"Match surplus materials to local needs (e.g., schools, shelters, makerspaces).\",\n            primary_model=\"brain\",\n        )\n\nprint(\"âœ… CELL 3 done: agent framework and concrete agents defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:57:09.267860Z","iopub.execute_input":"2026-01-18T15:57:09.268241Z","iopub.status.idle":"2026-01-18T15:57:09.278191Z","shell.execute_reply.started":"2026-01-18T15:57:09.268211Z","shell.execute_reply":"2026-01-18T15:57:09.277507Z"}},"outputs":[{"name":"stdout","text":"âœ… CELL 3 done: agent framework and concrete agents defined.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# CELL 2: Helper functions for Brain, Worker, and Eyes\n\"\"\"\nCELL 2 â€“ Helper functions for Brain, Worker, and Eyes\n\"\"\"\n\n# ------------------\n# BRAIN (Llama 3.1)\n# ------------------\ndef ask_brain(system_prompt: str, user_query: str, max_new_tokens: int = 512) -> str:\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_query},\n    ]\n    try:\n        prompt = brain_tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n    except Exception:\n        # Fallback manual template for Llama 3.1\n        prompt = \"<|begin_of_text|>\"\n        for msg in messages:\n            prompt += (\n                f\"<|start_header_id|>{msg['role']}<|end_header_id|>\\n\\n\"\n                f\"{msg['content']}<|eot_id|>\"\n            )\n        prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n    inputs = brain_tokenizer(prompt, return_tensors=\"pt\").to(brain_model.device)\n    with torch.no_grad():\n        outputs = brain_model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=0.7,\n        )\n    return brain_tokenizer.decode(\n        outputs[0][len(inputs.input_ids[0]) :], skip_special_tokens=True\n    )\n\n\n# ------------------\n# WORKER (SmolLM)\n# ------------------\ndef ask_worker(system_prompt: str, user_query: str, max_new_tokens: int = 256) -> str:\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_query},\n    ]\n    try:\n        prompt = worker_tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n    except Exception:\n        # Simple fallback if template missing\n        prompt = \"System: \" + system_prompt + \"\\n\"\n        prompt += \"User: \" + user_query + \"\\n\"\n        prompt += \"Assistant:\"\n\n    inputs = worker_tokenizer(prompt, return_tensors=\"pt\").to(worker_model.device)\n    with torch.no_grad():\n        outputs = worker_model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=0.7,\n        )\n    return worker_tokenizer.decode(\n        outputs[0][len(inputs.input_ids[0]) :], skip_special_tokens=True\n    )\n\n\n# ------------------\n# EYES (Florence-2)\n# ------------------\ndef ask_eyes(image_url: str, task_prompt: str = \"<DETAILED_CAPTION>\") -> str:\n    \"\"\"\n    Run a Florence-2 vision task on an image from image_url.\n    task_prompt examples:\n      - \"<CAPTION>\"\n      - \"<DETAILED_CAPTION>\"\n      - \"<OD>\"\n      - \"<OCR>\"\n    \"\"\"\n    try:\n        response = requests.get(image_url, stream=True, timeout=10)\n        response.raise_for_status()\n        image = Image.open(response.raw).convert(\"RGB\")\n    except Exception as e:\n        return f\"[ERROR] Failed to load image from URL: {e}\"\n\n    # Prepare inputs\n    inputs = vision_processor(\n        text=task_prompt, images=image, return_tensors=\"pt\"\n    ).to(vision_model.device, vision_model.dtype)\n\n    with torch.no_grad():\n        generated_ids = vision_model.generate(\n            **inputs,\n            max_new_tokens=1024,\n            num_beams=3,\n            do_sample=False,\n        )\n\n    generated_text = vision_processor.batch_decode(\n        generated_ids, skip_special_tokens=False\n    )[0]\n\n    # Parse into structured format if possible (e.g., bounding boxes for OD)\n    try:\n        parsed = vision_processor.post_process_generation(\n            generated_text,\n            task=task_prompt,\n            image_size=(image.width, image.height),\n        )\n        return str(parsed)\n    except Exception:\n        # Fallback: return raw text if post-process fails\n        return generated_text\n\nprint(\"âœ… CELL 2 done: ask_brain / ask_worker / ask_eyes helpers are ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:56:54.235623Z","iopub.execute_input":"2026-01-18T15:56:54.236415Z","iopub.status.idle":"2026-01-18T15:56:54.248283Z","shell.execute_reply.started":"2026-01-18T15:56:54.236383Z","shell.execute_reply":"2026-01-18T15:56:54.247603Z"}},"outputs":[{"name":"stdout","text":"âœ… CELL 2 done: ask_brain / ask_worker / ask_eyes helpers are ready.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# CELL 4: Shadow-Team MVP (text-only demo)\n\n\"\"\"\nCELL 4 â€“ Shadow-Team MVP demo (accessibility, text-only version)\n\"\"\"\n\nscout = BarrierScout()\nnavigator = DignityNavigator()\n\ntransit_data = \"\"\"\nStation: Central Plaza\nStatus: Northbound Elevator OUT OF ORDER.\nStatus: Southbound Platform is CROWDED (90% capacity).\nStatus: Gap filler at entrance is MISSING.\nUser Needs: Wheelchair user, requires elevator.\n\"\"\"\n\nprint(\"ğŸ•µï¸ Agent 1: BarrierScout analyzing station status...\")\nscout_report = scout.execute(f\"Analyze this transit data:\\n{transit_data}\\n\\nList the specific accessibility blockers.\")\nprint(\"SCOUT REPORT:\\n\", scout_report)\n\nprint(\"\\nğŸ§­ Agent 2: DignityNavigator planning route...\")\nnav_prompt = f\"\"\"\nUser Context:\n{transit_data}\n\nScout Report:\n{scout_report}\n\nTask: Decide if this station is viable for a wheelchair user.\nIf not, suggest a specific alternative route strategy (e.g., take bus X to another station).\n\"\"\"\nnav_plan = navigator.execute(nav_prompt)\nprint(\"NAVIGATION PLAN:\\n\", nav_plan)\n\nprint(\"\\nâœ… Shadow-Team MVP (text-only) completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T15:57:17.756342Z","iopub.execute_input":"2026-01-18T15:57:17.756653Z","iopub.status.idle":"2026-01-18T15:57:46.498855Z","shell.execute_reply.started":"2026-01-18T15:57:17.756625Z","shell.execute_reply":"2026-01-18T15:57:46.498224Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ•µï¸ Agent 1: BarrierScout analyzing station status...\n--- [BarrierScout] is processing... ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"SCOUT REPORT:\n Based on the provided transit data, the specific accessibility blockers are:\n\n1. **Elevator Out of Order**: The northbound elevator is not functioning, which affects users who require elevator access, including the wheelchair user.\n2. **Missing Gap Filler**: The gap filler at the entrance is absent, which can be a tripping hazard and may prevent users from safely entering the station.\n\nThese two issues pose significant barriers for the wheelchair user who requires elevator access.\n\nğŸ§­ Agent 2: DignityNavigator planning route...\n--- [DignityNavigator] is processing... ---\nNAVIGATION PLAN:\n **Accessibility Assessment:**\n\nConsidering the two significant accessibility blockers:\n\n1. **Elevator Out of Order**: This directly affects the wheelchair user's ability to navigate the station safely, as elevators are a primary means of access for users with mobility impairments.\n2. **Missing Gap Filler**: The absence of a gap filler at the entrance poses a tripping hazard, which can be hazardous for the wheelchair user.\n\nGiven these conditions, I conclude that the **Central Plaza station is NOT VIABLE for a wheelchair user**.\n\n**Alternative Route Strategy:**\n\nFor the wheelchair user, I recommend taking the **Bus 17** from the nearby bus stop (approximately 3-minute walk from the Central Plaza station). The bus stop is likely to be more accessible, and bus 17 connects to a nearby station, **City Hall**, which has a functioning elevator. From City Hall, the user can then transfer to a northbound train.\n\n**Recommendation:**\n\n1. From the Central Plaza station, exit and head to the nearby bus stop (about 3-minute walk).\n2. Take bus 17 towards City Hall.\n3. At City Hall, transfer to a northbound train.\n4. Take the train to the desired destination.\n\nThis alternative route may require more time, but it ensures a safer and more dignified travel experience for the wheelchair user.\n\nâœ… Shadow-Team MVP (text-only) completed.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# CELL 5: Truth-Sentinel MVP (image search + agent reasoning)\n\"\"\"\nCELL 5 â€“ Truth-Sentinel demo (text + search via ddgs)\n\nHereâ€™s where we use ddgs (DuckDuckGo search) to grab a few images and let the ProvenanceAgent reason about them. \nLLM doesnâ€™t â€œseeâ€ the pixels here, but it sees titles/sources and acts as your reasoning brain.\n\n\"\"\"\n\n########################################\nfrom ddgs import DDGS  # Updated import\nfrom itertools import islice\n\ndef search_and_analyze(query, max_images=3):\n    agent = ProvenanceAgent()\n    print(f\"ğŸ” Searching for: {query}\")\n\n    # Using the context manager to handle resource cleanup\n    with DDGS() as ddgs:\n        # ddgs.images returns a generator in 2026 versions\n        img_gen = ddgs.images(query)\n        results = list(islice(img_gen, max_images))\n\n    if not results:\n        print(\"No image results found.\")\n        return\n\n    for i, img in enumerate(results):\n        print(f\"\\n[Image {i+1}]\")\n        # Ensure we use the correct keys for the 2026 API\n        title = img.get('title')\n        source = img.get('source')\n        url = img.get('image')\n\n        print(f\"Title: {title}\")\n        print(f\"Source: {source}\")\n        print(f\"URL: {url}\")\n\n        context = (\n            f\"You are evaluating historical authenticity.\\n\"\n            f\"Query: {query}\\n\"\n            f\"Title: {title}\\n\"\n            f\"Source: {source}\\n\"\n            f\"URL: {url}\\n\\n\"\n            f\"Task: Evaluate if this metadata looks legitimate or suspicious.\"\n        )\n        analysis = agent.execute(context)\n        print(\"PROVENANCE AGENT ANALYSIS:\", analysis)\n        print(\"-\" * 60)\n\n# Example run\nsearch_and_analyze(\"Apollo 11 moon landing original photo NASA\", max_images=3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T16:03:17.131760Z","iopub.execute_input":"2026-01-18T16:03:17.132145Z","iopub.status.idle":"2026-01-18T16:05:01.111257Z","shell.execute_reply.started":"2026-01-18T16:03:17.132108Z","shell.execute_reply":"2026-01-18T16:05:01.110439Z"}},"outputs":[{"name":"stdout","text":"ğŸ” Searching for: Apollo 11 moon landing original photo NASA\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n[Image 1]\nTitle: Apollo 11 moon landing. Photograph taken by Neil A. Armstrong of Buzz ...\nSource: Bing\nURL: https://c8.alamy.com/comp/2K5E3PE/apollo-11-moon-landing-photograph-taken-by-neil-a-armstrong-of-buzz-aldrin-walking-on-the-surface-of-the-moon-july-20th-1969-a-unique-optimised-nasa-image-with-added-black-vertical-space-above-original-square-image-credit-nasa-2K5E3PE.jpg\n--- [ProvenanceAgent] is processing... ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"PROVENANCE AGENT ANALYSIS: As a digital archivist, I'll assess the metadata and context to determine the likelihood of authenticity.\n\n**Metadata Analysis:**\n\n1. **Source:** The image is sourced from Alamy, a reputable stock photo agency. This suggests that the image is likely legitimate, as Alamy verifies the authenticity of the images they sell.\n2. **Title:** The title \"Apollo 11 moon landing. Photograph taken by Neil A. Armstrong of Buzz Aldrin walking on the surface of the moon, July 20th, 1969\" is descriptive and matches the known historical event.\n3. **Image description:** The description mentions a unique, optimized image with added black vertical space above the original square image. This could be a processing step, but it's not uncommon for stock photo agencies to modify images for presentation.\n4. **Credit:** The credit \"Nasa\" is given, which is correct, as the Apollo 11 moon landing was a NASA mission.\n5. **URL:** The URL appears to be a legitimate Alamy URL, with a unique identifier (2K5E3PE).\n\n**Suspicious aspects:**\n\n1. **Copyright and licensing:** Alamy requires a subscription or a one-time purchase to access images. However, the URL provided doesn't indicate any licensing or copyright information.\n2. **Image processing:** The mention of \"added black vertical space\" above the original square image raises questions about the image's originality. It's possible that the image was altered or processed in a way that might compromise its authenticity.\n3. **Metadata inconsistencies:** There's no clear indication of the image's original resolution, format, or capture date besides the mention of July 20th, 1969. Typically, images from this era would have more detailed metadata.\n\n**Conclusion:**\n\nWhile the metadata suggests a legitimate source, the image's processing and lack of detailed metadata raise some concerns. I would classify this image as **possibly authentic**, but with some reservations. It's possible that the image is a legitimate NASA photograph, but the processing and metadata inconsistencies make it difficult to confirm its authenticity with absolute certainty.\n\nTo further verify the image's legitimacy, I would recommend:\n\n1. Checking the image's EXIF data (if available) to confirm the capture date and camera settings.\n2. Verifying the image's provenance through NASA's official archives or other reputable sources.\n3. Contacting Alamy to confirm the image's licensing and copyright information.\n\nUltimately, without further investigation, I would consider this image **likely authentic**, but\n------------------------------------------------------------\n\n[Image 2]\nTitle: BUZZ ALDRIN APOLLO 11 MOON LANDING 20 July 1969, Photo NASA Stock Photo ...\nSource: Bing\nURL: https://c8.alamy.com/comp/BPW39B/buzz-aldrin-apollo-11-moon-landing-20-july-1969-photo-nasa-BPW39B.jpg\n--- [ProvenanceAgent] is processing... ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"PROVENANCE AGENT ANALYSIS: As a digital archivist, I'll assess the metadata and context of the image to determine its legitimacy.\n\n**Metadata Analysis**\n\n1. **Source:** The image is sourced from Alamy, a reputable stock photo agency. This suggests that the image is being shared through a legitimate platform.\n2. **Title:** The title \"BUZZ ALDRIN APOLLO 11 MOON LANDING 20 July 1969, Photo NASA\" seems accurate, as it mentions the correct event and the name of the astronaut involved.\n3. **URL:** The URL appears to be a standard link to an image on the Alamy website, which is a common format for stock photo URLs.\n4. **Date:** The date \"20 July 1969\" is consistent with the actual Apollo 11 moon landing date, which occurred on July 20, 1969.\n\n**Contextual Analysis**\n\n1. **Image content:** The image appears to be a photograph of Buzz Aldrin on the moon's surface, which is consistent with the Apollo 11 mission.\n2. **Image quality:** The image quality is good, but it's a low-resolution scan or reproduction, which is common for historical images.\n3. **Licensing:** Alamy is a legitimate stock photo agency, and the image is likely licensed from a reputable source, such as NASA.\n\n**Suspicious Aspects**\n\n1. **Resolution:** The image resolution is relatively low, which might suggest that it's a scan or reproduction rather than an original high-resolution image.\n2. **Copyright:** As a historical image, the copyright status is unclear. Alamy might not be the original copyright holder, and the image could be a reproduction or a derivative work.\n\n**Conclusion**\n\nBased on the metadata and contextual analysis, I would rate the legitimacy of this image as likely authentic. The image appears to be a genuine representation of the Apollo 11 moon landing, and the metadata suggests that it's being shared through a reputable platform. However, the low resolution and unclear copyright status raise some questions about the image's provenance. To further verify its authenticity, I would recommend checking the image's metadata and provenance with NASA or other reputable sources.\n------------------------------------------------------------\n\n[Image 3]\nTitle: BUZZ ALDRIN APOLLO 11 MOON LANDING 20 July 1969, Photo NASA Stock Photo ...\nSource: Bing\nURL: https://c8.alamy.com/comp/BPWFGF/buzz-aldrin-apollo-11-moon-landing-20-july-1969-photo-nasa-BPWFGF.jpg\n--- [ProvenanceAgent] is processing... ---\nPROVENANCE AGENT ANALYSIS: As a digital archivist, I'll assess the metadata and context to determine the authenticity of this image.\n\n**Metadata Analysis:**\n\n1. **Title:** \"BUZZ ALDRIN APOLLO 11 MOON LANDING 20 July 1969, Photo NASA Stock Photo...\" - The title appears to be a descriptive and accurate representation of the image content. However, it's a bit generic and could be easily fabricated.\n2. **Source:** \"Bing\" - Bing is a legitimate search engine, but this doesn't provide any information about the image's origin or ownership.\n3. **URL:** \"https://c8.alamy.com/comp/BPWFGF/buzz-aldrin-apollo-11-moon-landing-20-july-1969-photo-nasa-BPWFGF.jpg\" - The URL appears to be a standard URL for a stock photo agency (Alamy). However, I'll investigate further.\n\n**Red Flags:**\n\n1. **Lack of explicit attribution to NASA:** While the title mentions \"Photo NASA,\" it's not a direct quote or a clear indication of the image being officially released by NASA.\n2. **No clear indication of originality:** The metadata doesn't explicitly state that the image is an original, exclusive, or authentic photograph taken by NASA.\n3. **Unclear ownership:** The URL suggests that the image is hosted on Alamy, but it's not clear if Alamy is the original distributor or a reseller.\n\n**Suspicious Aspects:**\n\n1. **Low resolution:** The image resolution is not specified, but the file size (57 KB) and the fact that it's a JPEG suggest a relatively low-resolution image.\n2. **Lack of EXIF data:** There's no EXIF (Exchangeable Image File Format) data available, which is unusual for a photo taken by a space agency like NASA.\n\n**Conclusion:**\n\nBased on the metadata and context, I would rate the authenticity of this image as **questionable**. While the title and URL suggest a legitimate image, the lack of explicit attribution to NASA, unclear ownership, and suspicious aspects (low resolution, lack of EXIF data) raise concerns about the image's originality and authenticity.\n\n**Recommendation:**\n\nTo verify the image's authenticity, I would recommend:\n\n1. **Checking NASA's official archives:** Search for the image in NASA's official archives or databases to see if it's listed as an authentic and original photograph.\n2. **Contacting Alamy:**\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# CELL 6: Circular Orchestrator MVP (waste-to-value)\n\"\"\"\nCELL 6 â€“ Circular Orchestrator demo (waste-to-value)\n\"\"\"\n\nspotter = ResourceSpotter()\nmatcher = UtilityMatcher()\n\nwaste_post = \"\"\"\nFree pallets and scrap lumber behind the cafe at 5th & Main.\nMost is 2x4 and plywood, some painted. Must pick up by Sunday.\n\"\"\"\n\nprint(\"ğŸ” ResourceSpotter categorizing surplus...\")\nspotter_output = spotter.execute(\n    f\"Analyze this post for potential reusable materials:\\n{waste_post}\\n\\n\"\n    \"Summarize what useful items are available and for whom they might be useful.\"\n)\nprint(\"SPOTTER OUTPUT:\\n\", spotter_output)\n\nprint(\"\\nğŸ”— UtilityMatcher finding local uses...\")\nmatch_context = f\"\"\"\nSurplus items:\n{spotter_output}\n\nPotential beneficiaries in the neighborhood include:\n- A high school woodshop\n- A community makerspace\n- A small DIY furniture builder\n- A local community garden (for wooden planter beds)\n\nTask: Propose 1â€“2 high-utility matches between this surplus and these potential beneficiaries.\n\"\"\"\nmatching_plan = matcher.execute(match_context)\nprint(\"MATCHING PLAN:\\n\", matching_plan)\n\nprint(\"\\nâœ… Circular Orchestrator MVP completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T16:05:17.167537Z","iopub.execute_input":"2026-01-18T16:05:17.168405Z","iopub.status.idle":"2026-01-18T16:05:39.963630Z","shell.execute_reply.started":"2026-01-18T16:05:17.168373Z","shell.execute_reply":"2026-01-18T16:05:39.962925Z"}},"outputs":[{"name":"stdout","text":"ğŸ” ResourceSpotter categorizing surplus...\n--- [ResourceSpotter] is processing... ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"SPOTTER OUTPUT:\n From this post, we can identify a few potentially reusable materials:\n\n1. Pallets and scrap lumber: These can be useful for construction, furniture, or landscape projects.\n2. 2x4 and plywood: These are common framing materials.\n3. Paint: This could be useful for DIY projects that require painting.\n\nThese materials might be most useful for:\n1. Construction workers, carpenters, or anyone needing materials for a DIY project.\n2. Landscapers or gardeners looking for free materials, such as wood, lumber, or scrap materials for fence posts or garden beds.\n3. Hobbyists, such as painters or carpenters, who need materials for their projects.\n\nğŸ”— UtilityMatcher finding local uses...\n--- [UtilityMatcher] is processing... ---\nMATCHING PLAN:\n Based on the surplus materials and potential beneficiaries, I recommend the following high-utility matches:\n\n**Match 1: High School Woodshop**\n\n* Beneficiary: High school woodshop\n* Surplus materials: Pallets and scrap lumber, 2x4 and plywood\n* Utility: The woodshop can utilize these materials for various projects, such as building furniture, crafting, or creating wooden tools. The pallets and scrap lumber can be repurposed as raw materials, while the 2x4 and plywood can be used for framing and construction projects.\n\n**Match 2: Community Makerspace**\n\n* Beneficiary: Community makerspace\n* Surplus materials: Paint\n* Utility: The makerspace can utilize the paint for various DIY projects, such as art, furniture restoration, or other creative endeavors. This match is particularly useful since makerspaces often encourage experimentation and innovation, making paint a valuable resource for their community.\n\nBoth of these matches align well with the needs and capabilities of the potential beneficiaries, maximizing the utility of the surplus materials and promoting resourcefulness and sustainability in the community.\n\nâœ… Circular Orchestrator MVP completed.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# CELL 7 (OPTIONAL): Shadow-Team with Florence-2 vision\n\n\"\"\"\nCELL 7 that uses Florence-2 to analyze a transit image and then feed that description into BarrierScout and DignityNavigator.\n\"\"\"\n\n# Example: a public transit / subway entrance image\n#test_image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/New_York_Subway_entrance.jpg/640px-New_York_Subway_entrance.jpg\"\ntest_image_url = \"https://ephemeralnewyork.wordpress.com/wp-content/uploads/2024/07/subwayglobeallgreentrinitywallstreet.jpeg\"\n\n\nprint(\"ğŸ‘€ Florence-2 analyzing station entrance image...\")\nvision_caption = ask_eyes(test_image_url, task_prompt=\"<DETAILED_CAPTION>\")\nprint(\"VISION CAPTION:\\n\", vision_caption)\n\nscout = BarrierScout()\nnavigator = DignityNavigator()\n\nprint(\"\\nğŸ•µï¸ BarrierScout reasoning from vision + user context...\")\nscout_input = f\"\"\"\nUser is a wheelchair user.\nImage description of a transit entrance:\n{vision_caption}\n\nTask: Based on this description, identify likely accessibility barriers (stairs, no visible elevator, etc.).\n\"\"\"\nscout_report_vision = scout.execute(scout_input)\nprint(\"SCOUT REPORT FROM VISION:\\n\", scout_report_vision)\n\nprint(\"\\nğŸ§­ DignityNavigator planning route based on vision scout...\")\nnav_input_vision = f\"\"\"\nUser is a wheelchair user.\nScout's barrier assessment:\n{scout_report_vision}\n\nTask: Suggest a practical alternative (e.g., find a different entrance, use a bus stop, or avoid this station).\n\"\"\"\nnav_plan_vision = navigator.execute(nav_input_vision)\nprint(\"NAVIGATION PLAN FROM VISION:\\n\", nav_plan_vision)\n\nprint(\"\\nâœ… Shadow-Team + Vision MVP completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T16:14:52.723617Z","iopub.execute_input":"2026-01-18T16:14:52.724319Z","iopub.status.idle":"2026-01-18T16:15:29.764342Z","shell.execute_reply.started":"2026-01-18T16:14:52.724286Z","shell.execute_reply":"2026-01-18T16:15:29.763650Z"}},"outputs":[{"name":"stdout","text":"ğŸ‘€ Florence-2 analyzing station entrance image...\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"VISION CAPTION:\n {'<DETAILED_CAPTION>': 'The image shows a group of people walking down a street next to tall buildings. There are street lights, a trash bin/can, and a person carrying a handbag. The street is lined with trees, railings, and boards with text. The sky is visible in the background.'}\n\nğŸ•µï¸ BarrierScout reasoning from vision + user context...\n--- [BarrierScout] is processing... ---\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"SCOUT REPORT FROM VISION:\n Based on the provided image description, I've identified some potential accessibility barriers for a wheelchair user:\n\n1. **Stairs**: The description mentions a group of people walking down the street, which implies that the entrance likely features stairs, as walking is typically associated with stairs. Stairs can be a significant barrier for wheelchair users.\n\n2. **No visible elevator**: There is no mention of an elevator in the description. This suggests that the entrance might not have an elevator, which could make it difficult or impossible for wheelchair users to access.\n\n3. **Large gaps**: The description does not mention any ramps or other accessible entrances. The presence of stairs and the lack of an elevator imply that there might be a large gap between the sidewalk and the building's entrance, making it difficult for wheelchair users to navigate.\n\n4. **Other potential barriers**: Although not directly related to the entrance, the description mentions \"boards with text\" which could potentially be a visual or tactile barrier for users with visual impairments or those who are blind. However, this is not directly related to physical mobility.\n\nTo confirm the presence of these barriers, a more detailed or in-situ inspection would be necessary. However, based on this description, it is likely that the entrance has stairs, no visible elevator, and possibly large gaps, which could pose significant barriers for wheelchair users.\n\nğŸ§­ DignityNavigator planning route based on vision scout...\n--- [DignityNavigator] is processing... ---\nNAVIGATION PLAN FROM VISION:\n Based on the potential barriers identified, I recommend the following practical alternative for a wheelchair user:\n\n1.  **Alternative Entrance**: If possible, try to locate another entrance that is wheelchair-accessible. This could be a ramp or a designated accessible entrance that allows for a smoother and safer transition.\n\n2.  **Use a Bus Stop**: If the station is not wheelchair-accessible, consider using a nearby bus stop. This will allow the user to avoid the potential barriers at the station and travel to their destination safely and comfortably.\n\n3.  **Avoid the Station**: If neither of the above options is feasible, it may be best to avoid the station altogether. This could mean planning an alternate route that takes into account the wheelchair user's needs and avoids this particular station.\n\n4.  **Contact Station Staff or Transit Agency**: Reach out to the station staff or the transit agency for assistance. They may be able to provide more information about accessible entrances, alternative routes, or other options that can accommodate the wheelchair user's needs.\n\nBy choosing a practical alternative, the wheelchair user can prioritize their safety, dignity, and comfort over speed and still reach their destination efficiently.\n\nâœ… Shadow-Team + Vision MVP completed.\n","output_type":"stream"}],"execution_count":24}]}