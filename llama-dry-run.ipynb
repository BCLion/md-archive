{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13941372,"sourceType":"datasetVersion","datasetId":8885103}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-02T01:56:40.882931Z","iopub.execute_input":"2025-12-02T01:56:40.883223Z","iopub.status.idle":"2025-12-02T01:56:40.891793Z","shell.execute_reply.started":"2025-12-02T01:56:40.883196Z","shell.execute_reply":"2025-12-02T01:56:40.891073Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/samplejobs/jobs.csv\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# AUTHENTICATE WITH YOUR HF TOKEN\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nfrom huggingface_hub import login\nlogin(token=hf_token)\nprint(\"HF AUTH COMPLETE — 70B GATE OPEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T01:49:47.126827Z","iopub.execute_input":"2025-12-02T01:49:47.127193Z","iopub.status.idle":"2025-12-02T01:49:47.779828Z","shell.execute_reply.started":"2025-12-02T01:49:47.127169Z","shell.execute_reply":"2025-12-02T01:49:47.779242Z"}},"outputs":[{"name":"stdout","text":"HF AUTH COMPLETE — 70B GATE OPEN\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#CMAKE_ARGS=\"-DGGML_CUDA=on\" FORCE_CMAKE=1 pip install 'llama-cpp-python[server]'\n#python3 -m llama_cpp.server --model models/7B/llama-model.gguf --n_gpu_layers 35","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install -q bitsandbytes accelerate transformers sentencepiece protobuf\n!pip install -q https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu124/llama_cpp_python-0.3.16-cp311-cp311-linux_x86_64.whl --force-reinstall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T01:54:26.019170Z","iopub.execute_input":"2025-12-02T01:54:26.019930Z","iopub.status.idle":"2025-12-02T01:55:13.394250Z","shell.execute_reply.started":"2025-12-02T01:54:26.019897Z","shell.execute_reply":"2025-12-02T01:55:13.393572Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m551.3/551.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.5 which is incompatible.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2025.10.0 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.1.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Download model - only once\n!wget -q https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T01:55:44.107542Z","iopub.execute_input":"2025-12-02T01:55:44.108203Z","iopub.status.idle":"2025-12-02T01:56:08.642603Z","shell.execute_reply.started":"2025-12-02T01:55:44.108172Z","shell.execute_reply":"2025-12-02T01:56:08.641817Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# KAGGLE VICTORY — PRE-BUILT CU124 WHEEL (270 ROWS IN ~6–8 MIN)\n# NO BUILD, NO DRAMA, FULL DUAL T4 OFFLOAD — TESTED DEC 2025\n\n# Install from official releases (cu124 = Kaggle's CUDA version)\n#!pip install -q https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu124/llama_cpp_python-0.3.16-cp311-cp311-linux_x86_64.whl --force-reinstall\n\nfrom llama_cpp import Llama\nimport pandas as pd\nimport json, re\nfrom tqdm import tqdm\ntqdm.pandas()\n\n# Download the perfect 8B GGUF (Q5_K_M — fits, fast, brutal)\n# Comment out after model is downloaded\n#!wget -q https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf\n\n# FULL DUAL T4 OFFLOAD\nllm = Llama(\n    model_path=\"Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf\",\n    n_gpu_layers=-1,  # ALL to GPU\n    n_batch=1024,\n    n_ctx=4096,\n    verbose=True  # see GPU offload logs\n)\n\nprint(\"Llama-3.1-8B Q5_K_M CU124 FULLY LOADED — T4s ENGAGED\")\n\n# Upload your CSV\ndf = pd.read_csv('/kaggle/input/samplejobs/jobs.csv')  # fix path\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T01:58:10.029727Z","iopub.execute_input":"2025-12-02T01:58:10.030276Z","iopub.status.idle":"2025-12-02T01:58:13.345267Z","shell.execute_reply.started":"2025-12-02T01:58:10.030247Z","shell.execute_reply":"2025-12-02T01:58:13.344463Z"}},"outputs":[{"name":"stderr","text":"ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 2 CUDA devices:\n  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n  Device 1: Tesla T4, compute capability 7.5, VMM: yes\nllama_model_load_from_file_impl: using device CUDA0 (Tesla T4) - 14992 MiB free\nllama_model_load_from_file_impl: using device CUDA1 (Tesla T4) - 14992 MiB free\nllama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from Meta-Llama-3.1-8B-Instruct-Q5_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\nllama_model_loader: - kv   5:                         general.size_label str              = 8B\nllama_model_loader: - kv   6:                            general.license str              = llama3.1\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\nllama_model_loader: - kv   9:                          llama.block_count u32              = 32\nllama_model_loader: - kv  10:                       llama.context_length u32              = 131072\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 17\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\nllama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\nllama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\nllama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\nllama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\nllama_model_loader: - type  f32:   66 tensors\nllama_model_loader: - type q5_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nprint_info: file format = GGUF V3 (latest)\nprint_info: file type   = Q5_K - Medium\nprint_info: file size   = 5.33 GiB (5.70 BPW) \ninit_tokenizer: initializing tokenizer for type 2\nload: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\nload: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\nload: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\nload: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\nload: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\nload: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\nload: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\nload: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\nload: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\nload: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\nload: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\nload: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\nload: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\nload: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\nload: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\nload: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\nload: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\nload: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\nload: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\nload: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\nload: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\nload: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\nload: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\nload: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\nload: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\nload: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\nload: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\nload: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\nload: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\nload: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\nload: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\nload: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\nload: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\nload: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\nload: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\nload: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\nload: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\nload: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\nload: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\nload: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\nload: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\nload: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\nload: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\nload: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\nload: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\nload: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\nload: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\nload: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\nload: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\nload: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\nload: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\nload: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\nload: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\nload: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\nload: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\nload: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\nload: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\nload: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\nload: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\nload: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\nload: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\nload: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\nload: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\nload: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\nload: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\nload: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\nload: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\nload: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\nload: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\nload: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\nload: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\nload: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\nload: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\nload: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\nload: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\nload: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\nload: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\nload: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\nload: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\nload: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\nload: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\nload: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\nload: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\nload: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\nload: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\nload: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\nload: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\nload: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\nload: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\nload: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\nload: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\nload: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\nload: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\nload: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\nload: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\nload: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\nload: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\nload: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\nload: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\nload: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\nload: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\nload: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\nload: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\nload: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\nload: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\nload: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\nload: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\nload: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\nload: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\nload: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\nload: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\nload: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\nload: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\nload: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\nload: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\nload: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\nload: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\nload: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\nload: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\nload: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\nload: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\nload: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\nload: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\nload: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\nload: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\nload: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\nload: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\nload: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\nload: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\nload: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\nload: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\nload: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\nload: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\nload: control token: 128007 '<|end_header_id|>' is not marked as EOG\nload: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\nload: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\nload: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\nload: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\nload: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\nload: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\nload: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\nload: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\nload: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\nload: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\nload: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\nload: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\nload: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\nload: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\nload: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\nload: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\nload: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\nload: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\nload: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\nload: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\nload: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\nload: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\nload: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\nload: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\nload: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\nload: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\nload: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\nload: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\nload: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\nload: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\nload: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\nload: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\nload: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\nload: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\nload: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\nload: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\nload: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\nload: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\nload: control token: 128006 '<|start_header_id|>' is not marked as EOG\nload: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\nload: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\nload: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\nload: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\nload: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\nload: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\nload: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\nload: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\nload: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\nload: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\nload: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\nload: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\nload: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\nload: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\nload: control token: 128000 '<|begin_of_text|>' is not marked as EOG\nload: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\nload: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\nload: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\nload: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\nload: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\nload: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\nload: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\nload: control token: 128010 '<|python_tag|>' is not marked as EOG\nload: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\nload: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\nload: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\nload: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\nload: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\nload: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\nload: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\nload: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\nload: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\nload: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\nload: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\nload: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\nload: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\nload: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\nload: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\nload: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\nload: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\nload: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\nload: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\nload: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\nload: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\nload: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\nload: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\nload: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\nload: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\nload: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\nload: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\nload: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\nload: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\nload: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\nload: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\nload: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\nload: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\nload: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\nload: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\nload: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\nload: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\nload: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\nload: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\nload: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\nload: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\nload: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\nload: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\nload: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\nload: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\nload: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\nload: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\nload: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\nload: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\nload: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\nload: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\nload: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\nload: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\nload: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\nload: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\nload: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\nload: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\nload: printing all EOG tokens:\nload:   - 128001 ('<|end_of_text|>')\nload:   - 128008 ('<|eom_id|>')\nload:   - 128009 ('<|eot_id|>')\nload: special tokens cache size = 256\nload: token to piece cache size = 0.7999 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 131072\nprint_info: n_embd           = 4096\nprint_info: n_layer          = 32\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 8\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: is_swa_any       = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 4\nprint_info: n_embd_k_gqa     = 1024\nprint_info: n_embd_v_gqa     = 1024\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-05\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: f_attn_scale     = 0.0e+00\nprint_info: n_ff             = 14336\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 500000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 131072\nprint_info: rope_finetuned   = unknown\nprint_info: model type       = 8B\nprint_info: model params     = 8.03 B\nprint_info: general.name     = Meta Llama 3.1 8B Instruct\nprint_info: vocab type       = BPE\nprint_info: n_vocab          = 128256\nprint_info: n_merges         = 280147\nprint_info: BOS token        = 128000 '<|begin_of_text|>'\nprint_info: EOS token        = 128009 '<|eot_id|>'\nprint_info: EOT token        = 128009 '<|eot_id|>'\nprint_info: EOM token        = 128008 '<|eom_id|>'\nprint_info: LF token         = 198 'Ċ'\nprint_info: EOG token        = 128001 '<|end_of_text|>'\nprint_info: EOG token        = 128008 '<|eom_id|>'\nprint_info: EOG token        = 128009 '<|eot_id|>'\nprint_info: max token length = 256\nload_tensors: loading model tensors, this can take a while... (mmap = true)\nload_tensors: layer   0 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   1 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   2 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   3 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   4 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   5 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   6 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   7 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   8 assigned to device CUDA0, is_swa = 0\nload_tensors: layer   9 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  10 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  11 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  12 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  13 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  14 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  15 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  16 assigned to device CUDA0, is_swa = 0\nload_tensors: layer  17 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  18 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  19 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  20 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  21 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  22 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  23 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  24 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  25 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  26 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  27 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  28 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  29 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  30 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  31 assigned to device CUDA1, is_swa = 0\nload_tensors: layer  32 assigned to device CUDA1, is_swa = 0\nload_tensors: tensor 'token_embd.weight' (q5_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\nload_tensors: offloading 32 repeating layers to GPU\nload_tensors: offloading output layer to GPU\nload_tensors: offloaded 33/33 layers to GPU\nload_tensors:        CUDA0 model buffer size =  2495.28 MiB\nload_tensors:        CUDA1 model buffer size =  2620.21 MiB\nload_tensors:   CPU_Mapped model buffer size =   344.44 MiB\n.......................................................................................\nllama_context: constructing llama_context\nllama_context: n_seq_max     = 1\nllama_context: n_ctx         = 4096\nllama_context: n_ctx_per_seq = 4096\nllama_context: n_batch       = 1024\nllama_context: n_ubatch      = 512\nllama_context: causal_attn   = 1\nllama_context: flash_attn    = 0\nllama_context: kv_unified    = false\nllama_context: freq_base     = 500000.0\nllama_context: freq_scale    = 1\nllama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\nset_abort_callback: call\nllama_context:  CUDA_Host  output buffer size =     0.49 MiB\ncreate_memory: n_ctx = 4096 (padded)\nllama_kv_cache_unified: layer   0: dev = CUDA0\nllama_kv_cache_unified: layer   1: dev = CUDA0\nllama_kv_cache_unified: layer   2: dev = CUDA0\nllama_kv_cache_unified: layer   3: dev = CUDA0\nllama_kv_cache_unified: layer   4: dev = CUDA0\nllama_kv_cache_unified: layer   5: dev = CUDA0\nllama_kv_cache_unified: layer   6: dev = CUDA0\nllama_kv_cache_unified: layer   7: dev = CUDA0\nllama_kv_cache_unified: layer   8: dev = CUDA0\nllama_kv_cache_unified: layer   9: dev = CUDA0\nllama_kv_cache_unified: layer  10: dev = CUDA0\nllama_kv_cache_unified: layer  11: dev = CUDA0\nllama_kv_cache_unified: layer  12: dev = CUDA0\nllama_kv_cache_unified: layer  13: dev = CUDA0\nllama_kv_cache_unified: layer  14: dev = CUDA0\nllama_kv_cache_unified: layer  15: dev = CUDA0\nllama_kv_cache_unified: layer  16: dev = CUDA0\nllama_kv_cache_unified: layer  17: dev = CUDA1\nllama_kv_cache_unified: layer  18: dev = CUDA1\nllama_kv_cache_unified: layer  19: dev = CUDA1\nllama_kv_cache_unified: layer  20: dev = CUDA1\nllama_kv_cache_unified: layer  21: dev = CUDA1\nllama_kv_cache_unified: layer  22: dev = CUDA1\nllama_kv_cache_unified: layer  23: dev = CUDA1\nllama_kv_cache_unified: layer  24: dev = CUDA1\nllama_kv_cache_unified: layer  25: dev = CUDA1\nllama_kv_cache_unified: layer  26: dev = CUDA1\nllama_kv_cache_unified: layer  27: dev = CUDA1\nllama_kv_cache_unified: layer  28: dev = CUDA1\nllama_kv_cache_unified: layer  29: dev = CUDA1\nllama_kv_cache_unified: layer  30: dev = CUDA1\nllama_kv_cache_unified: layer  31: dev = CUDA1\nllama_kv_cache_unified:      CUDA0 KV buffer size =   272.00 MiB\nllama_kv_cache_unified:      CUDA1 KV buffer size =   240.00 MiB\nllama_kv_cache_unified: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB\nllama_context: enumerating backends\nllama_context: backend_ptrs.size() = 3\nllama_context: max_nodes = 2344\nllama_context: pipeline parallelism enabled (n_copies=4)\nllama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\ngraph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\ngraph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\ngraph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\nllama_context:      CUDA0 compute buffer size =   368.02 MiB\nllama_context:      CUDA1 compute buffer size =   368.03 MiB\nllama_context:  CUDA_Host compute buffer size =    56.04 MiB\nllama_context: graph nodes  = 1126\nllama_context: graph splits = 3\n","output_type":"stream"},{"name":"stdout","text":"Llama-3.1-8B Q5_K_M CU124 FULLY LOADED — T4s ENGAGED\n","output_type":"stream"},{"name":"stderr","text":"CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \nModel metadata: {'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.chunks_count': '125', 'quantize.imatrix.file': '/models_out/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.imatrix', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '17', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Meta Llama 3.1 8B Instruct', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '8B', 'general.license': 'llama3.1', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\nAvailable chat formats from metadata: chat_template.default\nUsing gguf chat template: {{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\"' + arg_val + '\"' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n            {{- '\"parameters\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n\nUsing chat eos_token: <|eot_id|>\nUsing chat bos_token: <|begin_of_text|>\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# above script from 12-01-2025","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Neww updated temp and top_p make it a bit more creative\ndef roast(row):\n    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are a battle-hardened senior engineer who has survived FAANG, startups, and government contracts. \nYour verdicts are sharp, varied, and brutally honest. \nNever start with \"Another\", \"Yet another\", \"Classic\", or \"Typical\". \nUse fresh venom every time. \nReturn ONLY valid JSON. No explanations.<|eot_id|>\n<|start_header_id|>user<|end_header_id|>\nTitle: {row.get('title','')}\nCompany: {row.get('company','')}\nLocation: {row.get('location','')}\nDescription: {str(row.get('description',''))[:2200]}\n\nReturn exactly:\n{{\n  \"summary\": \"one savage sentence — be creative\",\n  \"vibe\": \"faang_tier|hidden_gem|startup_chaos|corporate_zombie|avoid\",\n  \"red_flags\": [\"max 4\"],\n  \"green_flags\": [\"max 3\"],\n  \"seniority\": \"junior|mid|senior|staff|principal\",\n  \"match_score\": 0-100\n}}<|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\"\"\"\n\n    out = llm(prompt, max_tokens=350, temperature=0.95, top_p=0.92, stop=[\"<|eot_id|>\"], echo=False)\n    text = out['choices'][0]['text'].strip()\n\n    # Aggressive JSON extraction\n    start = text.find(\"{\")\n    end = text.rfind(\"}\") + 1\n    if start != -1 and end > start:\n        try:\n            return json.loads(text[start:end])\n        except:\n            pass\n    \n    # Fallback: look for any JSON-like blob\n    match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n    if match:\n        try:\n            return json.loads(match.group(0))\n        except:\n            pass\n    \n    return {\"summary\": \"parse failed\", \"vibe\": \"avoid\"}\n\nprint(\"RE-ROASTING ALL 270 JOBS WITH BULLETPROOF PROMPT — ~6–8 MINUTES\")\ndf['insights'] = df.progress_apply(roast, axis=1)\n\n# Save the final victory\ndf.to_csv('SHOGUN9000K_FINAL_PERFECT_ROAST.csv', index=False)\nprint(\"VICTORY — DOWNLOAD SHOGUN9000K_FINAL_PERFECT_ROAST.csv — 99%+ BRUTAL JSON GUARANTEED\")\n# Below script is the OG, impressed with it.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T01:58:45.204032Z","iopub.execute_input":"2025-12-02T01:58:45.204619Z","iopub.status.idle":"2025-12-02T02:17:02.640973Z","shell.execute_reply.started":"2025-12-02T01:58:45.204591Z","shell.execute_reply":"2025-12-02T02:17:02.640335Z"}},"outputs":[{"name":"stdout","text":"RE-ROASTING ALL 270 JOBS WITH BULLETPROOF PROMPT — ~6–8 MINUTES\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/270 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     957.31 ms /   473 tokens (    2.02 ms per token,   494.09 tokens per second)\nllama_perf_context_print:        eval time =    2685.17 ms /   105 runs   (   25.57 ms per token,    39.10 tokens per second)\nllama_perf_context_print:       total time =    3766.15 ms /   578 tokens\nllama_perf_context_print:    graphs reused =        100\n  1%|          | 2/270 [00:03<08:25,  1.89s/it]Llama.generate: 82 prefix-match hit, remaining 399 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     421.43 ms /   399 tokens (    1.06 ms per token,   946.78 tokens per second)\nllama_perf_context_print:        eval time =    2614.01 ms /   103 runs   (   25.38 ms per token,    39.40 tokens per second)\nllama_perf_context_print:       total time =    3154.79 ms /   502 tokens\nllama_perf_context_print:    graphs reused =         99\n  1%|          | 3/270 [00:06<10:45,  2.42s/it]Llama.generate: 82 prefix-match hit, remaining 305 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     317.87 ms /   305 tokens (    1.04 ms per token,   959.50 tokens per second)\nllama_perf_context_print:        eval time =    2651.92 ms /   102 runs   (   26.00 ms per token,    38.46 tokens per second)\nllama_perf_context_print:       total time =    3087.09 ms /   407 tokens\nllama_perf_context_print:    graphs reused =         98\n  1%|▏         | 4/270 [00:10<11:49,  2.67s/it]Llama.generate: 82 prefix-match hit, remaining 389 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     424.02 ms /   389 tokens (    1.09 ms per token,   917.41 tokens per second)\nllama_perf_context_print:        eval time =    3308.22 ms /   130 runs   (   25.45 ms per token,    39.30 tokens per second)\nllama_perf_context_print:       total time =    3881.67 ms /   519 tokens\nllama_perf_context_print:    graphs reused =        125\n  2%|▏         | 5/270 [00:13<13:39,  3.09s/it]Llama.generate: 82 prefix-match hit, remaining 431 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     454.60 ms /   431 tokens (    1.05 ms per token,   948.09 tokens per second)\nllama_perf_context_print:        eval time =    3040.26 ms /   121 runs   (   25.13 ms per token,    39.80 tokens per second)\nllama_perf_context_print:       total time =    3635.02 ms /   552 tokens\nllama_perf_context_print:    graphs reused =        117\n  2%|▏         | 6/270 [00:17<14:24,  3.27s/it]Llama.generate: 82 prefix-match hit, remaining 427 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     435.40 ms /   427 tokens (    1.02 ms per token,   980.70 tokens per second)\nllama_perf_context_print:        eval time =    3628.67 ms /   144 runs   (   25.20 ms per token,    39.68 tokens per second)\nllama_perf_context_print:       total time =    4230.92 ms /   571 tokens\nllama_perf_context_print:    graphs reused =        138\n  3%|▎         | 7/270 [00:21<15:42,  3.58s/it]Llama.generate: 82 prefix-match hit, remaining 400 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     433.01 ms /   400 tokens (    1.08 ms per token,   923.76 tokens per second)\nllama_perf_context_print:        eval time =    3532.94 ms /   139 runs   (   25.42 ms per token,    39.34 tokens per second)\nllama_perf_context_print:       total time =    4127.15 ms /   539 tokens\nllama_perf_context_print:    graphs reused =        134\n  3%|▎         | 8/270 [00:25<16:24,  3.76s/it]Llama.generate: 82 prefix-match hit, remaining 411 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     436.85 ms /   411 tokens (    1.06 ms per token,   940.82 tokens per second)\nllama_perf_context_print:        eval time =    3121.37 ms /   123 runs   (   25.38 ms per token,    39.41 tokens per second)\nllama_perf_context_print:       total time =    3701.75 ms /   534 tokens\nllama_perf_context_print:    graphs reused =        118\n  3%|▎         | 9/270 [00:29<16:16,  3.74s/it]Llama.generate: 82 prefix-match hit, remaining 386 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     434.53 ms /   386 tokens (    1.13 ms per token,   888.32 tokens per second)\nllama_perf_context_print:        eval time =    3049.80 ms /   119 runs   (   25.63 ms per token,    39.02 tokens per second)\nllama_perf_context_print:       total time =    3621.36 ms /   505 tokens\nllama_perf_context_print:    graphs reused =        114\n  4%|▎         | 10/270 [00:33<16:03,  3.71s/it]Llama.generate: 82 prefix-match hit, remaining 374 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     373.38 ms /   374 tokens (    1.00 ms per token,  1001.65 tokens per second)\nllama_perf_context_print:        eval time =    3341.90 ms /   130 runs   (   25.71 ms per token,    38.90 tokens per second)\nllama_perf_context_print:       total time =    3865.77 ms /   504 tokens\nllama_perf_context_print:    graphs reused =        125\n  4%|▍         | 11/270 [00:37<16:12,  3.76s/it]Llama.generate: 82 prefix-match hit, remaining 381 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     376.62 ms /   381 tokens (    0.99 ms per token,  1011.64 tokens per second)\nllama_perf_context_print:        eval time =    2935.78 ms /   114 runs   (   25.75 ms per token,    38.83 tokens per second)\nllama_perf_context_print:       total time =    3442.12 ms /   495 tokens\nllama_perf_context_print:    graphs reused =        109\n  4%|▍         | 12/270 [00:40<15:44,  3.66s/it]Llama.generate: 84 prefix-match hit, remaining 403 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     442.76 ms /   403 tokens (    1.10 ms per token,   910.19 tokens per second)\nllama_perf_context_print:        eval time =    4107.46 ms /   161 runs   (   25.51 ms per token,    39.20 tokens per second)\nllama_perf_context_print:       total time =    4739.89 ms /   564 tokens\nllama_perf_context_print:    graphs reused =        155\n  5%|▍         | 13/270 [00:45<17:05,  3.99s/it]Llama.generate: 84 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     445.69 ms /   398 tokens (    1.12 ms per token,   893.00 tokens per second)\nllama_perf_context_print:        eval time =    6066.42 ms /   237 runs   (   25.60 ms per token,    39.07 tokens per second)\nllama_perf_context_print:       total time =    6803.42 ms /   635 tokens\nllama_perf_context_print:    graphs reused =        229\n  5%|▌         | 14/270 [00:52<20:39,  4.84s/it]Llama.generate: 84 prefix-match hit, remaining 410 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     454.41 ms /   410 tokens (    1.11 ms per token,   902.27 tokens per second)\nllama_perf_context_print:        eval time =    4839.56 ms /   189 runs   (   25.61 ms per token,    39.05 tokens per second)\nllama_perf_context_print:       total time =    5519.55 ms /   599 tokens\nllama_perf_context_print:    graphs reused =        182\n  6%|▌         | 15/270 [00:57<21:26,  5.05s/it]Llama.generate: 82 prefix-match hit, remaining 427 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     461.54 ms /   427 tokens (    1.08 ms per token,   925.16 tokens per second)\nllama_perf_context_print:        eval time =    4477.88 ms /   175 runs   (   25.59 ms per token,    39.08 tokens per second)\nllama_perf_context_print:       total time =    5147.38 ms /   602 tokens\nllama_perf_context_print:    graphs reused =        168\n  6%|▌         | 16/270 [01:02<21:29,  5.08s/it]Llama.generate: 82 prefix-match hit, remaining 394 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     458.07 ms /   394 tokens (    1.16 ms per token,   860.13 tokens per second)\nllama_perf_context_print:        eval time =    3261.19 ms /   126 runs   (   25.88 ms per token,    38.64 tokens per second)\nllama_perf_context_print:       total time =    3869.78 ms /   520 tokens\nllama_perf_context_print:    graphs reused =        121\n  6%|▋         | 17/270 [01:06<19:53,  4.72s/it]Llama.generate: 86 prefix-match hit, remaining 375 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     393.27 ms /   375 tokens (    1.05 ms per token,   953.55 tokens per second)\nllama_perf_context_print:        eval time =    3256.76 ms /   125 runs   (   26.05 ms per token,    38.38 tokens per second)\nllama_perf_context_print:       total time =    3794.25 ms /   500 tokens\nllama_perf_context_print:    graphs reused =        120\n  7%|▋         | 18/270 [01:10<18:39,  4.44s/it]Llama.generate: 82 prefix-match hit, remaining 355 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     389.81 ms /   355 tokens (    1.10 ms per token,   910.69 tokens per second)\nllama_perf_context_print:        eval time =    2977.53 ms /   113 runs   (   26.35 ms per token,    37.95 tokens per second)\nllama_perf_context_print:       total time =    3498.07 ms /   468 tokens\nllama_perf_context_print:    graphs reused =        108\n  7%|▋         | 19/270 [01:13<17:24,  4.16s/it]Llama.generate: 85 prefix-match hit, remaining 385 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     467.29 ms /   385 tokens (    1.21 ms per token,   823.90 tokens per second)\nllama_perf_context_print:        eval time =    4438.93 ms /   171 runs   (   25.96 ms per token,    38.52 tokens per second)\nllama_perf_context_print:       total time =    5112.52 ms /   556 tokens\nllama_perf_context_print:    graphs reused =        164\n  7%|▋         | 20/270 [01:19<18:31,  4.45s/it]Llama.generate: 82 prefix-match hit, remaining 403 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     473.51 ms /   403 tokens (    1.17 ms per token,   851.09 tokens per second)\nllama_perf_context_print:        eval time =    4105.70 ms /   158 runs   (   25.99 ms per token,    38.48 tokens per second)\nllama_perf_context_print:       total time =    4767.78 ms /   561 tokens\nllama_perf_context_print:    graphs reused =        152\n  8%|▊         | 21/270 [01:23<18:51,  4.55s/it]Llama.generate: 82 prefix-match hit, remaining 407 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     477.10 ms /   407 tokens (    1.17 ms per token,   853.07 tokens per second)\nllama_perf_context_print:        eval time =    2943.72 ms /   113 runs   (   26.05 ms per token,    38.39 tokens per second)\nllama_perf_context_print:       total time =    3553.68 ms /   520 tokens\nllama_perf_context_print:    graphs reused =        109\n  8%|▊         | 22/270 [01:27<17:33,  4.25s/it]Llama.generate: 82 prefix-match hit, remaining 422 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     482.84 ms /   422 tokens (    1.14 ms per token,   873.99 tokens per second)\nllama_perf_context_print:        eval time =    2908.10 ms /   112 runs   (   25.97 ms per token,    38.51 tokens per second)\nllama_perf_context_print:       total time =    3517.83 ms /   534 tokens\nllama_perf_context_print:    graphs reused =        107\n  9%|▊         | 23/270 [01:30<16:35,  4.03s/it]Llama.generate: 82 prefix-match hit, remaining 420 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     488.50 ms /   420 tokens (    1.16 ms per token,   859.78 tokens per second)\nllama_perf_context_print:        eval time =    3071.14 ms /   118 runs   (   26.03 ms per token,    38.42 tokens per second)\nllama_perf_context_print:       total time =    3694.19 ms /   538 tokens\nllama_perf_context_print:    graphs reused =        113\n  9%|▉         | 24/270 [01:34<16:07,  3.93s/it]Llama.generate: 82 prefix-match hit, remaining 399 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     488.93 ms /   399 tokens (    1.23 ms per token,   816.06 tokens per second)\nllama_perf_context_print:        eval time =    3981.09 ms /   152 runs   (   26.19 ms per token,    38.18 tokens per second)\nllama_perf_context_print:       total time =    4650.10 ms /   551 tokens\nllama_perf_context_print:    graphs reused =        147\n  9%|▉         | 25/270 [01:39<16:56,  4.15s/it]Llama.generate: 82 prefix-match hit, remaining 387 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     491.29 ms /   387 tokens (    1.27 ms per token,   787.72 tokens per second)\nllama_perf_context_print:        eval time =    3957.04 ms /   150 runs   (   26.38 ms per token,    37.91 tokens per second)\nllama_perf_context_print:       total time =    4625.86 ms /   537 tokens\nllama_perf_context_print:    graphs reused =        144\n 10%|▉         | 26/270 [01:43<17:27,  4.29s/it]Llama.generate: 82 prefix-match hit, remaining 395 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.11 ms /   395 tokens (    1.26 ms per token,   794.59 tokens per second)\nllama_perf_context_print:        eval time =    3545.20 ms /   134 runs   (   26.46 ms per token,    37.80 tokens per second)\nllama_perf_context_print:       total time =    4198.45 ms /   529 tokens\nllama_perf_context_print:    graphs reused =        128\n 10%|█         | 27/270 [01:48<17:16,  4.27s/it]Llama.generate: 82 prefix-match hit, remaining 399 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.58 ms /   399 tokens (    1.26 ms per token,   795.49 tokens per second)\nllama_perf_context_print:        eval time =    3213.69 ms /   121 runs   (   26.56 ms per token,    37.65 tokens per second)\nllama_perf_context_print:       total time =    3855.23 ms /   520 tokens\nllama_perf_context_print:    graphs reused =        117\n 10%|█         | 28/270 [01:52<16:43,  4.14s/it]Llama.generate: 84 prefix-match hit, remaining 479 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     590.21 ms /   479 tokens (    1.23 ms per token,   811.58 tokens per second)\nllama_perf_context_print:        eval time =    3042.60 ms /   115 runs   (   26.46 ms per token,    37.80 tokens per second)\nllama_perf_context_print:       total time =    3765.54 ms /   594 tokens\nllama_perf_context_print:    graphs reused =        110\n 11%|█         | 29/270 [01:55<16:11,  4.03s/it]Llama.generate: 82 prefix-match hit, remaining 408 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     508.21 ms /   408 tokens (    1.25 ms per token,   802.81 tokens per second)\nllama_perf_context_print:        eval time =    2984.01 ms /   112 runs   (   26.64 ms per token,    37.53 tokens per second)\nllama_perf_context_print:       total time =    3621.69 ms /   520 tokens\nllama_perf_context_print:    graphs reused =        108\n 11%|█         | 30/270 [01:59<15:38,  3.91s/it]Llama.generate: 82 prefix-match hit, remaining 417 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     517.29 ms /   417 tokens (    1.24 ms per token,   806.12 tokens per second)\nllama_perf_context_print:        eval time =    3081.94 ms /   116 runs   (   26.57 ms per token,    37.64 tokens per second)\nllama_perf_context_print:       total time =    3737.29 ms /   533 tokens\nllama_perf_context_print:    graphs reused =        111\n 11%|█▏        | 31/270 [02:03<15:22,  3.86s/it]Llama.generate: 82 prefix-match hit, remaining 396 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     517.77 ms /   396 tokens (    1.31 ms per token,   764.81 tokens per second)\nllama_perf_context_print:        eval time =    4738.59 ms /   177 runs   (   26.77 ms per token,    37.35 tokens per second)\nllama_perf_context_print:       total time =    5471.37 ms /   573 tokens\nllama_perf_context_print:    graphs reused =        170\n 12%|█▏        | 32/270 [02:08<17:14,  4.35s/it]Llama.generate: 84 prefix-match hit, remaining 388 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     518.39 ms /   388 tokens (    1.34 ms per token,   748.47 tokens per second)\nllama_perf_context_print:        eval time =    3339.53 ms /   124 runs   (   26.93 ms per token,    37.13 tokens per second)\nllama_perf_context_print:       total time =    4002.02 ms /   512 tokens\nllama_perf_context_print:    graphs reused =        119\n 12%|█▏        | 33/270 [02:12<16:45,  4.24s/it]Llama.generate: 82 prefix-match hit, remaining 413 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     524.65 ms /   413 tokens (    1.27 ms per token,   787.19 tokens per second)\nllama_perf_context_print:        eval time =    3249.91 ms /   122 runs   (   26.64 ms per token,    37.54 tokens per second)\nllama_perf_context_print:       total time =    3916.57 ms /   535 tokens\nllama_perf_context_print:    graphs reused =        117\n 13%|█▎        | 34/270 [02:16<16:18,  4.15s/it]Llama.generate: 82 prefix-match hit, remaining 377 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     438.15 ms /   377 tokens (    1.16 ms per token,   860.43 tokens per second)\nllama_perf_context_print:        eval time =    3778.48 ms /   141 runs   (   26.80 ms per token,    37.32 tokens per second)\nllama_perf_context_print:       total time =    4380.93 ms /   518 tokens\nllama_perf_context_print:    graphs reused =        136\n 13%|█▎        | 35/270 [02:20<16:31,  4.22s/it]Llama.generate: 82 prefix-match hit, remaining 427 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     513.46 ms /   427 tokens (    1.20 ms per token,   831.61 tokens per second)\nllama_perf_context_print:        eval time =    3207.42 ms /   122 runs   (   26.29 ms per token,    38.04 tokens per second)\nllama_perf_context_print:       total time =    3862.64 ms /   549 tokens\nllama_perf_context_print:    graphs reused =        117\n 13%|█▎        | 36/270 [02:24<16:02,  4.11s/it]Llama.generate: 82 prefix-match hit, remaining 406 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     505.13 ms /   406 tokens (    1.24 ms per token,   803.76 tokens per second)\nllama_perf_context_print:        eval time =    3246.41 ms /   123 runs   (   26.39 ms per token,    37.89 tokens per second)\nllama_perf_context_print:       total time =    3894.99 ms /   529 tokens\nllama_perf_context_print:    graphs reused =        118\n 14%|█▎        | 37/270 [02:28<15:43,  4.05s/it]Llama.generate: 82 prefix-match hit, remaining 299 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     369.54 ms /   299 tokens (    1.24 ms per token,   809.12 tokens per second)\nllama_perf_context_print:        eval time =    3179.30 ms /   117 runs   (   27.17 ms per token,    36.80 tokens per second)\nllama_perf_context_print:       total time =    3682.63 ms /   416 tokens\nllama_perf_context_print:    graphs reused =        112\n 14%|█▍        | 38/270 [02:32<15:14,  3.94s/it]Llama.generate: 82 prefix-match hit, remaining 378 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     422.29 ms /   378 tokens (    1.12 ms per token,   895.12 tokens per second)\nllama_perf_context_print:        eval time =    3320.32 ms /   125 runs   (   26.56 ms per token,    37.65 tokens per second)\nllama_perf_context_print:       total time =    3887.34 ms /   503 tokens\nllama_perf_context_print:    graphs reused =        120\n 14%|█▍        | 39/270 [02:36<15:06,  3.93s/it]Llama.generate: 82 prefix-match hit, remaining 416 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     494.85 ms /   416 tokens (    1.19 ms per token,   840.66 tokens per second)\nllama_perf_context_print:        eval time =    3197.92 ms /   122 runs   (   26.21 ms per token,    38.15 tokens per second)\nllama_perf_context_print:       total time =    3832.89 ms /   538 tokens\nllama_perf_context_print:    graphs reused =        117\n 15%|█▍        | 40/270 [02:40<14:57,  3.90s/it]Llama.generate: 82 prefix-match hit, remaining 383 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     397.00 ms /   383 tokens (    1.04 ms per token,   964.74 tokens per second)\nllama_perf_context_print:        eval time =    3055.44 ms /   115 runs   (   26.57 ms per token,    37.64 tokens per second)\nllama_perf_context_print:       total time =    3583.72 ms /   498 tokens\nllama_perf_context_print:    graphs reused =        110\n 15%|█▌        | 41/270 [02:43<14:31,  3.81s/it]Llama.generate: 85 prefix-match hit, remaining 395 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     490.60 ms /   395 tokens (    1.24 ms per token,   805.13 tokens per second)\nllama_perf_context_print:        eval time =    2565.24 ms /    97 runs   (   26.45 ms per token,    37.81 tokens per second)\nllama_perf_context_print:       total time =    3167.13 ms /   492 tokens\nllama_perf_context_print:    graphs reused =         93\n 16%|█▌        | 42/270 [02:46<13:44,  3.62s/it]Llama.generate: 82 prefix-match hit, remaining 417 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     494.14 ms /   417 tokens (    1.18 ms per token,   843.89 tokens per second)\nllama_perf_context_print:        eval time =    3451.03 ms /   132 runs   (   26.14 ms per token,    38.25 tokens per second)\nllama_perf_context_print:       total time =    4097.77 ms /   549 tokens\nllama_perf_context_print:    graphs reused =        127\n 16%|█▌        | 43/270 [02:50<14:14,  3.76s/it]Llama.generate: 82 prefix-match hit, remaining 409 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     493.55 ms /   409 tokens (    1.21 ms per token,   828.70 tokens per second)\nllama_perf_context_print:        eval time =    3438.89 ms /   131 runs   (   26.25 ms per token,    38.09 tokens per second)\nllama_perf_context_print:       total time =    4083.53 ms /   540 tokens\nllama_perf_context_print:    graphs reused =        126\n 16%|█▋        | 44/270 [02:55<14:32,  3.86s/it]Llama.generate: 82 prefix-match hit, remaining 274 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     336.61 ms /   274 tokens (    1.23 ms per token,   814.01 tokens per second)\nllama_perf_context_print:        eval time =    2864.30 ms /   106 runs   (   27.02 ms per token,    37.01 tokens per second)\nllama_perf_context_print:       total time =    3323.50 ms /   380 tokens\nllama_perf_context_print:    graphs reused =        102\n 17%|█▋        | 45/270 [02:58<13:52,  3.70s/it]Llama.generate: 82 prefix-match hit, remaining 405 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     495.24 ms /   405 tokens (    1.22 ms per token,   817.78 tokens per second)\nllama_perf_context_print:        eval time =    4052.62 ms /   154 runs   (   26.32 ms per token,    38.00 tokens per second)\nllama_perf_context_print:       total time =    4727.99 ms /   559 tokens\nllama_perf_context_print:    graphs reused =        148\n 17%|█▋        | 46/270 [03:03<14:58,  4.01s/it]Llama.generate: 82 prefix-match hit, remaining 378 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     423.70 ms /   378 tokens (    1.12 ms per token,   892.13 tokens per second)\nllama_perf_context_print:        eval time =    2622.28 ms /    98 runs   (   26.76 ms per token,    37.37 tokens per second)\nllama_perf_context_print:       total time =    3157.92 ms /   476 tokens\nllama_perf_context_print:    graphs reused =         94\n 17%|█▋        | 47/270 [03:06<13:57,  3.76s/it]Llama.generate: 82 prefix-match hit, remaining 406 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     500.71 ms /   406 tokens (    1.23 ms per token,   810.85 tokens per second)\nllama_perf_context_print:        eval time =    3560.31 ms /   135 runs   (   26.37 ms per token,    37.92 tokens per second)\nllama_perf_context_print:       total time =    4219.40 ms /   541 tokens\nllama_perf_context_print:    graphs reused =        130\n 18%|█▊        | 48/270 [03:10<14:25,  3.90s/it]Llama.generate: 82 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.63 ms /   398 tokens (    1.26 ms per token,   793.41 tokens per second)\nllama_perf_context_print:        eval time =    3334.80 ms /   126 runs   (   26.47 ms per token,    37.78 tokens per second)\nllama_perf_context_print:       total time =    3981.23 ms /   524 tokens\nllama_perf_context_print:    graphs reused =        122\n 18%|█▊        | 49/270 [03:14<14:27,  3.92s/it]Llama.generate: 82 prefix-match hit, remaining 436 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     532.79 ms /   436 tokens (    1.22 ms per token,   818.33 tokens per second)\nllama_perf_context_print:        eval time =    3064.85 ms /   117 runs   (   26.20 ms per token,    38.17 tokens per second)\nllama_perf_context_print:       total time =    3735.09 ms /   553 tokens\nllama_perf_context_print:    graphs reused =        113\n 19%|█▊        | 50/270 [03:18<14:11,  3.87s/it]Llama.generate: 84 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     506.89 ms /   398 tokens (    1.27 ms per token,   785.18 tokens per second)\nllama_perf_context_print:        eval time =    3335.44 ms /   126 runs   (   26.47 ms per token,    37.78 tokens per second)\nllama_perf_context_print:       total time =    3987.88 ms /   524 tokens\nllama_perf_context_print:    graphs reused =        122\n 19%|█▉        | 51/270 [03:22<14:15,  3.91s/it]Llama.generate: 82 prefix-match hit, remaining 392 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     503.24 ms /   392 tokens (    1.28 ms per token,   778.95 tokens per second)\nllama_perf_context_print:        eval time =    3191.34 ms /   120 runs   (   26.59 ms per token,    37.60 tokens per second)\nllama_perf_context_print:       total time =    3832.29 ms /   512 tokens\nllama_perf_context_print:    graphs reused =        115\n 19%|█▉        | 52/270 [03:26<14:06,  3.89s/it]Llama.generate: 85 prefix-match hit, remaining 271 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     341.05 ms /   271 tokens (    1.26 ms per token,   794.60 tokens per second)\nllama_perf_context_print:        eval time =    3228.14 ms /   119 runs   (   27.13 ms per token,    36.86 tokens per second)\nllama_perf_context_print:       total time =    3707.52 ms /   390 tokens\nllama_perf_context_print:    graphs reused =        115\n 20%|█▉        | 53/270 [03:29<13:51,  3.83s/it]Llama.generate: 82 prefix-match hit, remaining 412 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     508.76 ms /   412 tokens (    1.23 ms per token,   809.82 tokens per second)\nllama_perf_context_print:        eval time =    4584.01 ms /   174 runs   (   26.34 ms per token,    37.96 tokens per second)\nllama_perf_context_print:       total time =    5297.54 ms /   586 tokens\nllama_perf_context_print:    graphs reused =        168\n 20%|██        | 54/270 [03:35<15:23,  4.27s/it]Llama.generate: 82 prefix-match hit, remaining 418 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     507.03 ms /   418 tokens (    1.21 ms per token,   824.41 tokens per second)\nllama_perf_context_print:        eval time =    3520.05 ms /   134 runs   (   26.27 ms per token,    38.07 tokens per second)\nllama_perf_context_print:       total time =    4183.53 ms /   552 tokens\nllama_perf_context_print:    graphs reused =        129\n 20%|██        | 55/270 [03:39<15:13,  4.25s/it]Llama.generate: 82 prefix-match hit, remaining 435 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     531.87 ms /   435 tokens (    1.22 ms per token,   817.87 tokens per second)\nllama_perf_context_print:        eval time =    3028.32 ms /   116 runs   (   26.11 ms per token,    38.31 tokens per second)\nllama_perf_context_print:       total time =    3693.47 ms /   551 tokens\nllama_perf_context_print:    graphs reused =        112\n 21%|██        | 56/270 [03:42<14:33,  4.08s/it]Llama.generate: 83 prefix-match hit, remaining 383 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     426.04 ms /   383 tokens (    1.11 ms per token,   898.97 tokens per second)\nllama_perf_context_print:        eval time =    3239.02 ms /   122 runs   (   26.55 ms per token,    37.67 tokens per second)\nllama_perf_context_print:       total time =    3812.94 ms /   505 tokens\nllama_perf_context_print:    graphs reused =        117\n 21%|██        | 57/270 [03:46<14:12,  4.00s/it]Llama.generate: 82 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     499.83 ms /   398 tokens (    1.26 ms per token,   796.27 tokens per second)\nllama_perf_context_print:        eval time =    2514.03 ms /    95 runs   (   26.46 ms per token,    37.79 tokens per second)\nllama_perf_context_print:       total time =    3122.46 ms /   493 tokens\nllama_perf_context_print:    graphs reused =         92\n 21%|██▏       | 58/270 [03:49<13:13,  3.74s/it]Llama.generate: 82 prefix-match hit, remaining 396 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     498.94 ms /   396 tokens (    1.26 ms per token,   793.69 tokens per second)\nllama_perf_context_print:        eval time =    3094.31 ms /   117 runs   (   26.45 ms per token,    37.81 tokens per second)\nllama_perf_context_print:       total time =    3727.61 ms /   513 tokens\nllama_perf_context_print:    graphs reused =        112\n 22%|██▏       | 59/270 [03:53<13:08,  3.74s/it]Llama.generate: 82 prefix-match hit, remaining 396 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     499.87 ms /   396 tokens (    1.26 ms per token,   792.20 tokens per second)\nllama_perf_context_print:        eval time =    2963.38 ms /   112 runs   (   26.46 ms per token,    37.79 tokens per second)\nllama_perf_context_print:       total time =    3592.97 ms /   508 tokens\nllama_perf_context_print:    graphs reused =        107\n 22%|██▏       | 60/270 [03:57<12:56,  3.70s/it]Llama.generate: 82 prefix-match hit, remaining 372 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     419.14 ms /   372 tokens (    1.13 ms per token,   887.53 tokens per second)\nllama_perf_context_print:        eval time =    6126.01 ms /   232 runs   (   26.41 ms per token,    37.87 tokens per second)\nllama_perf_context_print:       total time =    6833.72 ms /   604 tokens\nllama_perf_context_print:    graphs reused =        224\n 23%|██▎       | 61/270 [04:04<16:09,  4.64s/it]Llama.generate: 82 prefix-match hit, remaining 412 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     500.39 ms /   412 tokens (    1.21 ms per token,   823.36 tokens per second)\nllama_perf_context_print:        eval time =    4119.79 ms /   157 runs   (   26.24 ms per token,    38.11 tokens per second)\nllama_perf_context_print:       total time =    4806.32 ms /   569 tokens\nllama_perf_context_print:    graphs reused =        151\n 23%|██▎       | 62/270 [04:08<16:15,  4.69s/it]Llama.generate: 82 prefix-match hit, remaining 366 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     419.72 ms /   366 tokens (    1.15 ms per token,   872.00 tokens per second)\nllama_perf_context_print:        eval time =    3127.47 ms /   117 runs   (   26.73 ms per token,    37.41 tokens per second)\nllama_perf_context_print:       total time =    3681.07 ms /   483 tokens\nllama_perf_context_print:    graphs reused =        113\n 23%|██▎       | 63/270 [04:12<15:08,  4.39s/it]Llama.generate: 82 prefix-match hit, remaining 367 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     420.94 ms /   367 tokens (    1.15 ms per token,   871.85 tokens per second)\nllama_perf_context_print:        eval time =    3520.71 ms /   132 runs   (   26.67 ms per token,    37.49 tokens per second)\nllama_perf_context_print:       total time =    4094.92 ms /   499 tokens\nllama_perf_context_print:    graphs reused =        127\n 24%|██▎       | 64/270 [04:16<14:46,  4.30s/it]Llama.generate: 82 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     498.03 ms /   398 tokens (    1.25 ms per token,   799.16 tokens per second)\nllama_perf_context_print:        eval time =    2813.04 ms /   106 runs   (   26.54 ms per token,    37.68 tokens per second)\nllama_perf_context_print:       total time =    3433.65 ms /   504 tokens\nllama_perf_context_print:    graphs reused =        102\n 24%|██▍       | 65/270 [04:20<13:48,  4.04s/it]Llama.generate: 82 prefix-match hit, remaining 369 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     421.61 ms /   369 tokens (    1.14 ms per token,   875.21 tokens per second)\nllama_perf_context_print:        eval time =    3186.11 ms /   119 runs   (   26.77 ms per token,    37.35 tokens per second)\nllama_perf_context_print:       total time =    3744.50 ms /   488 tokens\nllama_perf_context_print:    graphs reused =        115\n 24%|██▍       | 66/270 [04:23<13:26,  3.96s/it]Llama.generate: 82 prefix-match hit, remaining 429 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     506.97 ms /   429 tokens (    1.18 ms per token,   846.20 tokens per second)\nllama_perf_context_print:        eval time =    3692.35 ms /   141 runs   (   26.19 ms per token,    38.19 tokens per second)\nllama_perf_context_print:       total time =    4365.23 ms /   570 tokens\nllama_perf_context_print:    graphs reused =        135\n 25%|██▍       | 67/270 [04:28<13:48,  4.08s/it]Llama.generate: 82 prefix-match hit, remaining 402 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     505.02 ms /   402 tokens (    1.26 ms per token,   796.01 tokens per second)\nllama_perf_context_print:        eval time =    3331.30 ms /   126 runs   (   26.44 ms per token,    37.82 tokens per second)\nllama_perf_context_print:       total time =    3982.72 ms /   528 tokens\nllama_perf_context_print:    graphs reused =        121\n 25%|██▌       | 68/270 [04:32<13:38,  4.05s/it]Llama.generate: 82 prefix-match hit, remaining 434 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     528.23 ms /   434 tokens (    1.22 ms per token,   821.62 tokens per second)\nllama_perf_context_print:        eval time =    2850.45 ms /   109 runs   (   26.15 ms per token,    38.24 tokens per second)\nllama_perf_context_print:       total time =    3504.19 ms /   543 tokens\nllama_perf_context_print:    graphs reused =        105\n 26%|██▌       | 69/270 [04:35<13:01,  3.89s/it]Llama.generate: 82 prefix-match hit, remaining 308 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     373.54 ms /   308 tokens (    1.21 ms per token,   824.55 tokens per second)\nllama_perf_context_print:        eval time =    2891.00 ms /   106 runs   (   27.27 ms per token,    36.67 tokens per second)\nllama_perf_context_print:       total time =    3387.51 ms /   414 tokens\nllama_perf_context_print:    graphs reused =        102\n 26%|██▌       | 70/270 [04:39<12:27,  3.74s/it]Llama.generate: 82 prefix-match hit, remaining 393 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     500.15 ms /   393 tokens (    1.27 ms per token,   785.77 tokens per second)\nllama_perf_context_print:        eval time =    3078.07 ms /   116 runs   (   26.54 ms per token,    37.69 tokens per second)\nllama_perf_context_print:       total time =    3716.14 ms /   509 tokens\nllama_perf_context_print:    graphs reused =        111\n 26%|██▋       | 71/270 [04:42<12:23,  3.73s/it]Llama.generate: 82 prefix-match hit, remaining 383 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     427.93 ms /   383 tokens (    1.12 ms per token,   895.01 tokens per second)\nllama_perf_context_print:        eval time =    3038.14 ms /   114 runs   (   26.65 ms per token,    37.52 tokens per second)\nllama_perf_context_print:       total time =    3598.32 ms /   497 tokens\nllama_perf_context_print:    graphs reused =        109\n 27%|██▋       | 72/270 [04:46<12:11,  3.70s/it]Llama.generate: 83 prefix-match hit, remaining 394 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     500.76 ms /   394 tokens (    1.27 ms per token,   786.81 tokens per second)\nllama_perf_context_print:        eval time =    2897.18 ms /   109 runs   (   26.58 ms per token,    37.62 tokens per second)\nllama_perf_context_print:       total time =    3522.79 ms /   503 tokens\nllama_perf_context_print:    graphs reused =        104\n 27%|██▋       | 73/270 [04:49<11:58,  3.65s/it]Llama.generate: 82 prefix-match hit, remaining 410 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     505.38 ms /   410 tokens (    1.23 ms per token,   811.26 tokens per second)\nllama_perf_context_print:        eval time =    3092.96 ms /   117 runs   (   26.44 ms per token,    37.83 tokens per second)\nllama_perf_context_print:       total time =    3734.14 ms /   527 tokens\nllama_perf_context_print:    graphs reused =        112\n 27%|██▋       | 74/270 [04:53<11:59,  3.67s/it]Llama.generate: 82 prefix-match hit, remaining 435 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     529.47 ms /   435 tokens (    1.22 ms per token,   821.58 tokens per second)\nllama_perf_context_print:        eval time =    3327.54 ms /   127 runs   (   26.20 ms per token,    38.17 tokens per second)\nllama_perf_context_print:       total time =    4006.47 ms /   562 tokens\nllama_perf_context_print:    graphs reused =        122\n 28%|██▊       | 75/270 [04:57<12:16,  3.77s/it]Llama.generate: 82 prefix-match hit, remaining 376 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     424.95 ms /   376 tokens (    1.13 ms per token,   884.81 tokens per second)\nllama_perf_context_print:        eval time =    3991.58 ms /   150 runs   (   26.61 ms per token,    37.58 tokens per second)\nllama_perf_context_print:       total time =    4593.01 ms /   526 tokens\nllama_perf_context_print:    graphs reused =        145\n 28%|██▊       | 76/270 [05:02<13:00,  4.02s/it]Llama.generate: 82 prefix-match hit, remaining 435 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     529.86 ms /   435 tokens (    1.22 ms per token,   820.97 tokens per second)\nllama_perf_context_print:        eval time =    4140.83 ms /   158 runs   (   26.21 ms per token,    38.16 tokens per second)\nllama_perf_context_print:       total time =    4863.09 ms /   593 tokens\nllama_perf_context_print:    graphs reused =        152\n 29%|██▊       | 77/270 [05:07<13:45,  4.28s/it]Llama.generate: 82 prefix-match hit, remaining 404 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     503.74 ms /   404 tokens (    1.25 ms per token,   802.01 tokens per second)\nllama_perf_context_print:        eval time =    4236.85 ms /   161 runs   (   26.32 ms per token,    38.00 tokens per second)\nllama_perf_context_print:       total time =    4933.14 ms /   565 tokens\nllama_perf_context_print:    graphs reused =        155\n 29%|██▉       | 78/270 [05:12<14:19,  4.47s/it]Llama.generate: 82 prefix-match hit, remaining 438 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     530.48 ms /   438 tokens (    1.21 ms per token,   825.67 tokens per second)\nllama_perf_context_print:        eval time =    2847.52 ms /   109 runs   (   26.12 ms per token,    38.28 tokens per second)\nllama_perf_context_print:       total time =    3503.64 ms /   547 tokens\nllama_perf_context_print:    graphs reused =        105\n 29%|██▉       | 79/270 [05:15<13:19,  4.18s/it]Llama.generate: 82 prefix-match hit, remaining 386 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.88 ms /   386 tokens (    1.29 ms per token,   775.29 tokens per second)\nllama_perf_context_print:        eval time =    2924.10 ms /   110 runs   (   26.58 ms per token,    37.62 tokens per second)\nllama_perf_context_print:       total time =    3552.43 ms /   496 tokens\nllama_perf_context_print:    graphs reused =        105\n 30%|██▉       | 80/270 [05:19<12:39,  4.00s/it]Llama.generate: 82 prefix-match hit, remaining 385 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.05 ms /   385 tokens (    1.29 ms per token,   774.56 tokens per second)\nllama_perf_context_print:        eval time =    3498.74 ms /   132 runs   (   26.51 ms per token,    37.73 tokens per second)\nllama_perf_context_print:       total time =    4149.04 ms /   517 tokens\nllama_perf_context_print:    graphs reused =        127\n 30%|███       | 81/270 [05:23<12:44,  4.04s/it]Llama.generate: 82 prefix-match hit, remaining 394 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.89 ms /   394 tokens (    1.26 ms per token,   791.33 tokens per second)\nllama_perf_context_print:        eval time =    3017.97 ms /   114 runs   (   26.47 ms per token,    37.77 tokens per second)\nllama_perf_context_print:       total time =    3648.22 ms /   508 tokens\nllama_perf_context_print:    graphs reused =        109\n 30%|███       | 82/270 [05:27<12:18,  3.93s/it]Llama.generate: 82 prefix-match hit, remaining 388 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     496.68 ms /   388 tokens (    1.28 ms per token,   781.19 tokens per second)\nllama_perf_context_print:        eval time =    2714.87 ms /   102 runs   (   26.62 ms per token,    37.57 tokens per second)\nllama_perf_context_print:       total time =    3329.39 ms /   490 tokens\nllama_perf_context_print:    graphs reused =         98\n 31%|███       | 83/270 [05:30<11:41,  3.75s/it]Llama.generate: 82 prefix-match hit, remaining 420 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     504.10 ms /   420 tokens (    1.20 ms per token,   833.16 tokens per second)\nllama_perf_context_print:        eval time =    4194.93 ms /   160 runs   (   26.22 ms per token,    38.14 tokens per second)\nllama_perf_context_print:       total time =    4887.32 ms /   580 tokens\nllama_perf_context_print:    graphs reused =        154\n 31%|███       | 84/270 [05:35<12:41,  4.09s/it]Llama.generate: 82 prefix-match hit, remaining 318 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     371.75 ms /   318 tokens (    1.17 ms per token,   855.42 tokens per second)\nllama_perf_context_print:        eval time =    2856.88 ms /   105 runs   (   27.21 ms per token,    36.75 tokens per second)\nllama_perf_context_print:       total time =    3350.66 ms /   423 tokens\nllama_perf_context_print:    graphs reused =        101\n 31%|███▏      | 85/270 [05:38<11:56,  3.87s/it]Llama.generate: 82 prefix-match hit, remaining 397 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     496.45 ms /   397 tokens (    1.25 ms per token,   799.68 tokens per second)\nllama_perf_context_print:        eval time =    3226.79 ms /   122 runs   (   26.45 ms per token,    37.81 tokens per second)\nllama_perf_context_print:       total time =    3863.82 ms /   519 tokens\nllama_perf_context_print:    graphs reused =        117\n 32%|███▏      | 86/270 [05:42<11:52,  3.87s/it]Llama.generate: 82 prefix-match hit, remaining 387 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     496.56 ms /   387 tokens (    1.28 ms per token,   779.36 tokens per second)\nllama_perf_context_print:        eval time =    4546.06 ms /   172 runs   (   26.43 ms per token,    37.83 tokens per second)\nllama_perf_context_print:       total time =    5249.14 ms /   559 tokens\nllama_perf_context_print:    graphs reused =        165\n 32%|███▏      | 87/270 [05:47<13:04,  4.29s/it]Llama.generate: 82 prefix-match hit, remaining 391 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.11 ms /   391 tokens (    1.27 ms per token,   786.55 tokens per second)\nllama_perf_context_print:        eval time =    3545.58 ms /   134 runs   (   26.46 ms per token,    37.79 tokens per second)\nllama_perf_context_print:       total time =    4200.80 ms /   525 tokens\nllama_perf_context_print:    graphs reused =        129\n 33%|███▎      | 88/270 [05:51<12:55,  4.26s/it]Llama.generate: 82 prefix-match hit, remaining 405 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     500.57 ms /   405 tokens (    1.24 ms per token,   809.08 tokens per second)\nllama_perf_context_print:        eval time =    2979.95 ms /   113 runs   (   26.37 ms per token,    37.92 tokens per second)\nllama_perf_context_print:       total time =    3611.74 ms /   518 tokens\nllama_perf_context_print:    graphs reused =        109\n 33%|███▎      | 89/270 [05:55<12:16,  4.07s/it]Llama.generate: 82 prefix-match hit, remaining 395 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     496.67 ms /   395 tokens (    1.26 ms per token,   795.30 tokens per second)\nllama_perf_context_print:        eval time =    2731.40 ms /   103 runs   (   26.52 ms per token,    37.71 tokens per second)\nllama_perf_context_print:       total time =    3348.00 ms /   498 tokens\nllama_perf_context_print:    graphs reused =         98\n 33%|███▎      | 90/270 [05:58<11:33,  3.85s/it]Llama.generate: 82 prefix-match hit, remaining 408 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     498.69 ms /   408 tokens (    1.22 ms per token,   818.14 tokens per second)\nllama_perf_context_print:        eval time =    3105.27 ms /   118 runs   (   26.32 ms per token,    38.00 tokens per second)\nllama_perf_context_print:       total time =    3740.93 ms /   526 tokens\nllama_perf_context_print:    graphs reused =        114\n 34%|███▎      | 91/270 [06:02<11:23,  3.82s/it]Llama.generate: 85 prefix-match hit, remaining 388 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     499.52 ms /   388 tokens (    1.29 ms per token,   776.75 tokens per second)\nllama_perf_context_print:        eval time =    3548.22 ms /   134 runs   (   26.48 ms per token,    37.77 tokens per second)\nllama_perf_context_print:       total time =    4206.03 ms /   522 tokens\nllama_perf_context_print:    graphs reused =        129\n 34%|███▍      | 92/270 [06:06<11:41,  3.94s/it]Llama.generate: 82 prefix-match hit, remaining 399 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     500.43 ms /   399 tokens (    1.25 ms per token,   797.32 tokens per second)\nllama_perf_context_print:        eval time =    4118.03 ms /   156 runs   (   26.40 ms per token,    37.88 tokens per second)\nllama_perf_context_print:       total time =    4802.35 ms /   555 tokens\nllama_perf_context_print:    graphs reused =        151\n 34%|███▍      | 93/270 [06:11<12:23,  4.20s/it]Llama.generate: 82 prefix-match hit, remaining 389 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     498.06 ms /   389 tokens (    1.28 ms per token,   781.03 tokens per second)\nllama_perf_context_print:        eval time =    3502.12 ms /   132 runs   (   26.53 ms per token,    37.69 tokens per second)\nllama_perf_context_print:       total time =    4152.89 ms /   521 tokens\nllama_perf_context_print:    graphs reused =        127\n 35%|███▍      | 94/270 [06:15<12:16,  4.19s/it]Llama.generate: 82 prefix-match hit, remaining 415 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     503.81 ms /   415 tokens (    1.21 ms per token,   823.72 tokens per second)\nllama_perf_context_print:        eval time =    2898.33 ms /   110 runs   (   26.35 ms per token,    37.95 tokens per second)\nllama_perf_context_print:       total time =    3529.64 ms /   525 tokens\nllama_perf_context_print:    graphs reused =        106\n 35%|███▌      | 95/270 [06:19<11:38,  3.99s/it]Llama.generate: 82 prefix-match hit, remaining 431 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     527.36 ms /   431 tokens (    1.22 ms per token,   817.28 tokens per second)\nllama_perf_context_print:        eval time =    2854.28 ms /   109 runs   (   26.19 ms per token,    38.19 tokens per second)\nllama_perf_context_print:       total time =    3507.82 ms /   540 tokens\nllama_perf_context_print:    graphs reused =        105\n 36%|███▌      | 96/270 [06:22<11:09,  3.85s/it]Llama.generate: 82 prefix-match hit, remaining 427 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     509.91 ms /   427 tokens (    1.19 ms per token,   837.41 tokens per second)\nllama_perf_context_print:        eval time =    4357.43 ms /   166 runs   (   26.25 ms per token,    38.10 tokens per second)\nllama_perf_context_print:       total time =    5067.59 ms /   593 tokens\nllama_perf_context_print:    graphs reused =        159\n 36%|███▌      | 97/270 [06:27<12:09,  4.22s/it]Llama.generate: 82 prefix-match hit, remaining 400 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.87 ms /   400 tokens (    1.25 ms per token,   797.01 tokens per second)\nllama_perf_context_print:        eval time =    2781.48 ms /   105 runs   (   26.49 ms per token,    37.75 tokens per second)\nllama_perf_context_print:       total time =    3406.96 ms /   505 tokens\nllama_perf_context_print:    graphs reused =        101\n 36%|███▋      | 98/270 [06:31<11:23,  3.97s/it]Llama.generate: 82 prefix-match hit, remaining 305 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     373.94 ms /   305 tokens (    1.23 ms per token,   815.63 tokens per second)\nllama_perf_context_print:        eval time =    5467.04 ms /   204 runs   (   26.80 ms per token,    37.31 tokens per second)\nllama_perf_context_print:       total time =    6091.36 ms /   509 tokens\nllama_perf_context_print:    graphs reused =        197\n 37%|███▋      | 99/270 [06:37<13:08,  4.61s/it]Llama.generate: 82 prefix-match hit, remaining 386 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.17 ms /   386 tokens (    1.30 ms per token,   770.20 tokens per second)\nllama_perf_context_print:        eval time =    3007.40 ms /   113 runs   (   26.61 ms per token,    37.57 tokens per second)\nllama_perf_context_print:       total time =    3638.50 ms /   499 tokens\nllama_perf_context_print:    graphs reused =        108\n 37%|███▋      | 100/270 [06:41<12:14,  4.32s/it]Llama.generate: 82 prefix-match hit, remaining 374 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     427.28 ms /   374 tokens (    1.14 ms per token,   875.31 tokens per second)\nllama_perf_context_print:        eval time =    3862.35 ms /   145 runs   (   26.64 ms per token,    37.54 tokens per second)\nllama_perf_context_print:       total time =    4461.27 ms /   519 tokens\nllama_perf_context_print:    graphs reused =        140\n 37%|███▋      | 101/270 [06:45<12:17,  4.36s/it]Llama.generate: 82 prefix-match hit, remaining 381 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     426.21 ms /   381 tokens (    1.12 ms per token,   893.93 tokens per second)\nllama_perf_context_print:        eval time =    2930.02 ms /   110 runs   (   26.64 ms per token,    37.54 tokens per second)\nllama_perf_context_print:       total time =    3484.42 ms /   491 tokens\nllama_perf_context_print:    graphs reused =        106\n 38%|███▊      | 102/270 [06:49<11:29,  4.10s/it]Llama.generate: 84 prefix-match hit, remaining 403 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     504.39 ms /   403 tokens (    1.25 ms per token,   798.98 tokens per second)\nllama_perf_context_print:        eval time =    2928.30 ms /   111 runs   (   26.38 ms per token,    37.91 tokens per second)\nllama_perf_context_print:       total time =    3558.75 ms /   514 tokens\nllama_perf_context_print:    graphs reused =        107\n 38%|███▊      | 103/270 [06:52<10:58,  3.94s/it]Llama.generate: 84 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     502.06 ms /   398 tokens (    1.26 ms per token,   792.73 tokens per second)\nllama_perf_context_print:        eval time =    2961.49 ms /   112 runs   (   26.44 ms per token,    37.82 tokens per second)\nllama_perf_context_print:       total time =    3585.81 ms /   510 tokens\nllama_perf_context_print:    graphs reused =        108\n 39%|███▊      | 104/270 [06:56<10:36,  3.84s/it]Llama.generate: 82 prefix-match hit, remaining 427 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     507.91 ms /   427 tokens (    1.19 ms per token,   840.70 tokens per second)\nllama_perf_context_print:        eval time =    3372.56 ms /   129 runs   (   26.14 ms per token,    38.25 tokens per second)\nllama_perf_context_print:       total time =    4025.40 ms /   556 tokens\nllama_perf_context_print:    graphs reused =        124\n 39%|███▉      | 105/270 [07:00<10:42,  3.89s/it]Llama.generate: 82 prefix-match hit, remaining 412 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     505.57 ms /   412 tokens (    1.23 ms per token,   814.92 tokens per second)\nllama_perf_context_print:        eval time =    3423.69 ms /   130 runs   (   26.34 ms per token,    37.97 tokens per second)\nllama_perf_context_print:       total time =    4078.82 ms /   542 tokens\nllama_perf_context_print:    graphs reused =        125\n 39%|███▉      | 106/270 [07:04<10:47,  3.95s/it]Llama.generate: 82 prefix-match hit, remaining 379 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     425.24 ms /   379 tokens (    1.12 ms per token,   891.25 tokens per second)\nllama_perf_context_print:        eval time =    2960.84 ms /   111 runs   (   26.67 ms per token,    37.49 tokens per second)\nllama_perf_context_print:       total time =    3513.32 ms /   490 tokens\nllama_perf_context_print:    graphs reused =        107\n 40%|███▉      | 107/270 [07:07<10:22,  3.82s/it]Llama.generate: 86 prefix-match hit, remaining 390 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     499.64 ms /   390 tokens (    1.28 ms per token,   780.57 tokens per second)\nllama_perf_context_print:        eval time =    4102.18 ms /   155 runs   (   26.47 ms per token,    37.78 tokens per second)\nllama_perf_context_print:       total time =    4781.34 ms /   545 tokens\nllama_perf_context_print:    graphs reused =        149\n 40%|████      | 108/270 [07:12<11:05,  4.11s/it]Llama.generate: 82 prefix-match hit, remaining 355 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     419.01 ms /   355 tokens (    1.18 ms per token,   847.24 tokens per second)\nllama_perf_context_print:        eval time =    3334.41 ms /   124 runs   (   26.89 ms per token,    37.19 tokens per second)\nllama_perf_context_print:       total time =    3902.95 ms /   479 tokens\nllama_perf_context_print:    graphs reused =        119\n 40%|████      | 109/270 [07:16<10:52,  4.05s/it]Llama.generate: 84 prefix-match hit, remaining 420 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     507.35 ms /   420 tokens (    1.21 ms per token,   827.84 tokens per second)\nllama_perf_context_print:        eval time =    3541.50 ms /   135 runs   (   26.23 ms per token,    38.12 tokens per second)\nllama_perf_context_print:       total time =    4207.45 ms /   555 tokens\nllama_perf_context_print:    graphs reused =        130\n 41%|████      | 110/270 [07:20<10:55,  4.10s/it]Llama.generate: 84 prefix-match hit, remaining 386 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     496.95 ms /   386 tokens (    1.29 ms per token,   776.74 tokens per second)\nllama_perf_context_print:        eval time =    3686.95 ms /   139 runs   (   26.52 ms per token,    37.70 tokens per second)\nllama_perf_context_print:       total time =    4346.21 ms /   525 tokens\nllama_perf_context_print:    graphs reused =        133\n 41%|████      | 111/270 [07:25<11:03,  4.17s/it]Llama.generate: 82 prefix-match hit, remaining 377 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     426.13 ms /   377 tokens (    1.13 ms per token,   884.70 tokens per second)\nllama_perf_context_print:        eval time =    2861.30 ms /   107 runs   (   26.74 ms per token,    37.40 tokens per second)\nllama_perf_context_print:       total time =    3410.59 ms /   484 tokens\nllama_perf_context_print:    graphs reused =        103\n 41%|████▏     | 112/270 [07:28<10:23,  3.95s/it]Llama.generate: 82 prefix-match hit, remaining 407 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     503.20 ms /   407 tokens (    1.24 ms per token,   808.82 tokens per second)\nllama_perf_context_print:        eval time =    3504.42 ms /   133 runs   (   26.35 ms per token,    37.95 tokens per second)\nllama_perf_context_print:       total time =    4162.14 ms /   540 tokens\nllama_perf_context_print:    graphs reused =        128\n 42%|████▏     | 113/270 [07:32<10:30,  4.01s/it]Llama.generate: 82 prefix-match hit, remaining 395 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     504.91 ms /   395 tokens (    1.28 ms per token,   782.31 tokens per second)\nllama_perf_context_print:        eval time =    3518.69 ms /   133 runs   (   26.46 ms per token,    37.80 tokens per second)\nllama_perf_context_print:       total time =    4179.79 ms /   528 tokens\nllama_perf_context_print:    graphs reused =        127\n 42%|████▏     | 114/270 [07:36<10:34,  4.06s/it]Llama.generate: 83 prefix-match hit, remaining 421 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     506.14 ms /   421 tokens (    1.20 ms per token,   831.78 tokens per second)\nllama_perf_context_print:        eval time =    3482.36 ms /   133 runs   (   26.18 ms per token,    38.19 tokens per second)\nllama_perf_context_print:       total time =    4143.05 ms /   554 tokens\nllama_perf_context_print:    graphs reused =        128\n 43%|████▎     | 115/270 [07:41<10:33,  4.09s/it]Llama.generate: 82 prefix-match hit, remaining 420 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     504.81 ms /   420 tokens (    1.20 ms per token,   831.99 tokens per second)\nllama_perf_context_print:        eval time =    3331.28 ms /   127 runs   (   26.23 ms per token,    38.12 tokens per second)\nllama_perf_context_print:       total time =    3983.27 ms /   547 tokens\nllama_perf_context_print:    graphs reused =        122\n 43%|████▎     | 116/270 [07:45<10:25,  4.06s/it]Llama.generate: 84 prefix-match hit, remaining 397 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.38 ms /   397 tokens (    1.26 ms per token,   791.82 tokens per second)\nllama_perf_context_print:        eval time =    3486.04 ms /   132 runs   (   26.41 ms per token,    37.87 tokens per second)\nllama_perf_context_print:       total time =    4143.89 ms /   529 tokens\nllama_perf_context_print:    graphs reused =        127\n 43%|████▎     | 117/270 [07:49<10:25,  4.09s/it]Llama.generate: 84 prefix-match hit, remaining 385 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.92 ms /   385 tokens (    1.29 ms per token,   773.22 tokens per second)\nllama_perf_context_print:        eval time =    3445.11 ms /   130 runs   (   26.50 ms per token,    37.73 tokens per second)\nllama_perf_context_print:       total time =    4097.16 ms /   515 tokens\nllama_perf_context_print:    graphs reused =        125\n 44%|████▎     | 118/270 [07:53<10:21,  4.09s/it]Llama.generate: 82 prefix-match hit, remaining 399 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     499.71 ms /   399 tokens (    1.25 ms per token,   798.46 tokens per second)\nllama_perf_context_print:        eval time =    3484.80 ms /   132 runs   (   26.40 ms per token,    37.88 tokens per second)\nllama_perf_context_print:       total time =    4141.97 ms /   531 tokens\nllama_perf_context_print:    graphs reused =        127\n 44%|████▍     | 119/270 [07:57<10:20,  4.11s/it]Llama.generate: 82 prefix-match hit, remaining 481 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     582.09 ms /   481 tokens (    1.21 ms per token,   826.34 tokens per second)\nllama_perf_context_print:        eval time =    6373.19 ms /   242 runs   (   26.34 ms per token,    37.97 tokens per second)\nllama_perf_context_print:       total time =    7257.00 ms /   723 tokens\nllama_perf_context_print:    graphs reused =        233\n 44%|████▍     | 120/270 [08:04<12:38,  5.05s/it]Llama.generate: 82 prefix-match hit, remaining 408 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     500.78 ms /   408 tokens (    1.23 ms per token,   814.73 tokens per second)\nllama_perf_context_print:        eval time =    3579.37 ms /   136 runs   (   26.32 ms per token,    38.00 tokens per second)\nllama_perf_context_print:       total time =    4239.39 ms /   544 tokens\nllama_perf_context_print:    graphs reused =        131\n 45%|████▍     | 121/270 [08:08<11:56,  4.81s/it]Llama.generate: 82 prefix-match hit, remaining 396 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     504.54 ms /   396 tokens (    1.27 ms per token,   784.87 tokens per second)\nllama_perf_context_print:        eval time =    3729.82 ms /   141 runs   (   26.45 ms per token,    37.80 tokens per second)\nllama_perf_context_print:       total time =    4399.78 ms /   537 tokens\nllama_perf_context_print:    graphs reused =        135\n 45%|████▌     | 122/270 [08:13<11:34,  4.69s/it]Llama.generate: 84 prefix-match hit, remaining 388 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.57 ms /   388 tokens (    1.28 ms per token,   779.79 tokens per second)\nllama_perf_context_print:        eval time =    3005.74 ms /   113 runs   (   26.60 ms per token,    37.59 tokens per second)\nllama_perf_context_print:       total time =    3633.36 ms /   501 tokens\nllama_perf_context_print:    graphs reused =        108\n 46%|████▌     | 123/270 [08:16<10:42,  4.37s/it]Llama.generate: 82 prefix-match hit, remaining 413 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     504.54 ms /   413 tokens (    1.22 ms per token,   818.57 tokens per second)\nllama_perf_context_print:        eval time =    5428.64 ms /   206 runs   (   26.35 ms per token,    37.95 tokens per second)\nllama_perf_context_print:       total time =    6184.94 ms /   619 tokens\nllama_perf_context_print:    graphs reused =        199\n 46%|████▌     | 124/270 [08:23<11:58,  4.92s/it]Llama.generate: 82 prefix-match hit, remaining 377 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     424.90 ms /   377 tokens (    1.13 ms per token,   887.27 tokens per second)\nllama_perf_context_print:        eval time =    3022.39 ms /   113 runs   (   26.75 ms per token,    37.39 tokens per second)\nllama_perf_context_print:       total time =    3578.03 ms /   490 tokens\nllama_perf_context_print:    graphs reused =        109\n 46%|████▋     | 125/270 [08:26<10:55,  4.52s/it]Llama.generate: 82 prefix-match hit, remaining 427 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     509.42 ms /   427 tokens (    1.19 ms per token,   838.21 tokens per second)\nllama_perf_context_print:        eval time =    5332.66 ms /   203 runs   (   26.27 ms per token,    38.07 tokens per second)\nllama_perf_context_print:       total time =    6089.07 ms /   630 tokens\nllama_perf_context_print:    graphs reused =        195\n 47%|████▋     | 126/270 [08:32<11:58,  4.99s/it]Llama.generate: 82 prefix-match hit, remaining 406 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     502.18 ms /   406 tokens (    1.24 ms per token,   808.47 tokens per second)\nllama_perf_context_print:        eval time =    3407.22 ms /   129 runs   (   26.41 ms per token,    37.86 tokens per second)\nllama_perf_context_print:       total time =    4062.45 ms /   535 tokens\nllama_perf_context_print:    graphs reused =        124\n 47%|████▋     | 127/270 [08:36<11:14,  4.71s/it]Llama.generate: 82 prefix-match hit, remaining 299 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     371.52 ms /   299 tokens (    1.24 ms per token,   804.81 tokens per second)\nllama_perf_context_print:        eval time =    4768.90 ms /   177 runs   (   26.94 ms per token,    37.12 tokens per second)\nllama_perf_context_print:       total time =    5352.31 ms /   476 tokens\nllama_perf_context_print:    graphs reused =        170\n 47%|████▋     | 128/270 [08:42<11:36,  4.91s/it]Llama.generate: 82 prefix-match hit, remaining 406 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.76 ms /   406 tokens (    1.24 ms per token,   809.14 tokens per second)\nllama_perf_context_print:        eval time =    3712.19 ms /   141 runs   (   26.33 ms per token,    37.98 tokens per second)\nllama_perf_context_print:       total time =    4379.32 ms /   547 tokens\nllama_perf_context_print:    graphs reused =        136\n 48%|████▊     | 129/270 [08:46<11:09,  4.75s/it]Llama.generate: 82 prefix-match hit, remaining 403 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.75 ms /   403 tokens (    1.25 ms per token,   803.19 tokens per second)\nllama_perf_context_print:        eval time =    3011.32 ms /   114 runs   (   26.42 ms per token,    37.86 tokens per second)\nllama_perf_context_print:       total time =    3644.08 ms /   517 tokens\nllama_perf_context_print:    graphs reused =        110\n 48%|████▊     | 130/270 [08:50<10:18,  4.42s/it]Llama.generate: 82 prefix-match hit, remaining 416 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     502.16 ms /   416 tokens (    1.21 ms per token,   828.42 tokens per second)\nllama_perf_context_print:        eval time =    3180.45 ms /   121 runs   (   26.28 ms per token,    38.04 tokens per second)\nllama_perf_context_print:       total time =    3823.35 ms /   537 tokens\nllama_perf_context_print:    graphs reused =        116\n 49%|████▊     | 131/270 [08:54<09:49,  4.24s/it]Llama.generate: 82 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     498.12 ms /   398 tokens (    1.25 ms per token,   799.01 tokens per second)\nllama_perf_context_print:        eval time =    3279.14 ms /   124 runs   (   26.44 ms per token,    37.81 tokens per second)\nllama_perf_context_print:       total time =    3922.29 ms /   522 tokens\nllama_perf_context_print:    graphs reused =        120\n 49%|████▉     | 132/270 [08:58<09:32,  4.15s/it]Llama.generate: 82 prefix-match hit, remaining 417 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     503.35 ms /   417 tokens (    1.21 ms per token,   828.45 tokens per second)\nllama_perf_context_print:        eval time =    2995.09 ms /   114 runs   (   26.27 ms per token,    38.06 tokens per second)\nllama_perf_context_print:       total time =    3629.71 ms /   531 tokens\nllama_perf_context_print:    graphs reused =        109\n 49%|████▉     | 133/270 [09:01<09:07,  3.99s/it]Llama.generate: 82 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     499.81 ms /   398 tokens (    1.26 ms per token,   796.31 tokens per second)\nllama_perf_context_print:        eval time =    2834.33 ms /   107 runs   (   26.49 ms per token,    37.75 tokens per second)\nllama_perf_context_print:       total time =    3457.74 ms /   505 tokens\nllama_perf_context_print:    graphs reused =        103\n 50%|████▉     | 134/270 [09:05<08:41,  3.83s/it]Llama.generate: 85 prefix-match hit, remaining 271 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     337.32 ms /   271 tokens (    1.24 ms per token,   803.38 tokens per second)\nllama_perf_context_print:        eval time =    2846.55 ms /   105 runs   (   27.11 ms per token,    36.89 tokens per second)\nllama_perf_context_print:       total time =    3305.78 ms /   376 tokens\nllama_perf_context_print:    graphs reused =        101\n 50%|█████     | 135/270 [09:08<08:16,  3.68s/it]Llama.generate: 82 prefix-match hit, remaining 405 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.38 ms /   405 tokens (    1.24 ms per token,   807.78 tokens per second)\nllama_perf_context_print:        eval time =    4030.48 ms /   153 runs   (   26.34 ms per token,    37.96 tokens per second)\nllama_perf_context_print:       total time =    4717.15 ms /   558 tokens\nllama_perf_context_print:    graphs reused =        148\n 50%|█████     | 136/270 [09:13<08:54,  3.99s/it]Llama.generate: 82 prefix-match hit, remaining 406 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     502.42 ms /   406 tokens (    1.24 ms per token,   808.08 tokens per second)\nllama_perf_context_print:        eval time =    2851.90 ms /   108 runs   (   26.41 ms per token,    37.87 tokens per second)\nllama_perf_context_print:       total time =    3479.94 ms /   514 tokens\nllama_perf_context_print:    graphs reused =        104\n 51%|█████     | 137/270 [09:16<08:30,  3.84s/it]Llama.generate: 82 prefix-match hit, remaining 409 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     500.45 ms /   409 tokens (    1.22 ms per token,   817.27 tokens per second)\nllama_perf_context_print:        eval time =    3447.69 ms /   131 runs   (   26.32 ms per token,    38.00 tokens per second)\nllama_perf_context_print:       total time =    4103.50 ms /   540 tokens\nllama_perf_context_print:    graphs reused =        126\n 51%|█████     | 138/270 [09:20<08:37,  3.92s/it]Llama.generate: 82 prefix-match hit, remaining 400 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     498.16 ms /   400 tokens (    1.25 ms per token,   802.96 tokens per second)\nllama_perf_context_print:        eval time =    3404.84 ms /   129 runs   (   26.39 ms per token,    37.89 tokens per second)\nllama_perf_context_print:       total time =    4055.78 ms /   529 tokens\nllama_perf_context_print:    graphs reused =        124\n 51%|█████▏    | 139/270 [09:24<08:39,  3.96s/it]Llama.generate: 82 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     500.58 ms /   398 tokens (    1.26 ms per token,   795.07 tokens per second)\nllama_perf_context_print:        eval time =    3174.59 ms /   120 runs   (   26.45 ms per token,    37.80 tokens per second)\nllama_perf_context_print:       total time =    3816.58 ms /   518 tokens\nllama_perf_context_print:    graphs reused =        116\n 52%|█████▏    | 140/270 [09:28<08:29,  3.92s/it]Llama.generate: 82 prefix-match hit, remaining 378 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     424.94 ms /   378 tokens (    1.12 ms per token,   889.53 tokens per second)\nllama_perf_context_print:        eval time =    2882.07 ms /   108 runs   (   26.69 ms per token,    37.47 tokens per second)\nllama_perf_context_print:       total time =    3432.62 ms /   486 tokens\nllama_perf_context_print:    graphs reused =        104\n 52%|█████▏    | 141/270 [09:32<08:07,  3.78s/it]Llama.generate: 82 prefix-match hit, remaining 392 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     496.36 ms /   392 tokens (    1.27 ms per token,   789.76 tokens per second)\nllama_perf_context_print:        eval time =    4299.57 ms /   163 runs   (   26.38 ms per token,    37.91 tokens per second)\nllama_perf_context_print:       total time =    4991.70 ms /   555 tokens\nllama_perf_context_print:    graphs reused =        157\n 53%|█████▎    | 142/270 [09:37<08:50,  4.14s/it]Llama.generate: 85 prefix-match hit, remaining 271 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     336.84 ms /   271 tokens (    1.24 ms per token,   804.55 tokens per second)\nllama_perf_context_print:        eval time =    3358.13 ms /   124 runs   (   27.08 ms per token,    36.93 tokens per second)\nllama_perf_context_print:       total time =    3838.61 ms /   395 tokens\nllama_perf_context_print:    graphs reused =        120\n 53%|█████▎    | 143/270 [09:40<08:34,  4.05s/it]Llama.generate: 82 prefix-match hit, remaining 412 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     504.98 ms /   412 tokens (    1.23 ms per token,   815.88 tokens per second)\nllama_perf_context_print:        eval time =    2660.03 ms /   101 runs   (   26.34 ms per token,    37.97 tokens per second)\nllama_perf_context_print:       total time =    3280.21 ms /   513 tokens\nllama_perf_context_print:    graphs reused =         97\n 53%|█████▎    | 144/270 [09:44<08:01,  3.82s/it]Llama.generate: 82 prefix-match hit, remaining 418 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     505.51 ms /   418 tokens (    1.21 ms per token,   826.89 tokens per second)\nllama_perf_context_print:        eval time =    3145.76 ms /   120 runs   (   26.21 ms per token,    38.15 tokens per second)\nllama_perf_context_print:       total time =    3792.58 ms /   538 tokens\nllama_perf_context_print:    graphs reused =        115\n 54%|█████▎    | 145/270 [09:48<07:56,  3.81s/it]Llama.generate: 82 prefix-match hit, remaining 435 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     528.75 ms /   435 tokens (    1.22 ms per token,   822.70 tokens per second)\nllama_perf_context_print:        eval time =    3497.71 ms /   134 runs   (   26.10 ms per token,    38.31 tokens per second)\nllama_perf_context_print:       total time =    4184.90 ms /   569 tokens\nllama_perf_context_print:    graphs reused =        129\n 54%|█████▍    | 146/270 [09:52<08:07,  3.93s/it]Llama.generate: 82 prefix-match hit, remaining 372 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     421.69 ms /   372 tokens (    1.13 ms per token,   882.17 tokens per second)\nllama_perf_context_print:        eval time =    3645.56 ms /   137 runs   (   26.61 ms per token,    37.58 tokens per second)\nllama_perf_context_print:       total time =    4229.21 ms /   509 tokens\nllama_perf_context_print:    graphs reused =        132\n 54%|█████▍    | 147/270 [09:56<08:14,  4.02s/it]Llama.generate: 82 prefix-match hit, remaining 397 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     500.50 ms /   397 tokens (    1.26 ms per token,   793.20 tokens per second)\nllama_perf_context_print:        eval time =    3143.31 ms /   119 runs   (   26.41 ms per token,    37.86 tokens per second)\nllama_perf_context_print:       total time =    3783.93 ms /   516 tokens\nllama_perf_context_print:    graphs reused =        114\n 55%|█████▍    | 148/270 [10:00<08:01,  3.95s/it]Llama.generate: 86 prefix-match hit, remaining 380 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     425.15 ms /   380 tokens (    1.12 ms per token,   893.81 tokens per second)\nllama_perf_context_print:        eval time =    2953.94 ms /   111 runs   (   26.61 ms per token,    37.58 tokens per second)\nllama_perf_context_print:       total time =    3509.18 ms /   491 tokens\nllama_perf_context_print:    graphs reused =        106\n 55%|█████▌    | 149/270 [10:03<07:42,  3.82s/it]Llama.generate: 82 prefix-match hit, remaining 396 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.33 ms /   396 tokens (    1.27 ms per token,   789.90 tokens per second)\nllama_perf_context_print:        eval time =    4116.55 ms /   156 runs   (   26.39 ms per token,    37.90 tokens per second)\nllama_perf_context_print:       total time =    4805.34 ms /   552 tokens\nllama_perf_context_print:    graphs reused =        150\n 56%|█████▌    | 150/270 [10:08<08:13,  4.12s/it]Llama.generate: 82 prefix-match hit, remaining 396 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     502.21 ms /   396 tokens (    1.27 ms per token,   788.51 tokens per second)\nllama_perf_context_print:        eval time =    2840.03 ms /   107 runs   (   26.54 ms per token,    37.68 tokens per second)\nllama_perf_context_print:       total time =    3465.70 ms /   503 tokens\nllama_perf_context_print:    graphs reused =        102\n 56%|█████▌    | 151/270 [10:12<07:46,  3.92s/it]Llama.generate: 82 prefix-match hit, remaining 412 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     503.64 ms /   412 tokens (    1.22 ms per token,   818.04 tokens per second)\nllama_perf_context_print:        eval time =    3082.64 ms /   117 runs   (   26.35 ms per token,    37.95 tokens per second)\nllama_perf_context_print:       total time =    3721.50 ms /   529 tokens\nllama_perf_context_print:    graphs reused =        112\n 56%|█████▋    | 152/270 [10:15<07:35,  3.86s/it]Llama.generate: 82 prefix-match hit, remaining 366 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     424.62 ms /   366 tokens (    1.16 ms per token,   861.95 tokens per second)\nllama_perf_context_print:        eval time =    3869.74 ms /   145 runs   (   26.69 ms per token,    37.47 tokens per second)\nllama_perf_context_print:       total time =    4466.56 ms /   511 tokens\nllama_perf_context_print:    graphs reused =        140\n 57%|█████▋    | 153/270 [10:20<07:53,  4.05s/it]Llama.generate: 82 prefix-match hit, remaining 367 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     422.34 ms /   367 tokens (    1.15 ms per token,   868.97 tokens per second)\nllama_perf_context_print:        eval time =    3841.56 ms /   144 runs   (   26.68 ms per token,    37.48 tokens per second)\nllama_perf_context_print:       total time =    4432.64 ms /   511 tokens\nllama_perf_context_print:    graphs reused =        139\n 57%|█████▋    | 154/270 [10:24<08:02,  4.16s/it]Llama.generate: 82 prefix-match hit, remaining 369 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     423.60 ms /   369 tokens (    1.15 ms per token,   871.10 tokens per second)\nllama_perf_context_print:        eval time =    3632.19 ms /   136 runs   (   26.71 ms per token,    37.44 tokens per second)\nllama_perf_context_print:       total time =    4215.76 ms /   505 tokens\nllama_perf_context_print:    graphs reused =        131\n 57%|█████▋    | 155/270 [10:28<08:00,  4.18s/it]Llama.generate: 82 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.06 ms /   398 tokens (    1.26 ms per token,   794.32 tokens per second)\nllama_perf_context_print:        eval time =    2864.69 ms /   108 runs   (   26.52 ms per token,    37.70 tokens per second)\nllama_perf_context_print:       total time =    3492.63 ms /   506 tokens\nllama_perf_context_print:    graphs reused =        104\n 58%|█████▊    | 156/270 [10:32<07:33,  3.98s/it]Llama.generate: 82 prefix-match hit, remaining 434 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     531.38 ms /   434 tokens (    1.22 ms per token,   816.74 tokens per second)\nllama_perf_context_print:        eval time =    3219.67 ms /   123 runs   (   26.18 ms per token,    38.20 tokens per second)\nllama_perf_context_print:       total time =    3893.93 ms /   557 tokens\nllama_perf_context_print:    graphs reused =        119\n 58%|█████▊    | 157/270 [10:36<07:26,  3.95s/it]Llama.generate: 82 prefix-match hit, remaining 402 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     500.26 ms /   402 tokens (    1.24 ms per token,   803.58 tokens per second)\nllama_perf_context_print:        eval time =    4590.92 ms /   174 runs   (   26.38 ms per token,    37.90 tokens per second)\nllama_perf_context_print:       total time =    5299.74 ms /   576 tokens\nllama_perf_context_print:    graphs reused =        168\n 59%|█████▊    | 158/270 [10:41<08:08,  4.36s/it]Llama.generate: 82 prefix-match hit, remaining 429 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     506.73 ms /   429 tokens (    1.18 ms per token,   846.61 tokens per second)\nllama_perf_context_print:        eval time =    3450.72 ms /   132 runs   (   26.14 ms per token,    38.25 tokens per second)\nllama_perf_context_print:       total time =    4111.16 ms /   561 tokens\nllama_perf_context_print:    graphs reused =        126\n 59%|█████▉    | 159/270 [10:45<07:55,  4.29s/it]Llama.generate: 82 prefix-match hit, remaining 445 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     534.65 ms /   445 tokens (    1.20 ms per token,   832.32 tokens per second)\nllama_perf_context_print:        eval time =    2639.73 ms /   101 runs   (   26.14 ms per token,    38.26 tokens per second)\nllama_perf_context_print:       total time =    3291.65 ms /   546 tokens\nllama_perf_context_print:    graphs reused =         97\n 59%|█████▉    | 160/270 [10:48<07:18,  3.99s/it]Llama.generate: 84 prefix-match hit, remaining 391 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     499.65 ms /   391 tokens (    1.28 ms per token,   782.55 tokens per second)\nllama_perf_context_print:        eval time =    3052.53 ms /   115 runs   (   26.54 ms per token,    37.67 tokens per second)\nllama_perf_context_print:       total time =    3685.59 ms /   506 tokens\nllama_perf_context_print:    graphs reused =        110\n 60%|█████▉    | 161/270 [10:52<07:05,  3.90s/it]Llama.generate: 82 prefix-match hit, remaining 383 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     425.88 ms /   383 tokens (    1.11 ms per token,   899.31 tokens per second)\nllama_perf_context_print:        eval time =    3217.75 ms /   121 runs   (   26.59 ms per token,    37.60 tokens per second)\nllama_perf_context_print:       total time =    3782.80 ms /   504 tokens\nllama_perf_context_print:    graphs reused =        116\n 60%|██████    | 162/270 [10:56<06:57,  3.87s/it]Llama.generate: 83 prefix-match hit, remaining 394 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.32 ms /   394 tokens (    1.27 ms per token,   785.93 tokens per second)\nllama_perf_context_print:        eval time =    3618.83 ms /   137 runs   (   26.41 ms per token,    37.86 tokens per second)\nllama_perf_context_print:       total time =    4280.41 ms /   531 tokens\nllama_perf_context_print:    graphs reused =        131\n 60%|██████    | 163/270 [11:00<07:07,  3.99s/it]Llama.generate: 82 prefix-match hit, remaining 410 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     503.12 ms /   410 tokens (    1.23 ms per token,   814.91 tokens per second)\nllama_perf_context_print:        eval time =    3132.31 ms /   119 runs   (   26.32 ms per token,    37.99 tokens per second)\nllama_perf_context_print:       total time =    3773.70 ms /   529 tokens\nllama_perf_context_print:    graphs reused =        114\n 61%|██████    | 164/270 [11:04<06:56,  3.93s/it]Llama.generate: 82 prefix-match hit, remaining 435 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     525.23 ms /   435 tokens (    1.21 ms per token,   828.21 tokens per second)\nllama_perf_context_print:        eval time =    3082.06 ms /   118 runs   (   26.12 ms per token,    38.29 tokens per second)\nllama_perf_context_print:       total time =    3745.59 ms /   553 tokens\nllama_perf_context_print:    graphs reused =        114\n 61%|██████    | 165/270 [11:08<06:46,  3.87s/it]Llama.generate: 82 prefix-match hit, remaining 362 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     418.33 ms /   362 tokens (    1.16 ms per token,   865.34 tokens per second)\nllama_perf_context_print:        eval time =    2513.84 ms /    93 runs   (   27.03 ms per token,    37.00 tokens per second)\nllama_perf_context_print:       total time =    3039.46 ms /   455 tokens\nllama_perf_context_print:    graphs reused =         89\n 61%|██████▏   | 166/270 [11:11<06:17,  3.63s/it]Llama.generate: 82 prefix-match hit, remaining 376 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     423.35 ms /   376 tokens (    1.13 ms per token,   888.16 tokens per second)\nllama_perf_context_print:        eval time =    3309.93 ms /   124 runs   (   26.69 ms per token,    37.46 tokens per second)\nllama_perf_context_print:       total time =    3876.36 ms /   500 tokens\nllama_perf_context_print:    graphs reused =        119\n 62%|██████▏   | 167/270 [11:15<06:21,  3.70s/it]Llama.generate: 82 prefix-match hit, remaining 404 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.93 ms /   404 tokens (    1.24 ms per token,   804.89 tokens per second)\nllama_perf_context_print:        eval time =    3092.66 ms /   117 runs   (   26.43 ms per token,    37.83 tokens per second)\nllama_perf_context_print:       total time =    3729.45 ms /   521 tokens\nllama_perf_context_print:    graphs reused =        113\n 62%|██████▏   | 168/270 [11:18<06:18,  3.71s/it]Llama.generate: 82 prefix-match hit, remaining 435 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     528.15 ms /   435 tokens (    1.21 ms per token,   823.63 tokens per second)\nllama_perf_context_print:        eval time =    4137.22 ms /   158 runs   (   26.18 ms per token,    38.19 tokens per second)\nllama_perf_context_print:       total time =    4850.77 ms /   593 tokens\nllama_perf_context_print:    graphs reused =        152\n 63%|██████▎   | 169/270 [11:23<06:49,  4.06s/it]Llama.generate: 83 prefix-match hit, remaining 385 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.59 ms /   385 tokens (    1.29 ms per token,   773.73 tokens per second)\nllama_perf_context_print:        eval time =    3848.74 ms /   145 runs   (   26.54 ms per token,    37.67 tokens per second)\nllama_perf_context_print:       total time =    4516.23 ms /   530 tokens\nllama_perf_context_print:    graphs reused =        139\n 63%|██████▎   | 170/270 [11:28<06:59,  4.20s/it]Llama.generate: 82 prefix-match hit, remaining 438 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     531.50 ms /   438 tokens (    1.21 ms per token,   824.08 tokens per second)\nllama_perf_context_print:        eval time =    2775.91 ms /   106 runs   (   26.19 ms per token,    38.19 tokens per second)\nllama_perf_context_print:       total time =    3428.07 ms /   544 tokens\nllama_perf_context_print:    graphs reused =        102\n 63%|██████▎   | 171/270 [11:31<06:32,  3.97s/it]Llama.generate: 82 prefix-match hit, remaining 394 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     498.88 ms /   394 tokens (    1.27 ms per token,   789.77 tokens per second)\nllama_perf_context_print:        eval time =    2999.94 ms /   113 runs   (   26.55 ms per token,    37.67 tokens per second)\nllama_perf_context_print:       total time =    3628.27 ms /   507 tokens\nllama_perf_context_print:    graphs reused =        108\n 64%|██████▎   | 172/270 [11:35<06:18,  3.87s/it]Llama.generate: 82 prefix-match hit, remaining 388 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     499.51 ms /   388 tokens (    1.29 ms per token,   776.76 tokens per second)\nllama_perf_context_print:        eval time =    3688.81 ms /   139 runs   (   26.54 ms per token,    37.68 tokens per second)\nllama_perf_context_print:       total time =    4354.32 ms /   527 tokens\nllama_perf_context_print:    graphs reused =        133\n 64%|██████▍   | 173/270 [11:39<06:29,  4.01s/it]Llama.generate: 82 prefix-match hit, remaining 420 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     505.71 ms /   420 tokens (    1.20 ms per token,   830.52 tokens per second)\nllama_perf_context_print:        eval time =    3281.62 ms /   125 runs   (   26.25 ms per token,    38.09 tokens per second)\nllama_perf_context_print:       total time =    3932.47 ms /   545 tokens\nllama_perf_context_print:    graphs reused =        120\n 64%|██████▍   | 174/270 [11:43<06:23,  3.99s/it]Llama.generate: 82 prefix-match hit, remaining 391 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.09 ms /   391 tokens (    1.28 ms per token,   780.30 tokens per second)\nllama_perf_context_print:        eval time =    3498.23 ms /   132 runs   (   26.50 ms per token,    37.73 tokens per second)\nllama_perf_context_print:       total time =    4154.01 ms /   523 tokens\nllama_perf_context_print:    graphs reused =        127\n 65%|██████▍   | 175/270 [11:47<06:23,  4.04s/it]Llama.generate: 83 prefix-match hit, remaining 317 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     377.44 ms /   317 tokens (    1.19 ms per token,   839.88 tokens per second)\nllama_perf_context_print:        eval time =    3888.28 ms /   144 runs   (   27.00 ms per token,    37.03 tokens per second)\nllama_perf_context_print:       total time =    4439.23 ms /   461 tokens\nllama_perf_context_print:    graphs reused =        139\n 65%|██████▌   | 176/270 [11:52<06:31,  4.16s/it]Llama.generate: 82 prefix-match hit, remaining 407 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     502.71 ms /   407 tokens (    1.24 ms per token,   809.61 tokens per second)\nllama_perf_context_print:        eval time =    2591.24 ms /    98 runs   (   26.44 ms per token,    37.82 tokens per second)\nllama_perf_context_print:       total time =    3205.74 ms /   505 tokens\nllama_perf_context_print:    graphs reused =         94\n 66%|██████▌   | 177/270 [11:55<06:00,  3.88s/it]Llama.generate: 82 prefix-match hit, remaining 387 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     498.37 ms /   387 tokens (    1.29 ms per token,   776.53 tokens per second)\nllama_perf_context_print:        eval time =    3109.89 ms /   117 runs   (   26.58 ms per token,    37.62 tokens per second)\nllama_perf_context_print:       total time =    3745.33 ms /   504 tokens\nllama_perf_context_print:    graphs reused =        112\n 66%|██████▌   | 178/270 [11:59<05:53,  3.84s/it]Llama.generate: 84 prefix-match hit, remaining 403 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.94 ms /   403 tokens (    1.25 ms per token,   802.88 tokens per second)\nllama_perf_context_print:        eval time =    2799.30 ms /   106 runs   (   26.41 ms per token,    37.87 tokens per second)\nllama_perf_context_print:       total time =    3423.94 ms /   509 tokens\nllama_perf_context_print:    graphs reused =        102\n 66%|██████▋   | 179/270 [12:02<05:38,  3.72s/it]Llama.generate: 84 prefix-match hit, remaining 406 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.58 ms /   406 tokens (    1.24 ms per token,   809.45 tokens per second)\nllama_perf_context_print:        eval time =    3187.70 ms /   121 runs   (   26.34 ms per token,    37.96 tokens per second)\nllama_perf_context_print:       total time =    3830.81 ms /   527 tokens\nllama_perf_context_print:    graphs reused =        116\n 67%|██████▋   | 180/270 [12:06<05:37,  3.75s/it]Llama.generate: 82 prefix-match hit, remaining 404 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     500.68 ms /   404 tokens (    1.24 ms per token,   806.90 tokens per second)\nllama_perf_context_print:        eval time =    3112.55 ms /   118 runs   (   26.38 ms per token,    37.91 tokens per second)\nllama_perf_context_print:       total time =    3750.42 ms /   522 tokens\nllama_perf_context_print:    graphs reused =        114\n 67%|██████▋   | 181/270 [12:10<05:34,  3.75s/it]Llama.generate: 82 prefix-match hit, remaining 391 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.38 ms /   391 tokens (    1.27 ms per token,   786.13 tokens per second)\nllama_perf_context_print:        eval time =    2999.12 ms /   113 runs   (   26.54 ms per token,    37.68 tokens per second)\nllama_perf_context_print:       total time =    3629.44 ms /   504 tokens\nllama_perf_context_print:    graphs reused =        108\n 67%|██████▋   | 182/270 [12:13<05:27,  3.72s/it]Llama.generate: 82 prefix-match hit, remaining 399 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     498.62 ms /   399 tokens (    1.25 ms per token,   800.20 tokens per second)\nllama_perf_context_print:        eval time =    3563.86 ms /   135 runs   (   26.40 ms per token,    37.88 tokens per second)\nllama_perf_context_print:       total time =    4221.53 ms /   534 tokens\nllama_perf_context_print:    graphs reused =        130\n 68%|██████▊   | 183/270 [12:18<05:36,  3.87s/it]Llama.generate: 82 prefix-match hit, remaining 389 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     498.81 ms /   389 tokens (    1.28 ms per token,   779.85 tokens per second)\nllama_perf_context_print:        eval time =    4279.94 ms /   162 runs   (   26.42 ms per token,    37.85 tokens per second)\nllama_perf_context_print:       total time =    4972.70 ms /   551 tokens\nllama_perf_context_print:    graphs reused =        156\n 68%|██████▊   | 184/270 [12:23<06:01,  4.20s/it]Llama.generate: 82 prefix-match hit, remaining 431 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     527.00 ms /   431 tokens (    1.22 ms per token,   817.83 tokens per second)\nllama_perf_context_print:        eval time =    3663.52 ms /   140 runs   (   26.17 ms per token,    38.21 tokens per second)\nllama_perf_context_print:       total time =    4355.88 ms /   571 tokens\nllama_perf_context_print:    graphs reused =        135\n 69%|██████▊   | 185/270 [12:27<06:01,  4.25s/it]Llama.generate: 82 prefix-match hit, remaining 305 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     371.26 ms /   305 tokens (    1.22 ms per token,   821.52 tokens per second)\nllama_perf_context_print:        eval time =    3294.72 ms /   121 runs   (   27.23 ms per token,    36.73 tokens per second)\nllama_perf_context_print:       total time =    3806.87 ms /   426 tokens\nllama_perf_context_print:    graphs reused =        117\n 69%|██████▉   | 186/270 [12:31<05:45,  4.12s/it]Llama.generate: 82 prefix-match hit, remaining 427 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     505.71 ms /   427 tokens (    1.18 ms per token,   844.36 tokens per second)\nllama_perf_context_print:        eval time =    3481.33 ms /   133 runs   (   26.18 ms per token,    38.20 tokens per second)\nllama_perf_context_print:       total time =    4143.26 ms /   560 tokens\nllama_perf_context_print:    graphs reused =        127\n 69%|██████▉   | 187/270 [12:35<05:42,  4.13s/it]Llama.generate: 82 prefix-match hit, remaining 400 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     495.59 ms /   400 tokens (    1.24 ms per token,   807.11 tokens per second)\nllama_perf_context_print:        eval time =    4558.57 ms /   173 runs   (   26.35 ms per token,    37.95 tokens per second)\nllama_perf_context_print:       total time =    5263.31 ms /   573 tokens\nllama_perf_context_print:    graphs reused =        167\n 70%|██████▉   | 188/270 [12:40<06:06,  4.47s/it]Llama.generate: 82 prefix-match hit, remaining 386 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     496.00 ms /   386 tokens (    1.28 ms per token,   778.22 tokens per second)\nllama_perf_context_print:        eval time =    4232.82 ms /   160 runs   (   26.46 ms per token,    37.80 tokens per second)\nllama_perf_context_print:       total time =    4921.27 ms /   546 tokens\nllama_perf_context_print:    graphs reused =        154\n 70%|███████   | 189/270 [12:45<06:13,  4.61s/it]Llama.generate: 82 prefix-match hit, remaining 442 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     526.14 ms /   442 tokens (    1.19 ms per token,   840.09 tokens per second)\nllama_perf_context_print:        eval time =    3215.14 ms /   123 runs   (   26.14 ms per token,    38.26 tokens per second)\nllama_perf_context_print:       total time =    3888.59 ms /   565 tokens\nllama_perf_context_print:    graphs reused =        118\n 70%|███████   | 190/270 [12:49<05:51,  4.39s/it]Llama.generate: 82 prefix-match hit, remaining 374 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     424.64 ms /   374 tokens (    1.14 ms per token,   880.74 tokens per second)\nllama_perf_context_print:        eval time =    3095.36 ms /   116 runs   (   26.68 ms per token,    37.48 tokens per second)\nllama_perf_context_print:       total time =    3654.28 ms /   490 tokens\nllama_perf_context_print:    graphs reused =        112\n 71%|███████   | 191/270 [12:53<05:29,  4.17s/it]Llama.generate: 82 prefix-match hit, remaining 381 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     425.85 ms /   381 tokens (    1.12 ms per token,   894.68 tokens per second)\nllama_perf_context_print:        eval time =    3166.47 ms /   119 runs   (   26.61 ms per token,    37.58 tokens per second)\nllama_perf_context_print:       total time =    3731.91 ms /   500 tokens\nllama_perf_context_print:    graphs reused =        114\n 71%|███████   | 192/270 [12:56<05:15,  4.04s/it]Llama.generate: 84 prefix-match hit, remaining 403 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     498.80 ms /   403 tokens (    1.24 ms per token,   807.93 tokens per second)\nllama_perf_context_print:        eval time =    3821.38 ms /   145 runs   (   26.35 ms per token,    37.94 tokens per second)\nllama_perf_context_print:       total time =    4491.18 ms /   548 tokens\nllama_perf_context_print:    graphs reused =        140\n 71%|███████▏  | 193/270 [13:01<05:21,  4.18s/it]Llama.generate: 84 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.65 ms /   398 tokens (    1.26 ms per token,   793.38 tokens per second)\nllama_perf_context_print:        eval time =    2986.00 ms /   113 runs   (   26.42 ms per token,    37.84 tokens per second)\nllama_perf_context_print:       total time =    3617.59 ms /   511 tokens\nllama_perf_context_print:    graphs reused =        109\n 72%|███████▏  | 194/270 [13:05<05:04,  4.01s/it]Llama.generate: 84 prefix-match hit, remaining 410 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.77 ms /   410 tokens (    1.22 ms per token,   817.10 tokens per second)\nllama_perf_context_print:        eval time =    3390.57 ms /   129 runs   (   26.28 ms per token,    38.05 tokens per second)\nllama_perf_context_print:       total time =    4043.70 ms /   539 tokens\nllama_perf_context_print:    graphs reused =        124\n 72%|███████▏  | 195/270 [13:09<05:01,  4.02s/it]Llama.generate: 82 prefix-match hit, remaining 394 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.04 ms /   394 tokens (    1.26 ms per token,   792.70 tokens per second)\nllama_perf_context_print:        eval time =    3466.00 ms /   131 runs   (   26.46 ms per token,    37.80 tokens per second)\nllama_perf_context_print:       total time =    4121.66 ms /   525 tokens\nllama_perf_context_print:    graphs reused =        126\n 73%|███████▎  | 196/270 [13:13<05:00,  4.05s/it]Llama.generate: 86 prefix-match hit, remaining 375 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     422.87 ms /   375 tokens (    1.13 ms per token,   886.81 tokens per second)\nllama_perf_context_print:        eval time =    3145.09 ms /   118 runs   (   26.65 ms per token,    37.52 tokens per second)\nllama_perf_context_print:       total time =    3705.16 ms /   493 tokens\nllama_perf_context_print:    graphs reused =        113\n 73%|███████▎  | 197/270 [13:16<04:48,  3.95s/it]Llama.generate: 82 prefix-match hit, remaining 355 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     418.90 ms /   355 tokens (    1.18 ms per token,   847.46 tokens per second)\nllama_perf_context_print:        eval time =    3537.47 ms /   132 runs   (   26.80 ms per token,    37.31 tokens per second)\nllama_perf_context_print:       total time =    4113.94 ms /   487 tokens\nllama_perf_context_print:    graphs reused =        127\n 73%|███████▎  | 198/270 [13:21<04:48,  4.00s/it]Llama.generate: 82 prefix-match hit, remaining 427 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     504.45 ms /   427 tokens (    1.18 ms per token,   846.47 tokens per second)\nllama_perf_context_print:        eval time =    3217.59 ms /   123 runs   (   26.16 ms per token,    38.23 tokens per second)\nllama_perf_context_print:       total time =    3867.54 ms /   550 tokens\nllama_perf_context_print:    graphs reused =        118\n 74%|███████▎  | 199/270 [13:24<04:41,  3.96s/it]Llama.generate: 82 prefix-match hit, remaining 422 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     503.50 ms /   422 tokens (    1.19 ms per token,   838.13 tokens per second)\nllama_perf_context_print:        eval time =    3590.28 ms /   137 runs   (   26.21 ms per token,    38.16 tokens per second)\nllama_perf_context_print:       total time =    4254.94 ms /   559 tokens\nllama_perf_context_print:    graphs reused =        131\n 74%|███████▍  | 200/270 [13:29<04:43,  4.05s/it]Llama.generate: 82 prefix-match hit, remaining 403 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.36 ms /   403 tokens (    1.24 ms per token,   803.81 tokens per second)\nllama_perf_context_print:        eval time =    4609.09 ms /   175 runs   (   26.34 ms per token,    37.97 tokens per second)\nllama_perf_context_print:       total time =    5324.98 ms /   578 tokens\nllama_perf_context_print:    graphs reused =        169\n 74%|███████▍  | 201/270 [13:34<05:06,  4.44s/it]Llama.generate: 82 prefix-match hit, remaining 377 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     426.17 ms /   377 tokens (    1.13 ms per token,   884.62 tokens per second)\nllama_perf_context_print:        eval time =    2778.03 ms /   104 runs   (   26.71 ms per token,    37.44 tokens per second)\nllama_perf_context_print:       total time =    3327.09 ms /   481 tokens\nllama_perf_context_print:    graphs reused =        100\n 75%|███████▍  | 202/270 [13:37<04:39,  4.10s/it]Llama.generate: 82 prefix-match hit, remaining 387 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.22 ms /   387 tokens (    1.28 ms per token,   778.32 tokens per second)\nllama_perf_context_print:        eval time =    3208.79 ms /   121 runs   (   26.52 ms per token,    37.71 tokens per second)\nllama_perf_context_print:       total time =    3847.56 ms /   508 tokens\nllama_perf_context_print:    graphs reused =        116\n 75%|███████▌  | 203/270 [13:41<04:29,  4.03s/it]Llama.generate: 84 prefix-match hit, remaining 405 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     502.50 ms /   405 tokens (    1.24 ms per token,   805.97 tokens per second)\nllama_perf_context_print:        eval time =    3057.46 ms /   116 runs   (   26.36 ms per token,    37.94 tokens per second)\nllama_perf_context_print:       total time =    3695.16 ms /   521 tokens\nllama_perf_context_print:    graphs reused =        112\n 76%|███████▌  | 204/270 [13:45<04:19,  3.93s/it]Llama.generate: 82 prefix-match hit, remaining 399 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.04 ms /   399 tokens (    1.26 ms per token,   796.35 tokens per second)\nllama_perf_context_print:        eval time =    3119.55 ms /   118 runs   (   26.44 ms per token,    37.83 tokens per second)\nllama_perf_context_print:       total time =    3759.13 ms /   517 tokens\nllama_perf_context_print:    graphs reused =        114\n 76%|███████▌  | 205/270 [13:49<04:12,  3.88s/it]Llama.generate: 90 prefix-match hit, remaining 387 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     500.03 ms /   387 tokens (    1.29 ms per token,   773.96 tokens per second)\nllama_perf_context_print:        eval time =    2938.15 ms /   111 runs   (   26.47 ms per token,    37.78 tokens per second)\nllama_perf_context_print:       total time =    3566.46 ms /   498 tokens\nllama_perf_context_print:    graphs reused =        106\n 76%|███████▋  | 206/270 [13:52<04:02,  3.79s/it]Llama.generate: 82 prefix-match hit, remaining 399 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     502.06 ms /   399 tokens (    1.26 ms per token,   794.73 tokens per second)\nllama_perf_context_print:        eval time =    4037.09 ms /   153 runs   (   26.39 ms per token,    37.90 tokens per second)\nllama_perf_context_print:       total time =    4722.28 ms /   552 tokens\nllama_perf_context_print:    graphs reused =        148\n 77%|███████▋  | 207/270 [13:57<04:16,  4.07s/it]Llama.generate: 84 prefix-match hit, remaining 479 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     589.38 ms /   479 tokens (    1.23 ms per token,   812.72 tokens per second)\nllama_perf_context_print:        eval time =    3306.22 ms /   126 runs   (   26.24 ms per token,    38.11 tokens per second)\nllama_perf_context_print:       total time =    4042.83 ms /   605 tokens\nllama_perf_context_print:    graphs reused =        121\n 77%|███████▋  | 208/270 [14:01<04:11,  4.06s/it]Llama.generate: 82 prefix-match hit, remaining 422 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     505.75 ms /   422 tokens (    1.20 ms per token,   834.41 tokens per second)\nllama_perf_context_print:        eval time =    3381.61 ms /   129 runs   (   26.21 ms per token,    38.15 tokens per second)\nllama_perf_context_print:       total time =    4040.04 ms /   551 tokens\nllama_perf_context_print:    graphs reused =        124\n 77%|███████▋  | 209/270 [14:05<04:07,  4.06s/it]Llama.generate: 82 prefix-match hit, remaining 420 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     505.33 ms /   420 tokens (    1.20 ms per token,   831.14 tokens per second)\nllama_perf_context_print:        eval time =    3385.24 ms /   129 runs   (   26.24 ms per token,    38.11 tokens per second)\nllama_perf_context_print:       total time =    4042.06 ms /   549 tokens\nllama_perf_context_print:    graphs reused =        124\n 78%|███████▊  | 210/270 [14:09<04:03,  4.05s/it]Llama.generate: 82 prefix-match hit, remaining 417 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     506.78 ms /   417 tokens (    1.22 ms per token,   822.84 tokens per second)\nllama_perf_context_print:        eval time =    3387.46 ms /   129 runs   (   26.26 ms per token,    38.08 tokens per second)\nllama_perf_context_print:       total time =    4045.42 ms /   546 tokens\nllama_perf_context_print:    graphs reused =        124\n 78%|███████▊  | 211/270 [14:13<03:59,  4.05s/it]Llama.generate: 82 prefix-match hit, remaining 372 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     422.82 ms /   372 tokens (    1.14 ms per token,   879.81 tokens per second)\nllama_perf_context_print:        eval time =    4119.20 ms /   155 runs   (   26.58 ms per token,    37.63 tokens per second)\nllama_perf_context_print:       total time =    4725.54 ms /   527 tokens\nllama_perf_context_print:    graphs reused =        149\n 79%|███████▊  | 212/270 [14:18<04:06,  4.26s/it]Llama.generate: 82 prefix-match hit, remaining 396 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.41 ms /   396 tokens (    1.26 ms per token,   796.12 tokens per second)\nllama_perf_context_print:        eval time =    3312.71 ms /   125 runs   (   26.50 ms per token,    37.73 tokens per second)\nllama_perf_context_print:       total time =    3954.85 ms /   521 tokens\nllama_perf_context_print:    graphs reused =        120\n 79%|███████▉  | 213/270 [14:22<03:57,  4.17s/it]Llama.generate: 84 prefix-match hit, remaining 388 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     498.35 ms /   388 tokens (    1.28 ms per token,   778.57 tokens per second)\nllama_perf_context_print:        eval time =    3314.71 ms /   125 runs   (   26.52 ms per token,    37.71 tokens per second)\nllama_perf_context_print:       total time =    3957.71 ms /   513 tokens\nllama_perf_context_print:    graphs reused =        120\n 79%|███████▉  | 214/270 [14:26<03:49,  4.11s/it]Llama.generate: 82 prefix-match hit, remaining 413 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     502.21 ms /   413 tokens (    1.22 ms per token,   822.36 tokens per second)\nllama_perf_context_print:        eval time =    2925.62 ms /   111 runs   (   26.36 ms per token,    37.94 tokens per second)\nllama_perf_context_print:       total time =    3556.16 ms /   524 tokens\nllama_perf_context_print:    graphs reused =        107\n 80%|███████▉  | 215/270 [14:29<03:36,  3.94s/it]Llama.generate: 82 prefix-match hit, remaining 378 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     422.95 ms /   378 tokens (    1.12 ms per token,   893.72 tokens per second)\nllama_perf_context_print:        eval time =    3306.25 ms /   124 runs   (   26.66 ms per token,    37.50 tokens per second)\nllama_perf_context_print:       total time =    3876.72 ms /   502 tokens\nllama_perf_context_print:    graphs reused =        119\n 80%|████████  | 216/270 [14:33<03:31,  3.92s/it]Llama.generate: 82 prefix-match hit, remaining 400 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     499.39 ms /   400 tokens (    1.25 ms per token,   800.98 tokens per second)\nllama_perf_context_print:        eval time =    3256.39 ms /   123 runs   (   26.47 ms per token,    37.77 tokens per second)\nllama_perf_context_print:       total time =    3899.10 ms /   523 tokens\nllama_perf_context_print:    graphs reused =        119\n 80%|████████  | 217/270 [14:37<03:27,  3.92s/it]Llama.generate: 82 prefix-match hit, remaining 377 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     423.62 ms /   377 tokens (    1.12 ms per token,   889.95 tokens per second)\nllama_perf_context_print:        eval time =    3327.88 ms /   125 runs   (   26.62 ms per token,    37.56 tokens per second)\nllama_perf_context_print:       total time =    3896.60 ms /   502 tokens\nllama_perf_context_print:    graphs reused =        120\n 81%|████████  | 218/270 [14:41<03:23,  3.91s/it]Llama.generate: 82 prefix-match hit, remaining 416 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     500.16 ms /   416 tokens (    1.20 ms per token,   831.74 tokens per second)\nllama_perf_context_print:        eval time =    3255.68 ms /   124 runs   (   26.26 ms per token,    38.09 tokens per second)\nllama_perf_context_print:       total time =    3900.83 ms /   540 tokens\nllama_perf_context_print:    graphs reused =        119\n 81%|████████  | 219/270 [14:45<03:19,  3.91s/it]Llama.generate: 82 prefix-match hit, remaining 406 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     499.74 ms /   406 tokens (    1.23 ms per token,   812.43 tokens per second)\nllama_perf_context_print:        eval time =    3082.87 ms /   117 runs   (   26.35 ms per token,    37.95 tokens per second)\nllama_perf_context_print:       total time =    3719.02 ms /   523 tokens\nllama_perf_context_print:    graphs reused =        113\n 81%|████████▏ | 220/270 [14:49<03:12,  3.85s/it]Llama.generate: 82 prefix-match hit, remaining 427 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     507.78 ms /   427 tokens (    1.19 ms per token,   840.92 tokens per second)\nllama_perf_context_print:        eval time =    3212.60 ms /   123 runs   (   26.12 ms per token,    38.29 tokens per second)\nllama_perf_context_print:       total time =    3864.48 ms /   550 tokens\nllama_perf_context_print:    graphs reused =        118\n 82%|████████▏ | 221/270 [14:53<03:09,  3.86s/it]Llama.generate: 82 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.29 ms /   398 tokens (    1.26 ms per token,   793.95 tokens per second)\nllama_perf_context_print:        eval time =    4192.93 ms /   159 runs   (   26.37 ms per token,    37.92 tokens per second)\nllama_perf_context_print:       total time =    4884.37 ms /   557 tokens\nllama_perf_context_print:    graphs reused =        154\n 82%|████████▏ | 222/270 [14:57<03:20,  4.17s/it]Llama.generate: 82 prefix-match hit, remaining 417 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     503.86 ms /   417 tokens (    1.21 ms per token,   827.62 tokens per second)\nllama_perf_context_print:        eval time =    2864.09 ms /   109 runs   (   26.28 ms per token,    38.06 tokens per second)\nllama_perf_context_print:       total time =    3493.89 ms /   526 tokens\nllama_perf_context_print:    graphs reused =        105\n 83%|████████▎ | 223/270 [15:01<03:06,  3.97s/it]Llama.generate: 82 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.01 ms /   398 tokens (    1.26 ms per token,   794.39 tokens per second)\nllama_perf_context_print:        eval time =    4371.47 ms /   166 runs   (   26.33 ms per token,    37.97 tokens per second)\nllama_perf_context_print:       total time =    5073.04 ms /   564 tokens\nllama_perf_context_print:    graphs reused =        160\n 83%|████████▎ | 224/270 [15:06<03:17,  4.30s/it]Llama.generate: 82 prefix-match hit, remaining 364 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     422.71 ms /   364 tokens (    1.16 ms per token,   861.10 tokens per second)\nllama_perf_context_print:        eval time =    3233.23 ms /   121 runs   (   26.72 ms per token,    37.42 tokens per second)\nllama_perf_context_print:       total time =    3796.41 ms /   485 tokens\nllama_perf_context_print:    graphs reused =        116\n 83%|████████▎ | 225/270 [15:10<03:06,  4.15s/it]Llama.generate: 82 prefix-match hit, remaining 274 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     342.75 ms /   274 tokens (    1.25 ms per token,   799.42 tokens per second)\nllama_perf_context_print:        eval time =    2900.30 ms /   107 runs   (   27.11 ms per token,    36.89 tokens per second)\nllama_perf_context_print:       total time =    3366.92 ms /   381 tokens\nllama_perf_context_print:    graphs reused =        103\n 84%|████████▎ | 226/270 [15:13<02:52,  3.92s/it]Llama.generate: 82 prefix-match hit, remaining 406 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     505.22 ms /   406 tokens (    1.24 ms per token,   803.60 tokens per second)\nllama_perf_context_print:        eval time =    3866.75 ms /   147 runs   (   26.30 ms per token,    38.02 tokens per second)\nllama_perf_context_print:       total time =    4549.33 ms /   553 tokens\nllama_perf_context_print:    graphs reused =        142\n 84%|████████▍ | 227/270 [15:18<02:56,  4.11s/it]Llama.generate: 82 prefix-match hit, remaining 409 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     503.80 ms /   409 tokens (    1.23 ms per token,   811.82 tokens per second)\nllama_perf_context_print:        eval time =    3395.70 ms /   129 runs   (   26.32 ms per token,    37.99 tokens per second)\nllama_perf_context_print:       total time =    4049.83 ms /   538 tokens\nllama_perf_context_print:    graphs reused =        124\n 84%|████████▍ | 228/270 [15:22<02:51,  4.09s/it]Llama.generate: 82 prefix-match hit, remaining 400 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     496.89 ms /   400 tokens (    1.24 ms per token,   805.01 tokens per second)\nllama_perf_context_print:        eval time =    3122.77 ms /   118 runs   (   26.46 ms per token,    37.79 tokens per second)\nllama_perf_context_print:       total time =    3757.42 ms /   518 tokens\nllama_perf_context_print:    graphs reused =        114\n 85%|████████▍ | 229/270 [15:26<02:43,  3.99s/it]Llama.generate: 82 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     503.57 ms /   398 tokens (    1.27 ms per token,   790.35 tokens per second)\nllama_perf_context_print:        eval time =    2885.63 ms /   109 runs   (   26.47 ms per token,    37.77 tokens per second)\nllama_perf_context_print:       total time =    3516.40 ms /   507 tokens\nllama_perf_context_print:    graphs reused =        105\n 85%|████████▌ | 230/270 [15:29<02:34,  3.85s/it]Llama.generate: 82 prefix-match hit, remaining 378 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     424.51 ms /   378 tokens (    1.12 ms per token,   890.43 tokens per second)\nllama_perf_context_print:        eval time =    3460.71 ms /   130 runs   (   26.62 ms per token,    37.56 tokens per second)\nllama_perf_context_print:       total time =    4039.08 ms /   508 tokens\nllama_perf_context_print:    graphs reused =        125\n 86%|████████▌ | 231/270 [15:33<02:32,  3.91s/it]Llama.generate: 82 prefix-match hit, remaining 392 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.37 ms /   392 tokens (    1.27 ms per token,   788.15 tokens per second)\nllama_perf_context_print:        eval time =    3573.75 ms /   135 runs   (   26.47 ms per token,    37.78 tokens per second)\nllama_perf_context_print:       total time =    4232.61 ms /   527 tokens\nllama_perf_context_print:    graphs reused =        129\n 86%|████████▌ | 232/270 [15:37<02:32,  4.01s/it]Llama.generate: 85 prefix-match hit, remaining 271 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     336.75 ms /   271 tokens (    1.24 ms per token,   804.74 tokens per second)\nllama_perf_context_print:        eval time =    4294.13 ms /   158 runs   (   27.18 ms per token,    36.79 tokens per second)\nllama_perf_context_print:       total time =    4816.90 ms /   429 tokens\nllama_perf_context_print:    graphs reused =        152\n 86%|████████▋ | 233/270 [15:42<02:37,  4.25s/it]Llama.generate: 82 prefix-match hit, remaining 418 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     504.27 ms /   418 tokens (    1.21 ms per token,   828.92 tokens per second)\nllama_perf_context_print:        eval time =    3153.30 ms /   120 runs   (   26.28 ms per token,    38.06 tokens per second)\nllama_perf_context_print:       total time =    3795.78 ms /   538 tokens\nllama_perf_context_print:    graphs reused =        115\n 87%|████████▋ | 234/270 [15:46<02:28,  4.12s/it]Llama.generate: 82 prefix-match hit, remaining 412 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.03 ms /   412 tokens (    1.22 ms per token,   822.31 tokens per second)\nllama_perf_context_print:        eval time =    4185.02 ms /   159 runs   (   26.32 ms per token,    37.99 tokens per second)\nllama_perf_context_print:       total time =    4873.22 ms /   571 tokens\nllama_perf_context_print:    graphs reused =        153\n 87%|████████▋ | 235/270 [15:51<02:32,  4.35s/it]Llama.generate: 82 prefix-match hit, remaining 435 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     529.09 ms /   435 tokens (    1.22 ms per token,   822.17 tokens per second)\nllama_perf_context_print:        eval time =    3237.96 ms /   124 runs   (   26.11 ms per token,    38.30 tokens per second)\nllama_perf_context_print:       total time =    3912.58 ms /   559 tokens\nllama_perf_context_print:    graphs reused =        119\n 87%|████████▋ | 236/270 [15:55<02:23,  4.22s/it]Llama.generate: 83 prefix-match hit, remaining 383 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     425.59 ms /   383 tokens (    1.11 ms per token,   899.93 tokens per second)\nllama_perf_context_print:        eval time =    4387.01 ms /   166 runs   (   26.43 ms per token,    37.84 tokens per second)\nllama_perf_context_print:       total time =    5011.15 ms /   549 tokens\nllama_perf_context_print:    graphs reused =        160\n 88%|████████▊ | 237/270 [16:00<02:27,  4.46s/it]Llama.generate: 82 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     499.22 ms /   398 tokens (    1.25 ms per token,   797.25 tokens per second)\nllama_perf_context_print:        eval time =    3507.24 ms /   133 runs   (   26.37 ms per token,    37.92 tokens per second)\nllama_perf_context_print:       total time =    4159.62 ms /   531 tokens\nllama_perf_context_print:    graphs reused =        128\n 88%|████████▊ | 238/270 [16:04<02:19,  4.37s/it]Llama.generate: 82 prefix-match hit, remaining 396 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     496.61 ms /   396 tokens (    1.25 ms per token,   797.40 tokens per second)\nllama_perf_context_print:        eval time =    3197.20 ms /   121 runs   (   26.42 ms per token,    37.85 tokens per second)\nllama_perf_context_print:       total time =    3835.72 ms /   517 tokens\nllama_perf_context_print:    graphs reused =        116\n 89%|████████▊ | 239/270 [16:08<02:10,  4.21s/it]Llama.generate: 82 prefix-match hit, remaining 387 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     493.76 ms /   387 tokens (    1.28 ms per token,   783.78 tokens per second)\nllama_perf_context_print:        eval time =    3049.34 ms /   115 runs   (   26.52 ms per token,    37.71 tokens per second)\nllama_perf_context_print:       total time =    3674.46 ms /   502 tokens\nllama_perf_context_print:    graphs reused =        110\n 89%|████████▉ | 240/270 [16:11<02:01,  4.05s/it]Llama.generate: 82 prefix-match hit, remaining 372 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     421.88 ms /   372 tokens (    1.13 ms per token,   881.76 tokens per second)\nllama_perf_context_print:        eval time =    4080.32 ms /   154 runs   (   26.50 ms per token,    37.74 tokens per second)\nllama_perf_context_print:       total time =    4684.26 ms /   526 tokens\nllama_perf_context_print:    graphs reused =        149\n 89%|████████▉ | 241/270 [16:16<02:03,  4.24s/it]Llama.generate: 82 prefix-match hit, remaining 412 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     502.00 ms /   412 tokens (    1.22 ms per token,   820.72 tokens per second)\nllama_perf_context_print:        eval time =    3934.55 ms /   150 runs   (   26.23 ms per token,    38.12 tokens per second)\nllama_perf_context_print:       total time =    4612.11 ms /   562 tokens\nllama_perf_context_print:    graphs reused =        144\n 90%|████████▉ | 242/270 [16:21<02:01,  4.36s/it]Llama.generate: 82 prefix-match hit, remaining 366 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     419.24 ms /   366 tokens (    1.15 ms per token,   873.02 tokens per second)\nllama_perf_context_print:        eval time =    3178.82 ms /   119 runs   (   26.71 ms per token,    37.44 tokens per second)\nllama_perf_context_print:       total time =    3736.99 ms /   485 tokens\nllama_perf_context_print:    graphs reused =        115\n 90%|█████████ | 243/270 [16:25<01:52,  4.17s/it]Llama.generate: 82 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     499.02 ms /   398 tokens (    1.25 ms per token,   797.56 tokens per second)\nllama_perf_context_print:        eval time =    2876.76 ms /   109 runs   (   26.39 ms per token,    37.89 tokens per second)\nllama_perf_context_print:       total time =    3501.98 ms /   507 tokens\nllama_perf_context_print:    graphs reused =        105\n 90%|█████████ | 244/270 [16:28<01:43,  3.97s/it]Llama.generate: 84 prefix-match hit, remaining 443 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     531.25 ms /   443 tokens (    1.20 ms per token,   833.89 tokens per second)\nllama_perf_context_print:        eval time =    3023.74 ms /   116 runs   (   26.07 ms per token,    38.36 tokens per second)\nllama_perf_context_print:       total time =    3688.20 ms /   559 tokens\nllama_perf_context_print:    graphs reused =        111\n 91%|█████████ | 245/270 [16:32<01:37,  3.89s/it]Llama.generate: 82 prefix-match hit, remaining 398 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.73 ms /   398 tokens (    1.25 ms per token,   799.63 tokens per second)\nllama_perf_context_print:        eval time =    2987.06 ms /   113 runs   (   26.43 ms per token,    37.83 tokens per second)\nllama_perf_context_print:       total time =    3615.39 ms /   511 tokens\nllama_perf_context_print:    graphs reused =        109\n 91%|█████████ | 246/270 [16:35<01:31,  3.81s/it]Llama.generate: 82 prefix-match hit, remaining 402 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.01 ms /   402 tokens (    1.25 ms per token,   802.38 tokens per second)\nllama_perf_context_print:        eval time =    2613.06 ms /    99 runs   (   26.39 ms per token,    37.89 tokens per second)\nllama_perf_context_print:       total time =    3230.02 ms /   501 tokens\nllama_perf_context_print:    graphs reused =         95\n 91%|█████████▏| 247/270 [16:39<01:23,  3.64s/it]Llama.generate: 82 prefix-match hit, remaining 367 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     420.30 ms /   367 tokens (    1.15 ms per token,   873.18 tokens per second)\nllama_perf_context_print:        eval time =    2786.84 ms /   104 runs   (   26.80 ms per token,    37.32 tokens per second)\nllama_perf_context_print:       total time =    3326.22 ms /   471 tokens\nllama_perf_context_print:    graphs reused =        100\n 92%|█████████▏| 248/270 [16:42<01:17,  3.54s/it]Llama.generate: 82 prefix-match hit, remaining 434 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     528.90 ms /   434 tokens (    1.22 ms per token,   820.57 tokens per second)\nllama_perf_context_print:        eval time =    3392.15 ms /   130 runs   (   26.09 ms per token,    38.32 tokens per second)\nllama_perf_context_print:       total time =    4072.91 ms /   564 tokens\nllama_perf_context_print:    graphs reused =        125\n 92%|█████████▏| 249/270 [16:46<01:17,  3.70s/it]Llama.generate: 85 prefix-match hit, remaining 393 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     495.45 ms /   393 tokens (    1.26 ms per token,   793.22 tokens per second)\nllama_perf_context_print:        eval time =    3564.73 ms /   135 runs   (   26.41 ms per token,    37.87 tokens per second)\nllama_perf_context_print:       total time =    4214.58 ms /   528 tokens\nllama_perf_context_print:    graphs reused =        129\n 93%|█████████▎| 250/270 [16:50<01:17,  3.86s/it]Llama.generate: 82 prefix-match hit, remaining 395 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     499.48 ms /   395 tokens (    1.26 ms per token,   790.82 tokens per second)\nllama_perf_context_print:        eval time =    4662.11 ms /   177 runs   (   26.34 ms per token,    37.97 tokens per second)\nllama_perf_context_print:       total time =    5361.92 ms /   572 tokens\nllama_perf_context_print:    graphs reused =        170\n 93%|█████████▎| 251/270 [16:56<01:21,  4.31s/it]Llama.generate: 82 prefix-match hit, remaining 383 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     425.29 ms /   383 tokens (    1.11 ms per token,   900.57 tokens per second)\nllama_perf_context_print:        eval time =    3081.43 ms /   116 runs   (   26.56 ms per token,    37.64 tokens per second)\nllama_perf_context_print:       total time =    3643.22 ms /   499 tokens\nllama_perf_context_print:    graphs reused =        111\n 93%|█████████▎| 252/270 [16:59<01:14,  4.11s/it]Llama.generate: 83 prefix-match hit, remaining 394 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     496.82 ms /   394 tokens (    1.26 ms per token,   793.04 tokens per second)\nllama_perf_context_print:        eval time =    2887.77 ms /   109 runs   (   26.49 ms per token,    37.75 tokens per second)\nllama_perf_context_print:       total time =    3508.44 ms /   503 tokens\nllama_perf_context_print:    graphs reused =        104\n 94%|█████████▎| 253/270 [17:03<01:06,  3.93s/it]Llama.generate: 82 prefix-match hit, remaining 435 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     529.74 ms /   435 tokens (    1.22 ms per token,   821.15 tokens per second)\nllama_perf_context_print:        eval time =    2923.60 ms /   112 runs   (   26.10 ms per token,    38.31 tokens per second)\nllama_perf_context_print:       total time =    3581.94 ms /   547 tokens\nllama_perf_context_print:    graphs reused =        108\n 94%|█████████▍| 254/270 [17:06<01:01,  3.83s/it]Llama.generate: 83 prefix-match hit, remaining 391 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.54 ms /   391 tokens (    1.27 ms per token,   785.87 tokens per second)\nllama_perf_context_print:        eval time =    2628.70 ms /    99 runs   (   26.55 ms per token,    37.66 tokens per second)\nllama_perf_context_print:       total time =    3236.56 ms /   490 tokens\nllama_perf_context_print:    graphs reused =         95\n 94%|█████████▍| 255/270 [17:10<00:54,  3.65s/it]Llama.generate: 82 prefix-match hit, remaining 410 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     500.39 ms /   410 tokens (    1.22 ms per token,   819.36 tokens per second)\nllama_perf_context_print:        eval time =    4342.52 ms /   165 runs   (   26.32 ms per token,    38.00 tokens per second)\nllama_perf_context_print:       total time =    5048.35 ms /   575 tokens\nllama_perf_context_print:    graphs reused =        159\n 95%|█████████▍| 256/270 [17:15<00:57,  4.07s/it]Llama.generate: 82 prefix-match hit, remaining 435 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     528.58 ms /   435 tokens (    1.22 ms per token,   822.95 tokens per second)\nllama_perf_context_print:        eval time =    2536.23 ms /    97 runs   (   26.15 ms per token,    38.25 tokens per second)\nllama_perf_context_print:       total time =    3178.54 ms /   532 tokens\nllama_perf_context_print:    graphs reused =         93\n 95%|█████████▌| 257/270 [17:18<00:49,  3.81s/it]Llama.generate: 82 prefix-match hit, remaining 404 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     502.32 ms /   404 tokens (    1.24 ms per token,   804.26 tokens per second)\nllama_perf_context_print:        eval time =    3143.44 ms /   119 runs   (   26.42 ms per token,    37.86 tokens per second)\nllama_perf_context_print:       total time =    3782.28 ms /   523 tokens\nllama_perf_context_print:    graphs reused =        115\n 96%|█████████▌| 258/270 [17:22<00:45,  3.80s/it]Llama.generate: 82 prefix-match hit, remaining 438 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     527.08 ms /   438 tokens (    1.20 ms per token,   831.00 tokens per second)\nllama_perf_context_print:        eval time =    2690.50 ms /   103 runs   (   26.12 ms per token,    38.28 tokens per second)\nllama_perf_context_print:       total time =    3335.64 ms /   541 tokens\nllama_perf_context_print:    graphs reused =         99\n 96%|█████████▌| 259/270 [17:25<00:40,  3.66s/it]Llama.generate: 84 prefix-match hit, remaining 374 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     422.95 ms /   374 tokens (    1.13 ms per token,   884.27 tokens per second)\nllama_perf_context_print:        eval time =    3169.91 ms /   119 runs   (   26.64 ms per token,    37.54 tokens per second)\nllama_perf_context_print:       total time =    3729.88 ms /   493 tokens\nllama_perf_context_print:    graphs reused =        114\n 96%|█████████▋| 260/270 [17:29<00:36,  3.68s/it]Llama.generate: 82 prefix-match hit, remaining 385 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.52 ms /   385 tokens (    1.29 ms per token,   773.84 tokens per second)\nllama_perf_context_print:        eval time =    5100.47 ms /   193 runs   (   26.43 ms per token,    37.84 tokens per second)\nllama_perf_context_print:       total time =    5829.41 ms /   578 tokens\nllama_perf_context_print:    graphs reused =        186\n 97%|█████████▋| 261/270 [17:35<00:38,  4.33s/it]Llama.generate: 82 prefix-match hit, remaining 394 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     498.44 ms /   394 tokens (    1.27 ms per token,   790.47 tokens per second)\nllama_perf_context_print:        eval time =    4328.91 ms /   164 runs   (   26.40 ms per token,    37.88 tokens per second)\nllama_perf_context_print:       total time =    5021.92 ms /   558 tokens\nllama_perf_context_print:    graphs reused =        158\n 97%|█████████▋| 262/270 [17:40<00:36,  4.54s/it]Llama.generate: 82 prefix-match hit, remaining 388 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     497.61 ms /   388 tokens (    1.28 ms per token,   779.73 tokens per second)\nllama_perf_context_print:        eval time =    3183.79 ms /   120 runs   (   26.53 ms per token,    37.69 tokens per second)\nllama_perf_context_print:       total time =    3818.80 ms /   508 tokens\nllama_perf_context_print:    graphs reused =        115\n 97%|█████████▋| 263/270 [17:43<00:30,  4.32s/it]Llama.generate: 82 prefix-match hit, remaining 420 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     503.63 ms /   420 tokens (    1.20 ms per token,   833.94 tokens per second)\nllama_perf_context_print:        eval time =    3776.36 ms /   144 runs   (   26.22 ms per token,    38.13 tokens per second)\nllama_perf_context_print:       total time =    4449.60 ms /   564 tokens\nllama_perf_context_print:    graphs reused =        138\n 98%|█████████▊| 264/270 [17:48<00:26,  4.36s/it]Llama.generate: 82 prefix-match hit, remaining 391 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     498.47 ms /   391 tokens (    1.27 ms per token,   784.40 tokens per second)\nllama_perf_context_print:        eval time =    3105.18 ms /   117 runs   (   26.54 ms per token,    37.68 tokens per second)\nllama_perf_context_print:       total time =    3738.20 ms /   508 tokens\nllama_perf_context_print:    graphs reused =        112\n 98%|█████████▊| 265/270 [17:52<00:20,  4.18s/it]Llama.generate: 82 prefix-match hit, remaining 407 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     502.45 ms /   407 tokens (    1.23 ms per token,   810.03 tokens per second)\nllama_perf_context_print:        eval time =    4102.62 ms /   156 runs   (   26.30 ms per token,    38.02 tokens per second)\nllama_perf_context_print:       total time =    4788.74 ms /   563 tokens\nllama_perf_context_print:    graphs reused =        150\n 99%|█████████▊| 266/270 [17:56<00:17,  4.36s/it]Llama.generate: 82 prefix-match hit, remaining 318 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     376.49 ms /   318 tokens (    1.18 ms per token,   844.63 tokens per second)\nllama_perf_context_print:        eval time =    2938.79 ms /   108 runs   (   27.21 ms per token,    36.75 tokens per second)\nllama_perf_context_print:       total time =    3438.62 ms /   426 tokens\nllama_perf_context_print:    graphs reused =        104\n 99%|█████████▉| 267/270 [18:00<00:12,  4.09s/it]Llama.generate: 82 prefix-match hit, remaining 387 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     498.20 ms /   387 tokens (    1.29 ms per token,   776.79 tokens per second)\nllama_perf_context_print:        eval time =    3917.05 ms /   148 runs   (   26.47 ms per token,    37.78 tokens per second)\nllama_perf_context_print:       total time =    4587.94 ms /   535 tokens\nllama_perf_context_print:    graphs reused =        142\n 99%|█████████▉| 268/270 [18:04<00:08,  4.24s/it]Llama.generate: 84 prefix-match hit, remaining 403 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     504.08 ms /   403 tokens (    1.25 ms per token,   799.48 tokens per second)\nllama_perf_context_print:        eval time =    3375.95 ms /   128 runs   (   26.37 ms per token,    37.92 tokens per second)\nllama_perf_context_print:       total time =    4030.64 ms /   531 tokens\nllama_perf_context_print:    graphs reused =        123\n100%|█████████▉| 269/270 [18:08<00:04,  4.18s/it]Llama.generate: 84 prefix-match hit, remaining 406 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     501.04 ms /   406 tokens (    1.23 ms per token,   810.32 tokens per second)\nllama_perf_context_print:        eval time =    3427.51 ms /   130 runs   (   26.37 ms per token,    37.93 tokens per second)\nllama_perf_context_print:       total time =    4079.14 ms /   536 tokens\nllama_perf_context_print:    graphs reused =        125\n100%|██████████| 270/270 [18:13<00:00,  4.15s/it]Llama.generate: 82 prefix-match hit, remaining 404 prompt tokens to eval\nllama_perf_context_print:        load time =     958.07 ms\nllama_perf_context_print: prompt eval time =     503.42 ms /   404 tokens (    1.25 ms per token,   802.50 tokens per second)\nllama_perf_context_print:        eval time =    3716.47 ms /   141 runs   (   26.36 ms per token,    37.94 tokens per second)\nllama_perf_context_print:       total time =    4388.29 ms /   545 tokens\nllama_perf_context_print:    graphs reused =        136\n100%|██████████| 270/270 [18:17<00:00,  4.06s/it]","output_type":"stream"},{"name":"stdout","text":"VICTORY — DOWNLOAD SHOGUN9000K_FINAL_PERFECT_ROAST.csv — 99%+ BRUTAL JSON GUARANTEED\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# FIXED & FINAL — 99%+ PERFECT JSON — RUN THIS CELL ONLY\n# Original - This one works great, just repetitious words being used\nimport json\nimport re\nfrom tqdm import tqdm\ntqdm.pandas()\n\ndef roast(row):\n    prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are a cynical senior engineer who has seen every tech scam. You return ONLY a single valid JSON object and nothing else — no explanations, no markdown, no extra text.<|eot_id|>\n<|start_header_id|>user<|end_header_id|>\nTitle: {row.get('title','')}\nCompany: {row.get('company','')}\nLocation: {row.get('location','')}\nDescription: {str(row.get('description',''))[:2200]}\n\nReturn exactly this JSON structure and nothing else:\n{{\n  \"summary\": \"one brutal sentence verdict\",\n  \"vibe\": \"faang_tier|hidden_gem|startup_chaos|corporate_zombie|avoid\",\n  \"red_flags\": [\"flag1\", \"flag2\"],\n  \"green_flags\": [\"flag1\"],\n  \"seniority\": \"junior|mid|senior|staff|principal\",\n  \"match_score\": 0-100\n}}<|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\"\"\"\n    \n\n    out = llm(prompt, max_tokens=350, temperature=0.82, stop=[\"<|eot_id|>\"], echo=False)\n    \n    text = out['choices'][0]['text'].strip()\n\n    # Aggressive JSON extraction\n    start = text.find(\"{\")\n    end = text.rfind(\"}\") + 1\n    if start != -1 and end > start:\n        try:\n            return json.loads(text[start:end])\n        except:\n            pass\n    \n    # Fallback: look for any JSON-like blob\n    match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n    if match:\n        try:\n            return json.loads(match.group(0))\n        except:\n            pass\n    \n    return {\"summary\": \"parse failed\", \"vibe\": \"avoid\"}\n\nprint(\"RE-ROASTING ALL 270 JOBS WITH BULLETPROOF PROMPT — ~6–8 MINUTES\")\ndf['insights'] = df.progress_apply(roast, axis=1)\n\n# Save the final victory\ndf.to_csv('SHOGUN9000K_FINAL_PERFECT_ROAST.csv', index=False)\nprint(\"VICTORY — DOWNLOAD SHOGUN9000K_FINAL_PERFECT_ROAST.csv — 99%+ BRUTAL JSON GUARANTEED\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}